<p><span color="#000000" style="color: #000000;"><span style="font-size: 14.6667px; white-space: pre-wrap;">Dans ce module, vous apprendrez trois concepts fondamentaux en apprentissage profond : l'approche modulaire, l'algorithme de r&eacute;tropropagation pour le graphe de calcul et l'optimisation. L'approche modulaire est une technique de conception qui sous-tend la cr&eacute;ation de graphes de calcul complexes. Avec l'approche modulaire, la programmation d'un nouveau mod&egrave;le d'apprentissage profond devient tr&egrave;s facile. Cependant, cette flexibilit&eacute; permet &eacute;galement de tester un nombre exponentiel de mod&egrave;les possibles. Pour aider les scientifiques des donne&eacute;es &agrave; faire face &agrave; cette complexit&eacute;, nous pr&eacute;sentons quelques mod&egrave;les de conception qui sont utilis&eacute;s par les praticiens. </span></span></p>
<p><span color="#000000" style="color: #000000;"><span style="font-size: 14.6667px; white-space: pre-wrap;">La r&eacute;tropropagation est la cheville ouvri&egrave;re de l'apprentissage profond. Cet algorithme, qui remonte &agrave; 1970 dans la communaut&eacute; de la diff&eacute;rentiation automatique, a &eacute;t&eacute; red&eacute;couvert dans l'apprentissage automatique. La r&eacute;tropropagation calcule efficacement les gradients de la fonction de perte par rapport aux param&egrave;tres de n'importe quel graphique de calcul. &Agrave; la fin du module, vous comprendrez la r&eacute;tropropagation comme un algorithme de programmation dynamique avec une application sur un petit mod&egrave;le. </span></span></p>
<p><span color="#000000" style="color: #000000;"><span style="font-size: 14.6667px; white-space: pre-wrap;">L'optimisation est un &eacute;l&eacute;ment central de l'apprentissage automatique; elle est si omnipr&eacute;sente que l'on peut se demander si l'apprentissage ne consiste pas seulement &agrave; optimiser. Dans ce module, vous apprendrez pourquoi ce n'est pas le cas. Vous apprendrez &eacute;galement un algorithme d'optimisation simple, la descente de gradient, ainsi que certaines am&eacute;liorations telles que le pr&eacute;conditionnement et le momentum. Enfin, vous comprendrez comment l'entra&icirc;nement de votre mod&egrave;le &agrave; l'aide de mini-lots de donn&eacute;es cr&eacute;e une approximation stochastique du gradient, ce qui permet d'obtenir un processus d'entra&icirc;nement plus bruyant mais plus rapide.</span></span></p>
