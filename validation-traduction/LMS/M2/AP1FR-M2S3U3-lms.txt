<p>Dans cette vid&eacute;o, Ga&eacute;tan pr&eacute;sente le <strong>Momentum</strong>, un outil important pour am&eacute;liorer les taux de convergence dans les algorithmes d&apos;optimisation de descente de gradient. Ga&eacute;tan pr&eacute;sente aussi l&rsquo;optimiseur omnipr&eacute;sent Adam, qui combine le momentum et le pr&eacute;conditionnement pour am&eacute;liorer consid&eacute;rablement le taux de convergence de la descente de gradient classique. Cette vid&eacute;o est importante pour comprendre l&apos;un des outils d&apos;optimisation les plus courants dans la bo&icirc;te &agrave; outils de tout praticien d&apos;apprentissage profond.</p>
<p>Tous les documents de r&eacute;f&eacute;rence peuvent &ecirc;tre consult&eacute;s ici :&nbsp;<a href="https://courses.edx.org/courses/course-v1:UMontrealX+IVADO-DL-101+1T2020/courseware/c93beedafb844f82a523b126db12bb5a/0c51ba73f73e4a728873432d94264456/7?activate_block_id=block-v1%3AUMontrealX%2BIVADO-DL-101%2B1T2020%2Btype%40vertical%2Bblock%40aa09d021276247f8a5907ad73360df45" target="_blank">2.3 - R&eacute;f&eacute;rences</a></p>