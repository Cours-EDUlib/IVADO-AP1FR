L&rsquo;entra&icirc;nement de grands RNC peut n&eacute;cessiter beaucoup de donn&eacute;es et de puissance de calcul qui ne sont pas toujours disponibles. Jeremy pr&eacute;sente l&apos;apprentissage par transfert, qui permet d&apos;utiliser des mod&egrave;les pr&eacute;-entra&icirc;n&eacute;s sur des ensembles de donn&eacute;es plus importants comme point de d&eacute;part pour l&rsquo;entra&icirc;nement sur des ensembles de donn&eacute;es plus petits. C&apos;est une astuce couramment utilis&eacute;e dans l&apos;industrie et dans les universit&eacute;s. Les poids d&apos;un mod&egrave;le appris (pr&eacute;-entra&icirc;n&eacute; par exemple sur ImageNet) peuvent &ecirc;tre utilis&eacute;s comme point de d&eacute;part pour l&rsquo;entra&icirc;nement sur une t&acirc;che distincte en aval, en ajoutant ou en supprimant simplement quelques unes des derni&egrave;res couches d&apos;un mod&egrave;le.