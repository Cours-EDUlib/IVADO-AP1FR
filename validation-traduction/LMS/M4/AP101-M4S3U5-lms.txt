<p>Mirko goes over the problem of polysemy - when a word has a completely different meaning based on context. To overcome this issue, he presents ELMo, a model that generates contextualized word embeddings using bidirectional LSTMs. He also compares the performance of ELMo with the previous State of the Art on some common NLP benchmark tasks, showing an improvement in performance across the board. Contextualized word embeddings are a key innovation in NLP and should be understood thoroughly.</p>
<p>All reverence materials can be viewed here: <a href="https://courses.edx.org/courses/course-v1:UMontrealX+IVADO-DL-101+1T2020/courseware/695f1a4ca0ea464b9b898695473ab7c6/c11ca704a56c42a0a48e05a757d2a926/9?activate_block_id=block-v1%3AUMontrealX%2BIVADO-DL-101%2B1T2020%2Btype%40vertical%2Bblock%40aaf31cffb7804d3c96af09c31d29e81b" target="_blank">4.3 - References</a></p>
<p></p>