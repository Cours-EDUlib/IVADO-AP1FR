<p>Dans ce module, Mirko présente des modèles qui traitent des problèmes basés sur des séquences de longueur variable.</p>
<p>Mirko commence par présenter les architectures de réseaux neuronaux récurrents (RNR) en définissant leur mécanisme interne, en expliquant l'algorithme de rétropropagation temporelle et en discutant des problèmes qui peuvent survenir pendant la phase d'entraînement. Puis, il présente les réseaux de neurones récurrents à longue mémoire à court terme (LMCT), une version plus sophistiquée des RNR qui peut fonctionner sur des longues séquences plus complexes.</p>
<p>Ensuite, Mirko présente les architectures séquence à séquence, qui ont permis d'améliorer considérablement des problèmes complexes tels que la traduction automatique et la synthèse automatique. Il montre également comment améliorer cette architecture grâce à l'idée fondatrice du mécanisme d'attention. Il termine la section des architectures séquence à séquence en introduisant le réseau de neurones à auto-attention original, qui se trouve au cœur de nombreux systèmes de pointe aujourd'hui.</p>
<p></p>
<p>Enfin, Mirko se concentre sur le domaine du traitement du langage naturel (TLN), en présentant quelques exemples de tâches de TLN ainsi qu'un enjeu principal dans le monde du TLN : relier les mots à la signification sémantique. Pour surmonter cette difficulté, il présente les vecteurs-mots, qui constituent un moyen très efficace de représenter les mots tout en saisissant leur signification sémantique. Mirko termine sa présentation en abordant les architectures ELMo et BERT, qui sont toutes les deux construites sur l'idée de pré-entraînement et qui ont fortement contribué à atteindre des résultats de pointe en TLN.</p>
<p></p>