<ul>
<li>Présentation de l'école d'été IVADO-Mila (diapositives originales)<br /><a href="/static/3_natural_language_processing.pdf" target="_blank">Natural Language Processing.pdf</a></li>
<li><a href="https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/" target="_blank">Get Busy with Word Embeddings – An Introduction<br /></a>(<a href="https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/" target="_blank">https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/</a>)</li>
<li><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality" target="_blank">Distributed Representations of Words and Phrases and their Compositionality<br /></a>(<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality" target="_blank">https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality</a>)</li>
<li><a href="https://arxiv.org/abs/1301.3781" target="_blank">Efficient Estimation of Word Representations in Vector Space<br /></a>(<a href="https://arxiv.org/abs/1301.3781" target="_blank">https://arxiv.org/abs/1301.3781</a>)</li>
<li><a href="https://arxiv.org/abs/1607.04606" target="_blank">Enriching Word Vectors with Subword Information<br /></a>(<a href="https://arxiv.org/abs/1607.04606" target="_blank">https://arxiv.org/abs/1607.04606</a>)</li>
<li><a href="https://arxiv.org/abs/1802.05365" target="_blank">Deep contextualized word representations<br /></a>(<a href="https://arxiv.org/abs/1802.05365" target="_blank">https://arxiv.org/abs/1802.05365</a>)</li>
<li><a href="https://towardsdatascience.com/visualizing-elmo-contextual-vectors-94168768fdaa" target="_blank">Visualizing ELMo Contextual Vectors<br /></a>(<a href="https://towardsdatascience.com/visualizing-elmo-contextual-vectors-94168768fdaa" target="_blank">https://towardsdatascience.com/visualizing-elmo-contextual-vectors-94168768fdaa</a>)</li>
<li><a href="https://arxiv.org/abs/1810.04805" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br /></a>(<a href="https://arxiv.org/abs/1810.04805" target="_blank">https://arxiv.org/abs/1810.04805</a>)</li>
<li><a href="https://arxiv.org/abs/1906.08237" target="_blank">XLNet: Generalized Autoregressive Pretraining for Language Understanding<br /></a>(<a href="https://arxiv.org/abs/1906.08237" target="_blank">https://arxiv.org/abs/1906.08237</a>)</li>
<li><a href="https://arxiv.org/abs/1905.07129" target="_blank">ERNIE: Enhanced Language Representation with Informative Entities<br /></a>(<a href="https://arxiv.org/abs/1905.07129" target="_blank">https://arxiv.org/abs/1905.07129</a>)</li>
<li><a href="https://arxiv.org/abs/1907.11692" target="_blank">RoBERTa: A Robustly Optimized BERT Pretraining Approach<br /></a>(<a href="https://arxiv.org/abs/1907.11692" target="_blank">https://arxiv.org/abs/1907.11692</a>)</li>
</ul>