<problem display_name="Question 2" markdown="We have a binary classification task with the following test labels and predicted labels by your model:&#10;True labels:              [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0]&#10;Predicted labels:      &#9;  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]&#10;Class 1 is more important than class 0, e.g., it represents when a failure can happen in a system.&#10;&#10;&gt;&gt;a) What is the accuracy of our model?&lt;&lt;&#10;&#10;(x) 16/20 {{Correct}}&#10;( ) 18/20 {{Incorrect}}&#10;( ) 10/20 {{Incorrect}}&#10;( ) 5/20 {{Incorrect}}&#10;&#10;b) The following table gives the definition of true positive, true negative, false positive and false negative:&#10;&lt;img src=&quot;/static/Module_1_Question_10_TP_FN_table.PNG&quot; alt=&quot;'Zero' represents False, and 'one' represents true. A True Negative (TN) is when the Predicted Label and the True label are both 0. A True Positive (TP) is when the Predicted label and the True label are both 1. A False Positive (FP) occurs when the True label is 0, but the Predicted label is 1. A False Negative (FN) occurs when the True Label is 1, but the Predicted label is 0.&quot;/&gt;&#10;&gt;&gt;Compute the precision= TP/(TP+FP) and recall=TP/(TP+FN).&lt;&lt;&#10;&#10;( ) precision=1/1, recall=1/2 {{Incorrect}}&#10;( ) precision=1/2, recall=⅕ {{Incorrect}}&#10;(x) precision=1/1, recall=⅕ {{Correct}}&#10;( ) precision=1/2, recall=1/1 {{Incorrect}}&#10;&#10;&gt;&gt;c) Compute the F1 score = 2 * recall * precision/(precision +recall)&lt;&lt;&#10;&#10;( ) ⅙ {{Incorrect}}&#10;(x) ⅓ {{Correct}}&#10;( ) ⅛ {{Incorrect}}&#10;( ) ½ {{Incorrect}}&#10;&#10;&gt;&gt;d) Which is the best metric to communicate your results? &lt;&lt;&#10;&#10;( ) Accuracy  {{d): Incorrect. The accuracy gives more weight to the dominant class, here class 0. If class 1 is the class of interest, then F1-score is more interesting. }}&#10;(x) F1-score {{Correct. The accuracy gives more weight to the dominant class, here class 0. If class 1 is the class of interest, then F1-score is more interesting.}}&#10;&#10;||Refer to the performance metrics mentioned in the video 1.1.2 - Machine Learning.||" max_attempts="2">
  <p>Nous avons une tâche de classification binaire avec les étiquettes d&apos;évaluation suivantes et les étiquettes prédites par votre modèle :</p>
  <p>Étiquettes vraies : [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0]</p>
  <p>Étiquettes prédites: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</p>
  <p>La classe 1 est plus importante que la classe 0, p. ex. elle représente le moment où une défaillance peut survenir dans un système.</p>
  <label>a) quelle est l’exactitude de notre modèle?</label>
  <multiplechoiceresponse>
    <choicegroup type="MultipleChoice">
      <choice correct="true">16/20 <choicehint>Bonne réponse</choicehint></choice>
      <choice correct="false">18/20 <choicehint>Mauvaise réponse</choicehint></choice>
      <choice correct="false">10/20 <choicehint>Mauvaise réponse</choicehint></choice>
      <choice correct="false">5/20 <choicehint>Mauvaise réponse</choicehint></choice>
    </choicegroup>
  </multiplechoiceresponse>
  <p>b) le tableau suivant donne la définition de « vrai positif », de « vrai négatif », de « faux positif » et de « faux négatif » :</p>
  <img src="/static/Module_1_Question_10_TP_FN_table.PNG" alt="'Zero' represents False, and 'one' represents true. A True Negative (TN) is when the Predicted Label and the True label are both 0. A True Positive (TP) is when the Predicted label and the True label are both 1. A False Positive (FP) occurs when the True label is 0, but the Predicted label is 1. A False Negative (FN) occurs when the True Label is 1, but the Predicted label is 0."/>
  <label>Calculer la précision= VP/(VP+FP) et le rappel=VP/(VP+FN).</label>
  <multiplechoiceresponse>
    <choicegroup type="MultipleChoice">
      <choice correct="false">Précision=1/1, Rappel=1/2 <choicehint>Mauvaise réponse</choicehint></choice>
      <choice correct="false">Précision=1/2, Rappel= ⅕ <choicehint>Mauvaise réponse</choicehint></choice>
      <choice correct="true">Précision=1/1, Rappel= ⅕ <choicehint>Bonne réponse</choicehint></choice>
      <choice correct="false">Précision=1/2, rappel=1/1 <choicehint>Mauvaise réponse</choicehint></choice>
    </choicegroup>
  </multiplechoiceresponse>
  <label>c) calculer le score F1 = 2 * rappel * précision/(précision + rappel)</label>
  <multiplechoiceresponse>
    <choicegroup type="MultipleChoice">
      <choice correct="false">⅙ <choicehint>mauvaise réponse</choicehint></choice>
      <choice correct="true">⅓ <choicehint>bonne réponse</choicehint></choice>
      <choice correct="false">⅛ <choicehint>mauvaise réponse</choicehint></choice>
      <choice correct="false">½ <choicehint>mauvaise réponse</choicehint></choice>
    </choicegroup>
  </multiplechoiceresponse>
  <label>d) quelle est la meilleure métrique à prendre pour communiquer vos résultats?</label>
  <multiplechoiceresponse>
    <choicegroup type="MultipleChoice">
      <choice correct="false">Exactitude <choicehint>d) : Mauvaise réponse. L’exactitude donne plus de poids à la classe dominante, ici la classe 0. Si la classe 1 est la classe d&apos;intérêt, le score F1 est plus intéressant.</choicehint></choice>
      <choice correct="true">Score F1 <choicehint>bonne réponse. L’exactitude donne plus de poids à la classe dominante, ici la classe 0. Si la classe 1 est la classe d&apos;intérêt, le score F1 est plus intéressant.</choicehint></choice>
    </choicegroup>
  </multiplechoiceresponse>
  <demandhint>
    <hint>Référez-vous aux mesures de performance dans la vidéo 1.1.2 – apprentissage automatique.</hint>
  </demandhint>
</problem>