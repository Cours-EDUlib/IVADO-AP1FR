<problem display_name="Quiz" markdown="2. Which of the following is not the reason for the need of activation functions in neural networks?&#10;&#10;(x) We can compute the scalar product between representations and patterns with activation functions.&#10;( ) We have theoretical results (e.g. universal approximation theorem) that show that activation functions increase the power of neural networks {{The universal approximation theorem states that, with certain activation functions and a big enough width size, a network with one hidden layer can approximate any continuous function on a compact subset of the real numbers in n dimensions.}}&#10;( ) Activation functions add non-linearities that are essential for increasing the model complexity {{Composing several linear layers does not increase model complexity since the composition of linear transformations is also a linear transformation.}}&#10;( ) Activation functions can modulate the representations so that the learning process is easier. {{Activation functions can restrict the range of the representation. For example, the sigmoid function changes the range of the representation to be between 0 and 1. }}&#10;&#10;" max_attempts="3" weight="0.0">
  <multiplechoiceresponse>
    <p>2. Quel énoncé n&apos;est pas une raison qui justifie que les fonctions d&apos;activation soient nécessaires dans les réseaux de neurones?</p>
    <choicegroup type="MultipleChoice">
      <choice correct="true">Nous pouvons calculer le produit scalaire entre les représentations et les formes avec des fonctions d&apos;activation.</choice>
      <choice correct="false">Nous avons des résultats théoriques (par exemple, le théorème d&apos;approximation universelle) qui montrent que les fonctions d&apos;activation augmentent la puissance des réseaux de neurones. Le théorème d&apos;approximation universelle indique que, avec certaines fonctions d&apos;activation et une largeur suffisante du réseau, un réseau avec une couche cachée peut représenter une fonction continue sur un sous-ensemble compact des nombres réels en n dimensions.</choice>
      <choice correct="false">Les fonctions d&apos;activation ajoutent des non-linéarités qui sont essentielles pour augmenter la complexité du modèle. <choicehint>la composition de plusieurs couches linéaires n&apos;augmente pas la complexité du modèle puisque la composition de transformations linéaires est également une transformation linéaire.</choicehint></choice>
      <choice correct="false">Les fonctions d&apos;activation peuvent moduler les représentations afin que le processus d&apos;apprentissage soit plus facile. <choicehint>Les fonctions d&apos;activation peuvent limiter la plage de la représentation. Par exemple, la fonction sigmoïde modifie la plage de la représentation pour qu&apos;elle soit comprise entre 0 et 1.</choicehint></choice>
    </choicegroup>
  </multiplechoiceresponse>
</problem>
