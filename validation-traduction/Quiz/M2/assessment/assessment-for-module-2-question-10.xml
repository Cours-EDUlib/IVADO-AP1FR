<problem display_name="Question 10" markdown="&gt;&gt;You are training a model with stochastic gradient descent and you observe the following learning curve on the training loss. What are the two next actions you want to test?&lt;&lt;&#10;&#10;&lt;img src=&quot;/static/Module_3_question_10_SGD.PNG&quot; height=&quot;300&quot; alt=&quot;The graph of the loss function as a function of the number of epochs. As the number of epochs increases, the loss decreases overall. But the function is very noisy, so it is not converging (i.e. it goes up and down very quickly).&quot;/&gt;&#10;&#10;[x] A. Reduce the learning rate  {{ selected: A. True. By decreasing the learning rate, we can reduce the fluctuations caused by 1) a learning rate that is too large or 2) stochasticity that is too high because of the minibatch size. }}&#10;[x] B. Reduce the momentum {{ selected: B. True. With momentum, the optimizer can overshoot and the training loss can increase}}&#10;[ ] C. Reduce the minibatch size {{ selected: C. False. Reducing the number of examples in a minibatch increases the stochasticity of the optimization problem. If the stochasticity increases, the learning process becomes more unstable.}}&#10;[ ] D. Reduce the model complexity {{ selected: D. False. The problem concerns the optimizer and reducing the model complexity changes the underlying optimization settings (we change the loss landscape in the parameter space by changing the model architecture). While it may help in some cases, you should first address the problem by changing the optimizer hyperparameters.}}&#10;&#10;{{ ((A B C)) Please choose 2 options. }}&#10;{{ ((A B D)) Please choose 2 options. }}&#10;{{ ((A C D)) Please choose 2 options. }}&#10;{{ ((B C D)) Please choose 2 options. }}&#10;{{ ((A B C D)) Please choose 2 options. }}&#10;&#10;||Try to determine if this is an optimization problem or a learning problem.||&#10;" max_attempts="2">
  <choiceresponse>
    <label>Vous êtes en train d’entraîner un modèle avec la descente de gradient stochastique et vous observez la courbe d&apos;apprentissage suivante sur la perte d&apos;entraînement. Quelles sont les deux actions suivantes que vous souhaitez faire?</label>
    <img src="/static/Module_3_question_10_SGD.PNG" height="300" alt="The graph of the loss function as a function of the number of epochs. As the number of epochs increases, the loss decreases overall. But the function is very noisy, so it is not converging (i.e. it goes up and down very quickly)."/>
    <checkboxgroup>
      <choice correct="true">A. Réduire le taux d&apos;apprentissage <choicehint selected="true">A. Vrai. En diminuant le taux d&apos;apprentissage, nous pouvons réduire les fluctuations causées par 1) un taux d&apos;apprentissage trop élevé ou 2) une stochasticité trop élevée en raison de la taille du mini-lot.</choicehint></choice>
      <choice correct="true">B. Réduire le momentum<choicehint selected="true">B.</choicehint> Vrai.<choicehint selected="true"> Avec le momentum, l&apos;optimiseur peut dépasser et la perte d&apos;entraînement peut augmenter</choicehint></choice>
      <choice correct="false">C. Réduire la taille du mini-lot <choicehint selected="true">C. Faux. La réduction du nombre d&apos;exemples dans un mini-lot augmente la stochasticité du problème d&apos;optimisation. Si la stochasticité augmente, le processus d&apos;apprentissage devient plus instable.</choicehint></choice>
      <choice correct="false">D. Réduire la complexité du modèle <choicehint selected="true">D. Faux. Le problème concerne l&apos;optimiseur et la réduction de la complexité du modèle modifie l’espace des paramètres d&apos;optimisation sous-jacents (nous modifions le paysage de perte dans l&apos;espace des paramètres en modifiant l&apos;architecture du modèle). Bien que cela puisse vous aider dans certains cas, vous devez d&apos;abord résoudre le problème en modifiant les hyperparamètres de l&apos;optimiseur.</choicehint></choice>
      <compoundhint value="A B C">Veuillez choisir 2 options.</compoundhint>
      <compoundhint value="A B D">Veuillez choisir 2 options.</compoundhint>
      <compoundhint value="A C D">Veuillez choisir 2 options.</compoundhint>
      <compoundhint value="B C D">Veuillez choisir 2 options.</compoundhint>
      <compoundhint value="A B C D">Veuillez choisir 2 options.</compoundhint>
    </checkboxgroup>
  </choiceresponse>
  <demandhint>
    <hint>Essayez de déterminer s&apos;il s&apos;agit d&apos;un problème d&apos;optimisation ou d&apos;apprentissage.</hint>
  </demandhint>
</problem>