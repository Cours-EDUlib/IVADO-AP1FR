<problem display_name="Question 4" markdown="&gt;&gt;Which of the following statements are true? || Select three options.&lt;&lt;&#10;&#10;[x] A. Stochastic Gradient Descent (SGD) uses mini-batches of data. {{selected: A. True. SGD updates parameters on subsets of the data, which increases the noisiness of updates but accelerates training.}}&#10;[x] B. Momentum improves convergence rates. {{selected: B. True. Momentum uses information from the previous parameter update to avoid “flip-flopping” around the loss landscape, leading to better results. }}&#10;[ ] C. Stochastic Gradient Descent (SGD) converges easier than Gradient Descent (which uses the whole batch). {{selected: C. Incorrect answer. SGD accelerates training by updating parameters on a small subset of data known as a mini-batch. Although this makes training faster, it also makes the updates more noisy (less accurate). Hence, convergence problems arise since the additional noise can interfere toward the end of training.}}&#10;[x] D. ADAM combines preconditioning (RMSProp) with Momentum. {{selected: D. True. ADAM is a ubiquitous optimizer in deep learning. It does much more than just preconditioning and Momentum, and is worth looking into.}}&#10;&#10;{{ ((A B C D)) Please choose 3 options. }}&#10;&#10;|| Check Unit 3.3.4. ||&#10;&#10;" max_attempts="2">
  <choiceresponse>
    <label>Parmi les affirmations suivantes, lesquelles sont vraies ?</label>
    <description>Sélectionnez trois options.</description>
    <checkboxgroup>
      <choice correct="true">A. La fonction de descente de gradient stochastique (SGD) utilise des mini-lots de données. <choicehint selected="true">A. Vrai. SGD met à jour les paramètres des sous-ensembles de données, ce qui augmente le bruit des mises à jour, mais accélère l’entraînement.</choicehint></choice>
      <choice correct="true">B. Le momentum améliore les taux de convergence. <choicehint selected="true">B. vrai. Le momentum utilise les informations de la mise à jour précédente des paramètres pour éviter les « sauts » autour du paysage des pertes, ce qui permet d&apos;obtenir de meilleurs résultats.</choicehint></choice>
      <choice correct="false">C. La descente de gradient stochastique converge plus facilement que la descente de gradient (qui utilise toutes les données). <choicehint selected="true">C. Réponse incorrecte. SGD accélère l’entraînement en mettant à jour les paramètres sur un petit sous-ensemble de données appelé mini-lot. Bien que cela accélère l&apos;entraînement, ça rend également les mises à jour plus bruyantes (moins exactes). Par conséquent, il peut y avoir des problèmes de convergence puisque le bruit supplémentaire peut interférer vers la fin de l’entraînement.</choicehint></choice>
      <choice correct="true">D. ADAM combine le préconditionnement (RMSProp) avec le momentum. <choicehint selected="true">D. Vrai. ADAM est un optimiseur omniprésent dans l&apos;apprentissage profond. Il fait beaucoup plus que juste du préconditionnement et du momentum, et ça vaut la peine de l&apos;examiner.</choicehint></choice>
      <compoundhint value="A B C D">C&apos;est en effet vrai.</compoundhint>
    </checkboxgroup>
  </choiceresponse>
  <demandhint>
    <hint>Vérifiez l&apos;unité 3.3.4.</hint>
  </demandhint>
</problem>