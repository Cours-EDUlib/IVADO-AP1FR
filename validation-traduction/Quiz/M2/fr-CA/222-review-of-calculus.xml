<problem display_name="Quiz" markdown="&gt;&gt; The derivative of the _________ activation function is itself multiplied by one minus itself. &lt;&lt;&#10;&#10;[[&#10;Softmax {{The &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function#Applications&quot; target=&quot;blank&quot;&gt;softmax&lt;/a&gt; function's derivative is different.}}&#10;Tanh {{The &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperbolic_functions#Derivatives&quot; target=&quot;blank&quot;&gt;tanh&lt;/a&gt; function's derivative is different.}}&#10;(Sigmoid) &#10;ReLU {{The &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Potential_problems&quot; target=&quot;blank&quot;&gt;ReLU&lt;/a&gt; function's derivative is different.}}&#10;]]&#10;&#10;||Check the video from 5:50 to 6:06.||&#10;" max_attempts="3" weight="0.0">
  <optionresponse>
    <label>La dérivée de la fonction d&apos;activation ________ est elle-même multipliée par un moins.</label>
    <optioninput>
      <option correct="False">SoftMax <optionhint>la dérivée de la fonction <a href="https://en.wikipedia.org/wiki/Softmax_function#Applications" target="blank">softmax</a> est différente.</optionhint></option>
      <option correct="False">Tanh <optionhint>la dérivée de la fonction <a href="https://en.wikipedia.org/wiki/Hyperbolic_functions#Derivatives" target="blank">tanh</a> est différente.</optionhint></option>
      <option correct="True">Sigmoïde</option>
      <option correct="False">ReLU <optionhint>la dérivée de la fonction <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Potential_problems" target="blank">ReLU</a> est différente.</optionhint></option>
    </optioninput>
  </optionresponse>
  <demandhint>
    <hint>Regardez la vidéo de 05:50 à 06:06.</hint>
  </demandhint>
</problem>