<problem>
<multiplechoiceresponse>
  <label>Dans les RNR, le gradient peut disparaître ou exploser parce qu&apos;une matrice de poids est multipliée par elle-même plusieurs fois dans le processus de calcul du gradient.</label>
<description>Sélectionnez vrai ou faux.</description>
<choicegroup type="MultipleChoice">
    <choice correct="true">Vrai <choicehint>C&apos;est un problème auquel les RNR sont confrontés. Le problème de l&apos;explosion du gradient peut être résolu par un petit ajustement comme l&apos;écrêtage de gradient, mais le problème de dissipation du gradient est plus difficile à résoudre.</choicehint></choice>
    <choice correct="false">Faux <choicehint>Le problème de dissipation du gradient se produit lorsqu&apos;une matrice de poids avec de petites valeurs est multipliée par elle-même plusieurs fois dans le processus de calcul du gradient</choicehint>.<choicehint> Le problème de l&apos;explosion du gradient se produit lorsqu&apos;une matrice de poids avec de grandes valeurs est multipliée par elle-même plusieurs fois dans le processus de calcul du gradient. Comme les RNR utilisent les mêmes matrices de poids à chaque pas de temps, ils sont confrontés à ces problèmes.</choicehint></choice>
  </choicegroup>
</multiplechoiceresponse>
</problem>