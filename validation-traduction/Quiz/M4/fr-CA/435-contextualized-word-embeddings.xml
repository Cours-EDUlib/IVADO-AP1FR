<problem display_name="Quiz" markdown="&gt;&gt; __________ is a solution for the polysemy problem in word embeddings.  &lt;&lt;&#10;&#10;[[&#10;Language modelling {{ Language modelling is not a direct solution to the polysemy problem; however, we can partially fix the issue by using it in a certain way }}&#10;Pre-training {{ Pre-training is a general concept, but a specific type of it can be used to tackle the polysemy problem }}&#10;(Contextualized word embeddings) {{Contextualized word embeddings use the information of nearby tokens to generate the embedding, thereby giving the same word a different vector representation based on the context in which that word is used.}}&#10;]]&#10;&#10;" max_attempts="3" weight="0.0">
  <optionresponse>
    <label>__________ est une solution au problème de la polysémie dans les vecteurs-mots.</label>
    <optioninput>
      <option correct="False">La modélisation du langage <optionhint>La modélisation du langage n&apos;est pas une solution directe au problème de la polysémie.</optionhint> Cependant, nous pouvons partiellement résoudre le problème en l&apos;utilisant d&apos;une certaine manière.</option>
      <option correct="False">Le préentraînement <optionhint>Le préentraînement est un concept général, mais un type spécifique de préentraînement peut être utilisé pour s&apos;attaquer au problème de la polysémie</optionhint></option>
      <option correct="True">Les vecteurs-mots contextualisés <optionhint>Les vecteurs-mots contextualisés utilisent les informations des unités lexicales voisines pour générer le plongement, donnant ainsi au même mot une représentation vectorielle différente en fonction du contexte dans lequel il est utilisé.</optionhint></option>
    </optioninput>
  </optionresponse>
</problem>