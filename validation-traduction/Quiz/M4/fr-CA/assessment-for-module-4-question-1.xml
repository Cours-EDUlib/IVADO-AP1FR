<problem display_name="Question 1" markdown="&gt;&gt;Of the following statements, select those three that apply to RNNs.&#10;|| Please choose 3 options.||&lt;&lt;&#10;&#10;[ ] A. RNNs are made deep by increasing the length of the inputs. {{selected: A. Incorrect answer, RNNs are made deep by “stacking” RNNs, feeding the hidden states of one RNN to another RNN on top. RNNs are specifically designed to handle variable-sized inputs, so inputs are likely to vary in size (although this is not a necessary feature of the inputs).}}&#10;[x] B. RNNs were designed to accommodate variable-sized inputs. {{ selected: B. Correct, MLPs and CNNs were not designed for variable-sized inputs, which motivated the development of the RNN. }}&#10;[x] C. RNNs can be trained using backpropagation but can encounter problems such as Vanishing Gradient. {{selected: C. Correct, various modifications to the backpropagation algorithm have been proposed for RNNs, but the guiding principle of gradient descent still remains.}}&#10;[x] D. RNNs have a hidden state in which information from all previous elements of the input sequence can be retained. {{selected: D. Correct, the hidden state is continuously updated and retains relevant information for the current task, although an information bottleneck can occur for longer sequences.}}&#10;&#10;{{ ((A B C D)) Please choose 3 options. }}&#10;&#10;||What happens if you “stack” RNNs together?||&#10;&#10;" max_attempts="2">
  <choiceresponse>
    <label>Parmi les affirmations suivantes, sélectionnez les trois qui s&apos;appliquent aux RNR (RNN).</label>
    <description>Veuillez choisir 3 options.</description>
    <checkboxgroup>
      <choice correct="false">A. On peut rendre les RNR profonds en augmentant la longueur des entrées. <choicehint selected="true">A. Incorrect, les RNR sont rendus profonds en « empilant » les RNR, ce qui permet d’envoyer les états cachés d&apos;un RNR à un autre RNR par-dessus. Les RNR sont spécifiquement conçus pour traiter des entrées de taille variable, de sorte que les entrées sont susceptibles de varier en taille (bien que ce ne soit pas une caractéristique nécessaire des entrées).</choicehint></choice>
      <choice correct="true">B. Les RNR ont été conçus pour s&apos;adapter à des données d’entrée de taille variable. <choicehint selected="true">B. Correct, les perceptrons multicouches et les CNN n&apos;ont pas été conçus pour des données d’entrée de taille variable, ce qui a motivé le développement du RNR.</choicehint></choice>
      <choice correct="true">C. L&apos;entraînement des RNR peut être fait par rétropropagation, mais des problèmes tels que la disparition du gradient peuvent survenir. <choicehint selected="true">C. Correct, diverses modifications de l&apos;algorithme de rétropropagation ont été proposées pour les RNR, mais il reste le principe directeur de la descente de gradient.</choicehint></choice>
      <choice correct="true">D. Les RNR ont un état caché dans lequel les informations de tous les éléments précédents de la séquence d&apos;entrée peuvent être conservées. <choicehint selected="true">D. Correct, l&apos;état caché est continuellement mis à jour et conserve les informations pertinentes pour la tâche en cours, bien qu’il existe un risque de goulot d&apos;étranglement de l&apos;information pour les séquences plus longues.</choicehint></choice>
      <compoundhint value="A B C D">Veuillez choisir 3 options.</compoundhint>
    </checkboxgroup>
  </choiceresponse>
  <demandhint>
    <hint>Que se passe-t-il si vous « empilez » les RNN ensemble ?</hint>
  </demandhint>
</problem>