<problem display_name="Question 7" markdown="&gt;&gt;How does Self-Attention replace RNNs? &lt;&lt;&#10;&#10;( ) By allowing the decoder to extract information from the encoder more easily.{{Self-Attention uses the Attention Mechanism on the same sequence (as the name implies). Thus, there is no encoder/decoder structure.}}&#10;(x) By using the Attention Mechanism to find which elements in the sequence are important for the context of the current element. {{Correct! Self-Attention is used to produce an updated contextualized encoded representation of the input sequence, which will indicate which elements in the sequence are important for the context of the current element.}}&#10;( ) By using the Attention Mechanism to decide when to stop producing the output. {{Self-Attention is used to produce an updated contextualized encoded representation of the input sequence, not to produce a new sequence in an autoregressive way.}}&#10;( ) By computing dot-products over elements in the sequence to find out which elements are similar. {{Self-Attention is indeed based on dot-products, but the goal is to produce an updated contextualized encoded representation of the input sequence, not to find similar elements.}}&#10;" max_attempts="2">
  <multiplechoiceresponse>
    <label>Comment le mécanisme d’auto-attention remplace-t-il les RNR?</label>
    <choicegroup type="MultipleChoice">
      <choice correct="false">En permettant au décodeur d&apos;extraire plus facilement des informations de l’encodeur. <choicehint>L&apos;auto-attention utilise le mécanisme d&apos;attention sur la même séquence (comme son nom l&apos;indique). Il n&apos;y a donc pas de structure d’encodeur/décodeur.</choicehint></choice>
      <choice correct="true">En utilisant le mécanisme d&apos;attention pour trouver quels éléments de la séquence sont importants pour le contexte de l&apos;élément en cours. <choicehint>Correct! L&apos;auto-attention est utilisée pour produire une représentation encodée contextualisée et actualisée de la séquence d&apos;entrée, qui indiquera quels éléments de la séquence sont importants pour le contexte de l&apos;élément en cours.</choicehint></choice>
      <choice correct="false">By using the Attention Mechanism to decide when to stop producing the output. <choicehint>En utilisant le mécanisme d&apos;attention pour décider du moment où il faut arrêter de produire la sortie L&apos;auto-attention est utilisée pour produire une représentation encodée contextualisée et actualisée de la séquence d&apos;entrée, et non pour produire une nouvelle séquence de manière autorégressive.</choicehint></choice>
      <choice correct="false">En calculant les produits scalaires des éléments de la séquence pour savoir quels éléments sont similaires. <choicehint>L&apos;auto-attention est en effet basée sur des produits scalaires, mais le but est de produire une représentation codée contextualisée et actualisée de la séquence d&apos;entrée, et non de trouver des éléments similaires.</choicehint></choice>
    </choicegroup>
  </multiplechoiceresponse>
</problem>