<problem>
<choiceresponse>
  <label>Sélectionnez les énoncés qui s&apos;appliquent à l&apos;équité en apprentissage automatique.</label>
<description>Veuillez choisir deux options.</description>
<checkboxgroup>
    <choice correct="true">A. Le théorème d&apos;impossibilité stipule qu&apos;il est impossible de satisfaire simultanément toutes les définitions de l&apos;équité. <choicehint selected="true">A. Correct, l&apos;équité n&apos;est pas un concept général, et elle implique souvent de nombreux compromis, par exemple, entre l&apos;équité individuelle et l&apos;équité de groupe. Souvent, un professionnel en apprentissage automatique devra choisir les définitions de l&apos;équité qu&apos;il est prêt à satisfaire. L&apos;important est de se rappeler que ces choix doivent être faits explicitement, avec une explication de la manière dont ils ont été faits.</choicehint></choice>
    <choice correct="false">B. L&apos;équité est un concept général qui n&apos;a pas de définition mathématique. <choicehint selected="true">B. Incorrect, cet énoncé est faux. Premièrement, l&apos;équité n&apos;est pas un concept général. L’équité est subjective, c&apos;est pourquoi il existe de nombreux concepts différents d&apos;équité. Deuxièmement, de nombreux concepts d&apos;équité ont une définition mathématique, ce qui permet à un professionnel apprentissage automatique de les inclure directement dans le pipeline de l&apos;apprentissage automatique.</choicehint></choice>
    <choice correct="false">C. La précision élevée globale du modèle garantit que le modèle est juste. <choicehint selected="true">C. Incorrect, l&apos;exactitude globale d&apos;un modèle ne garantit pas que tous les sous-ensembles de données sont traités de la même manière. Un modèle pourrait afficher une précision globale élevée, mais afficher des précisions médiocres sur des sous-groupes. Par exemple, un modèle pourrait classer correctement les images des hommes par rapport aux femmes (classification par sexe) avec une exactitude élevée, mais obtenir de mauvais résultats dans le sous-groupe des femmes afro-américaines (classification par sexe et race).</choicehint></choice>
    <choice correct="true">D. Lorsque des contraintes d&apos;équité sont ajoutées à une tâche d&apos;apprentissage automatique, celle-ci peut devenir plus difficile à résoudre. <choicehint selected="true">D. Correct, l’ajout de l’équité peut entraîner des contraintes supplémentaires à la partie optimisation de l&apos;apprentissage automatique. Il est donc plus difficile pour l&apos;optimiseur de paramètres de trouver une bonne solution. Le développement de solutions créatives concernant ces contraintes supplémentaires est un domaine de recherche actif et représente une belle occasion.</choicehint></choice>
    <compoundhint value="A B C">Veuillez choisir 2 options.</compoundhint>
    <compoundhint value="A B D">Veuillez choisir 2 options.</compoundhint>
    <compoundhint value="A C D">Veuillez choisir 2 options.</compoundhint>
    <compoundhint value="B C D">Veuillez choisir 2 options.</compoundhint>
    <compoundhint value="A B C D">Veuillez choisir 2 options.</compoundhint>
  </checkboxgroup>
</choiceresponse>
</problem>