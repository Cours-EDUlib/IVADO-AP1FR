<problem>
<multiplechoiceresponse>
  <label>Seul un des énoncés suivants sur le biais et la discrimination en apprentissage automatique est faux. Veuillez sélectionner l’énoncé.</label>
<choicegroup type="MultipleChoice">
    <choice correct="false">Le biais dans les données provient d&apos;une multitude de sources possibles. <choicehint>Le biais dans les données peut provenir de sources telles que le biais de population, le biais comportemental, le biais de production de contenu, le biais de liaison et le biais temporel.</choicehint></choice>
    <choice correct="false">Les méthodes de nettoyage des données comme le massage des données ne permettent pas de résoudre le problème du biais dans les données. <choicehint>Les méthodes de nettoyage des données comme le massage des données sont dangereuses car elles modifient la répartition des données en fonction de la méthode utilisée. La plupart de ces méthodes présentent des inconvénients limitatifs qui les rendent dangereuses à utiliser et elles ne devraient pas être utilisées sans une expertise considérable.</choicehint></choice>
    <choice correct="false">Le biais dans les données et la qualité des données ne sont pas des termes interchangeables. <choicehint>Bien que ces termes soient liés, le biais dans les données est une distorsion systématique des données qui compromet leur utilisation pour une tâche. Il s&apos;agit d&apos;un problème beaucoup plus spécifique que celui de la qualité des données, qui peut être de caractère aléatoire. Les problèmes de biais dans les données entraînent des problèmes de qualité des données, l&apos;inverse n&apos;est pas nécessairement vrai.</choicehint></choice>
    <choice correct="true">Les problèmes d&apos;équité surviennent pendant les phases de prétraitement et de traitement, mais pas pendant la phase de post-traitement. <choicehint>Comme de nombreux systèmes d&apos;apprentissage automatique sont complexes, il peut être difficile de justifier leur sorties. Il peut donc être difficile de déterminer si un modèle est équitable ou non. Les professionnels en apprentissage automatique peuvent recourir à des méthodes de boîte noire : au lieu d&apos;analyser le fonctionnement interne d’un modèle, ils fournissent au modèle des exemples variés pour voir si celui-ci fait de la discrimination sur la base d&apos;attributs sensibles.</choicehint></choice>
  </choicegroup>
</multiplechoiceresponse>
</problem>