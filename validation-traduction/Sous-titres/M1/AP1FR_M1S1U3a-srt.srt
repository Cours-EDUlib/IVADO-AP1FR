0
00:00:13,770 --> 00:00:22,360
Maintenant, passons à la partie probablement la plus difficile du travail d'un

1
00:00:22,360 --> 00:00:27,250
scientifique des données: respecter un protocole expérimental

2
00:00:27,250 --> 00:00:32,140
ou utiliser un protocole expérimental. J'ai d'abord besoin d'un concept

3
00:00:32,140 --> 00:00:38,500
appelé "hyperparamètre": le modèle que nous allons entraîner a des paramètres

4
00:00:38,500 --> 00:00:42,640
et ces paramètres seront ajustés automatiquement par un algorithme d'apprentissage.

5
00:00:42,640 --> 00:00:47,620
Ce sont des paramètres faciles à ajuster et ils seront ajustés automatiquement. Il

6
00:00:47,620 --> 00:00:53,320
y a d'autres paramètres de l'algorithme d'apprentissage qui sont plus difficiles

7
00:00:53,320 --> 00:00:58,390
à ajuster. Généralement ce sera l'humain qui entraînera les hyperparamètres, tels que

8
00:00:58,390 --> 00:01:02,700
le choix de l'architecture et la taille du pas d'apprentissage de l'optimiseur.

9
00:01:02,700 --> 00:01:12,070
Ça peut être plusieurs choses qui sont difficiles à optimiser automatiquement: il y a

10
00:01:12,070 --> 00:01:16,090
des algorithmes de réglage d'hyperparamètres. Au MILA, nous faisons des recherches

11
00:01:16,090 --> 00:01:22,600
sur une plate-forme appelée Orion. Orion implémente différents algorithmes

12
00:01:22,600 --> 00:01:27,849
de réglage d'hyperparamètres. Nous avons comparé ces différents algorithmes sur cette

13
00:01:27,849 --> 00:01:33,190
plate-forme. Elle est disponible gratuitement, mais cela ne résoudra pas le problème

14
00:01:33,190 --> 00:01:39,580
de l'optimisation des hyperparamètres: cet espace à optimiser est trop

15
00:01:39,580 --> 00:01:43,599
grand et un bon scientifique des données identifie certains hyperparamètres

16
00:01:43,599 --> 00:01:48,759
facilement optimisés par ces algorithmes de réglage d'hyperparamètres.

17
00:01:48,759 --> 00:01:54,280
Pour les autres hyperparamètres, on utilise notre intuition, notre connaissance, notre

18
00:01:54,280 --> 00:01:59,830
expérience pour l'adapter par nous-mêmes. C'est de même que les étudiants travaillent

19
00:01:59,830 --> 00:02:04,599
pour obtenir des résultats de pointe sur différentes tâches: ils utilisent un mélange

20
00:02:04,599 --> 00:02:12,910
des deux et cela fait partie de l'expertise en apprentissage automatique. Quelle est

21
00:02:12,910 --> 00:02:19,629
la pipeline d'expérimentation? Tout d'abord nous sommes des scientifiques de données.

22
00:02:19,629 --> 00:02:24,340
En tant que scientifiques, nous voulons faire de la science: ça signifie que j'aurai

23
00:02:24,340 --> 00:02:28,810
une hypothèse sur la performance probable de cette architecture de modèle sur mes

24
00:02:28,810 --> 00:02:33,879
données et je veux prouver, je veux montrer pour démontrer que c'est le cas. Donc, je

25
00:02:33,879 --> 00:02:39,099
définirai une expérience, puis je récupérerai mes données. Je dois apporter ces

26
00:02:39,099 --> 00:02:44,860
données efficacement d'un stockage permanent à l'appareil qui fera l'expérimentation

27
00:02:44,860 --> 00:02:51,010
Ici, cela s'appelle un GPU: nous coderons le pipeline d'entrée. La troisième

28
00:02:51,010 --> 00:02:55,930
étape est la visualisation des expériences. Je veux visualiser quels sont

29
00:02:55,930 --> 00:03:00,010
les hyperparamètres, quelles sont les mesures de performance, quelle est

30
00:03:00,010 --> 00:03:04,480
l'évolution de la formation du modèle. Nous allons donc consacrer du temps au

31
00:03:04,480 --> 00:03:11,890
développement de visualisations de ces informations. La quatrième étape consiste à

32
00:03:11,890 --> 00:03:16,930
créer la boucle de formation. C'est un code qui est assez standard, que vous pouvez

33
00:03:16,930 --> 00:03:20,440
réutiliser dans différentes expériences. Nous vous fournirons un exemple de

34
00:03:20,440 --> 00:03:26,049
boucle de formation dans les tutoriels à la fin du module. Si vous souhaitez optimiser

35
00:03:26,049 --> 00:03:32,470
certains hyperparamètres, vous devez également développer la boucle de réglage

36
00:03:32,470 --> 00:03:37,870
d' hyperparamètre. Qu'est-ce qu'une expérience? Dans l'expérience, nous allons

37
00:03:37,870 --> 00:03:44,410
identifier tous les degrés de liberté: ce sont les valeurs que je dois fournir pour

38
00:03:44,410 --> 00:03:48,880
pouvoir reproduire l'expérience. Disons que quelqu'un vient me voir et me dit: "j'ai une

39
00:03:48,880 --> 00:03:55,989
précision de 95 % sur cette tâche." Je dirai: "ok je veux reproduire votre résultat je

40
00:03:55,989 --> 00:04:00,870
veux aussi obtenir une précision de 95 pour cent." De quoi ai-je besoin pour

41
00:04:00,870 --> 00:04:07,150
reproduire l'expérience pour être sûr qu'il n'y a pas d'erreurs et

42
00:04:07,150 --> 00:04:12,639
que ce modèle a cette performance sur ce jeu de données? Les premières choses

43
00:04:12,639 --> 00:04:18,280
à spécifier sont les ordinateurs: quel ordinateur vous avez utilisé et quel est le

44
00:04:18,280 --> 00:04:22,960
modèle du GPU que vous avez utilisé. Les performances peuvent changer en fonction

45
00:04:22,960 --> 00:04:28,240
du modèle du GPU, pas beaucoup, mais il est important de le savoir. Une autre chose

46
00:04:28,240 --> 00:04:32,380
à spécifier est la version des bibliothèques que vous verrez dans l'après-midi, même

47
00:04:32,380 --> 00:04:37,930
si vous utilisez juste un environnement de très haut niveau comme PyTorch.

48
00:04:37,930 --> 00:04:42,460
PyTorch est vraiment un écosystème de bibliothèques avec différentes bibliothèques à

49
00:04:42,460 --> 00:04:47,680
différents niveaux de détail et la plus élémentaire implémente l'algèbre linéaire

50
00:04:47,680 --> 00:04:52,870
sur le GPU. Si vous utilisez différentes versions de ces bibliothèques, les

51
00:04:52,870 --> 00:04:58,300
performances peuvent varier considérablement en termes de vitesse de calcul, mais

52
00:04:58,300 --> 00:05:04,150
également en termes de la reproductibilité. La version de vos données: lorsque vous

53
00:05:04,150 --> 00:05:09,009
dites: "j'ai une précision de 95%", c'est avec un ensemble de données spécifique que

54
00:05:09,009 --> 00:05:13,419
vous avez évalué votre mesure de performance. Si vous recueillez de nouvelles données

55
00:05:13,419 --> 00:05:17,860
chaque jour et que vous oubliez quel ensemble de données vous utilisez, il peut être

56
00:05:17,860 --> 00:05:24,520
difficile de reproduire le résultat. On peut sauvegarder l'ensemble de données à un

57
00:05:24,520 --> 00:05:29,289
moment pour bien reproduire l'expérience. La version du code d'entraînement, c'est

58
00:05:29,289 --> 00:05:34,330
 c'est facile. C'est comme un projet de génie logiciel: on peut utiliser GitHub pour

59
00:05:34,330 --> 00:05:41,590
enregistrer le code et nous avons une version du code qui a été utilisé pour cette

60
00:05:41,590 --> 00:05:46,780
expérience. Nous entrons ensuite un peu plus dans le contexte de l'AA:

61
00:05:46,780 --> 00:05:52,300
quelle était l'architecture utilisée? Une architecture peut être définie par plusieurs

62
00:05:52,300 --> 00:05:57,400
modules et il peut être difficile de se rappeler exactement quelle architecture

63
00:05:57,400 --> 00:06:03,220
j'ai utilisé et quelle était la connexion entre ces deux modules dans l'architecture.

64
00:06:03,220 --> 00:06:08,800
Ça peut être très difficile si vous n'enregistrez pas ces informations. Il faut aussi

65
00:06:08,800 --> 00:06:13,539
les valeurs utilisées des hyperparamètres. Nous utilisons beaucoup d'aléa

66
00:06:13,539 --> 00:06:20,650
pour entraîner des réseaux de neurones profonds. L'aléa n'est pas purement aléatoire:

67
00:06:20,650 --> 00:06:26,349
il est simulé par un générateur aléatoire qui génère une

68
00:06:26,349 --> 00:06:33,430
séquence de valeurs pseudo aléatoires. Le générateur aléatoire est paramétré par

69
00:06:33,430 --> 00:06:38,710
ce que nous appelons une suite aléatoire. Si vous oubliez d'enregistrer la

70
00:06:38,710 --> 00:06:45,280
suite aléatoire , alors une expérience peut être non-reproductible. Maintenant, vous

71
00:06:45,280 --> 00:06:49,389
pensez surement: il y a beaucoup à enregistrer. Peut-être que vous faites déjà de l'AA

72
00:06:49,389 --> 00:06:54,789
et que vous n'enregistrez pas toutes ces valeurs et en fait, selon

73
00:06:54,789 --> 00:07:01,599
l'expérience, ne pas connaître exactement le GPU n'aura pas un fort impact sur la

74
00:07:01,599 --> 00:07:06,309
reproduction d'une valeur exacte de la métrique de performance. Donc, il y aura comme

75
00:07:06,309 --> 00:07:11,439
une incertitude autour du 95% que je veux reproduire. Si j'en ai 93 c'est pas

76
00:07:11,439 --> 00:07:16,269
si mal, si j'en ai 97 c'est pas si mal: ça veut juste dire que j'ai du bruit autour de

77
00:07:16,269 --> 00:07:24,759
ce score. Si vous voulez vraiment avoir un protocole expérimental où vous

78
00:07:24,759 --> 00:07:30,689
enregistrez toutes ces informations, je vous conseille d'utiliser une

79
00:07:30,689 --> 00:07:38,829
plateforme d'AA pour gérer vos expériences comme MLflow. Vous pouvez voir ici que sur

80
00:07:38,829 --> 00:07:44,379
MLflow, je peux avoir une expérience par ligne: j'aurai la date et l'heure

81
00:07:44,379 --> 00:07:50,949
que je débute cette expérience, l'utilisateur qui a débuté cette expérience, le code

82
00:07:50,949 --> 00:07:56,469
source, une version du code et/ou de l'ensemble de données, et les valeurs des

83
00:07:56,469 --> 00:08:01,389
hyperparamètres que je veux essayer. Ici, il n'y a que deux colonnes qui

84
00:08:01,389 --> 00:08:07,389
m'intéressent: si je modifie l'une des deux, comment ma métrique de performance

85
00:08:07,389 --> 00:08:11,499
va-t-elle changer. Ici, j'utilise trois métriques de performance, vous n'avez pas à en

86
00:08:11,499 --> 00:08:18,489
choisir une seule, vous pouvez en choisir autant que vous le souhaitez si ça vous est

87
00:08:18,489 --> 00:08:24,219
utile. N'utilisez pas des mesures de performance seulement que parce qu'elles sont

88
00:08:24,219 --> 00:08:32,889
dans la littérature. Si vous avez vraiment besoin de mesurer une propriété avec,

89
00:08:32,889 --> 00:08:38,709
utilisez-la. Vous pensez sûrement que c'est un peu trop d'enregistrer toutes ces

90
00:08:38,709 --> 00:08:45,699
informations, mais au début, j'ai dit que lorsque nous spécifions une tâche, nous

91
00:08:45,699 --> 00:08:49,930
demandons ce que nous voulons réaliser. Nous ne définissons pas comment y parvenir,

92
00:08:49,930 --> 00:08:55,029
c'est l'algorithme d' apprentissage qui trouvera comment réaliser la tâche. Pour une

93
00:08:55,029 --> 00:08:59,319
IA, tout les moyens sont bons pour réaliser une tâche. S'il existe un moyen facile de

94
00:08:59,319 --> 00:09:03,330
réaliser la tâche car il y a un bug dans votre jeu de données ou un bug

95
00:09:03,330 --> 00:09:10,589
dans un simulateur, vous pouvez être sûr que l'IA utilisera cette faille, cette bug

96
00:09:10,589 --> 00:09:17,519
pour maximiser la métrique de performance. L'un des exemples les plus célèbres est

97
00:09:17,519 --> 00:09:21,990
l'apprentissage par renforcement: un algorithme apprend à jouer à un

98
00:09:21,990 --> 00:09:26,670
jeu d'Atari appelé Montezuma, vous avez donc ce jeu Atari où nous avons un agent

99
00:09:26,670 --> 00:09:32,550
qui joue à ce jeu et l'agent est assez bon à ce stade. Ce que les scientifiques des

100
00:09:32,550 --> 00:09:38,310
données ont fini par découvrir, c'est que nous avons demandé à l'agent de maximiser le

101
00:09:38,310 --> 00:09:45,149
score, mais l'algorithme a trouvé un bogue dans le jeu qui n'avait jamais été trouvé

102
00:09:45,149 --> 00:09:53,600
auparavant pendant je pense 20 ou 30 ans de personnes jouant à ce jeu.

103
00:09:53,600 --> 00:09:59,399
nous pouvons voir qu'il a trouvé un bug et qu'il a amené le jeu dans un état de bogue

104
00:09:59,399 --> 00:10:05,010
où les diamants apparaissent toujours. Il est donc très facile de maximiser

105
00:10:05,010 --> 00:10:09,240
le score parce que je dois juste sauter et attraper ça, je n'ai pas toute la

106
00:10:09,240 --> 00:10:14,339
difficulté du jeu initial et l'algorithme d'apprentissage l'a trouvé par

107
00:10:14,339 --> 00:10:21,300
lui-même. Il exploite ce bug pour maximiser le score, c'est un

108
00:10:21,300 --> 00:10:27,870
exemple impressionnant mais vraiment ça arrive beaucoup dans la littérature.

109
00:10:27,870 --> 00:10:34,850
Les gens essaient de mieux comprendre comment l'algorithme résout la tâche

110
00:10:34,850 --> 00:10:39,380
après l'entraînement, après qu'on est sûr qu'il a de bonnes performances.

111
00:10:39,380 --> 00:10:44,610
Intuitivement si je veux reconnaître s'il y a un chien dans l'image et

112
00:10:44,610 --> 00:10:51,390
l'algorithme me dit qu'il y a un chien en raison des caractéristiques de cette

113
00:10:51,390 --> 00:10:57,420
zone de l'image, je peux être sûr que l'algorithme sait ce qu'est un chien

114
00:10:57,420 --> 00:11:03,390
dans l'image. Par contre, si l'algorithme me donne des attributs qui dépendent sur cet

115
00:11:03,390 --> 00:11:08,010
objet car c'est un objet plus simple et c'est fortement corrélé à la présence d'un

116
00:11:08,010 --> 00:11:14,339
chien, je peux me demander si mon modèle triche ou non.

117
00:11:14,339 --> 00:11:18,020
Cela s'appelle vérifier la cartographie des caractéristiques saillantes pour

118
00:11:18,020 --> 00:11:25,190
interpréter comment le modèle résout la tâche. C'est difficile, ça dépend vraiment du

119
00:11:25,190 --> 00:11:30,260
type de données et du type de modèles utilisés: la cartographie des caractéristiques

120
00:11:30,260 --> 00:11:34,340
saillantes est quelque chose qui attire beaucoup de recherche pour avoir une bonne

121
00:11:34,340 --> 00:11:39,500
technique pour calculer la la cartographie des caractéristiques saillantes. Il faut

122
00:11:39,500 --> 00:11:44,750
également être critique: une valeur de métrique de performance haute n'indique pas que

123
00:11:44,750 --> 00:11:49,220
votre travail est fini, vous critiquer l'algorithme et essayer de trouver des exemples

124
00:11:49,220 --> 00:11:55,010
où le modèle fonctionne mal. L'une des premières étapes pour essayer de contester

125
00:11:55,010 --> 00:12:02,390
le modèle est de diviser votre ensemble de données avec des exemples en deux

126
00:12:02,390 --> 00:12:06,860
ensembles de données différents: un ensemble de données sera l'ensemble d'entraînement

127
00:12:06,860 --> 00:12:12,320
et l'autre est l'ensemble de test. L'algorithme d'apprentissage utilise l'ensemble

128
00:12:12,320 --> 00:12:18,560
d'entraînement pour apprendre les paramètres du modèle par essai-erreur. Pour apprendre

129
00:12:18,560 --> 00:12:25,000
à partir de cet ensemble d'entraînement, vous ne devez jamais montrer l'ensemble de

130
00:12:25,000 --> 00:12:30,770
test à l'algorithme d'apprentissage durant l'entraînement, L'ensemble de test sert à

131
00:12:30,770 --> 00:12:36,380
l'évaluation: on utilise des exemples que le modèle n'a pas vus durant l'entraînement

132
00:12:36,380 --> 00:12:44,120
pour tester s'il est capable de généraliser. Une façon de tricher sur une tâche est de

133
00:12:44,120 --> 00:12:48,590
mémoriser tous les exemples dans l'ensemble de formation: je mémorise simplement tout

134
00:12:48,590 --> 00:12:54,230
et j'aurai une très haute métrique de performance sur mon ensemble d'entraînement.

135
00:12:54,230 --> 00:12:59,150
Si je lui donne un nouvel exemple, puisque qu'il a mémorisé et ne comprend pas la

136
00:12:59,150 --> 00:13:04,820
tâche, il ne sera pas en mesure de bien classer l'ensemble de test. C'est un moyen de

137
00:13:04,820 --> 00:13:10,940
détecter si mon modèle généralise. Il est essentiel d'utiliser cette division

138
00:13:10,940 --> 00:13:19,190
d'ensembles pour l'entraînement et l'évaluation parce que s'il triche sur

139
00:13:19,190 --> 00:13:23,510
l'ensemble de formation, vous serez convaincu que les performances sont

140
00:13:23,510 --> 00:13:27,200
bonnes et lorsque vous déploierez le modèle en production où de nouvelles données

141
00:13:27,200 --> 00:13:31,490
qui ne sont pas dans l'ensemble de formation arriveront, votre modèle

142
00:13:31,490 --> 00:13:35,839
ne généralisera pas. En fait, vous étiez juste trop confiant quant aux performances

143
00:13:35,839 --> 00:13:42,950
du modèle. Je parlais aussi d'hyperparamètres sur lesquels on peut faire plusieurs

144
00:13:42,950 --> 00:13:48,620
expériences et choisir le meilleur modèle en fonction des hyperparamètres.

145
00:13:48,620 --> 00:13:54,350
C'est une bonne idée de choisir le meilleur modèle d'une liste d'hyperparamètres sur

146
00:13:54,350 --> 00:13:59,420
un autre ensemble appelé ensemble de validation: l'ensemble d'entraînement est utilisé

147
00:13:59,420 --> 00:14:04,700
pour apprendre les paramètres du modèle. Je ferai plusieurs expériences sur l'ensemble

148
00:14:04,700 --> 00:14:09,709
d'entraînement, puis j'utiliserai l'ensemble de validation pour choisir la meilleure

149
00:14:09,709 --> 00:14:15,529
expérience. Je testerai une dernière fois le meilleur modèle sur un autre ensemble:

150
00:14:15,529 --> 00:14:22,459
l'ensemble de test. Ici, ce sont les ratios du nombre d'exemples de mon ensemble de

151
00:14:22,459 --> 00:14:26,839
données pour faire cette répartition. Ici, on a 60% pour l'ensemble d'entraînement, et

152
00:14:26,839 --> 00:14:32,209
20% - 20%; cela dépend du nombre d'exemples, de points de données que

153
00:14:32,209 --> 00:14:35,290
vous avez dans votre ensemble de données.

