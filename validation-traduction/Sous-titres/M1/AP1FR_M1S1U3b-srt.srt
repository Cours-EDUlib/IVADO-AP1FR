0
00:00:13,179 --> 00:00:18,770
Je fais donc une division: je prends quelques exemples et je partitionne ces exemples

1
00:00:18,770 --> 00:00:24,680
en deux ensembles ici et un autre ensemble ici. Puis, je peux diviser à nouveau cette

2
00:00:24,680 --> 00:00:29,929
partition en ensemble d'entraînement et en ensemble de validation. On le fait en deux

3
00:00:29,929 --> 00:00:35,150
étapes parce que je vais montrer vous une méthode utile si vous n'avez pas beaucoup de

4
00:00:35,150 --> 00:00:42,620
données: vous pouvez utiliser une astuce pour être sûr que le modèle ne triche pas. La

5
00:00:42,620 --> 00:00:48,820
première technique est la validation croisée: j'ai une expérience par

6
00:00:48,820 --> 00:00:55,190
modèle par hyperparamètre et j'entraînerai le modèle sur l'ensemble de formation.

7
00:00:55,190 --> 00:01:01,760
J'évaluerai sur l'ensemble de validation et j'aurai une performance par expérience. Je

8
00:01:01,760 --> 00:01:06,110
choisirai l'expérience B car les performances sont les meilleures ici et je vais

9
00:01:06,110 --> 00:01:12,940
évaluer. Si vous n'avez pas beaucoup de données, ce que vous pouvez faire avec cette

10
00:01:12,940 --> 00:01:19,460
partition entraînement-validation, c'est une rotation de l'ensemble de validation.

11
00:01:19,460 --> 00:01:25,040
Nous diviserons de sorte que la première division partitionne les premiers éléments en

12
00:01:25,040 --> 00:01:31,220
ensemble d'entraînement et les dernier éléments en ensemble de validation. Ici,

13
00:01:31,220 --> 00:01:35,420
j'utiliserai les exemples du milieu pour l'ensemble de validation et les deux autres

14
00:01:35,420 --> 00:01:40,159
pour l'ensemble d'entraînement. Vous pouvez voir qu'au début, j'adapterai les

15
00:01:40,159 --> 00:01:45,829
hyperparamètres dans le modèle pour chaque expérience une fois pour toutes. Ensuite,

16
00:01:45,829 --> 00:01:53,630
je fais mon entraînement pour chaque fraction sur l'ensemble d'entraînement associé et

17
00:01:53,630 --> 00:01:58,579
j'évalue sur l'ensemble de validation. J'aurai donc ce tableau et je peux calculer

18
00:01:58,579 --> 00:02:04,340
quelques statistiques sur les lignes de ce tableau. Ici j'aurai une moyenne

19
00:02:04,340 --> 00:02:09,619
des performances associées à l'expérience A, une performance moyenne pour B et

20
00:02:09,619 --> 00:02:14,840
pour C et je peux également calculer l'écart type empirique.

21
00:02:14,840 --> 00:02:20,540
S'il y a du bruit et que la métrique de performance varie en raison

22
00:02:20,540 --> 00:02:24,380
du choix de l'ensemble de validation, je pourrai le mesurer, mais c'est

23
00:02:24,380 --> 00:02:27,110
seulement quand vous n'avez pas beaucoup de données. Quand vous

24
00:02:27,110 --> 00:02:33,800
manquez de données dans l'ensemble de validation, la validation croisée

25
00:02:33,800 --> 00:02:39,770
"k-fold" devient intéressante. Lorsque vous utilisez un ensemble de données

26
00:02:39,770 --> 00:02:46,910
de référence dans l'apprentissage profond avec 100 000 exemples ou des millions

27
00:02:46,910 --> 00:02:53,750
d'exemples, vous pouvez simplement utiliser la validation croisée comme celle-ci.

28
00:02:53,750 --> 00:03:01,489
Quand je choisis la meilleure expérience, je dois faire une dernière vérification sur

29
00:03:01,489 --> 00:03:07,280
l'ensemble d'évaluation. Si les performances de mon ensemble d'évaluation se situent dans la plage

30
00:03:07,280 --> 00:03:12,319
d'incertitude définie par l'erreur type autour de la moyenne, je peux être sûr que ça

31
00:03:12,319 --> 00:03:17,900
généralise bien et je peux être plus confiant quant au déploiement du modèle.

32
00:03:17,900 --> 00:03:21,799
Maintenant que le travail du scientifique des données est fini, on a le meilleur modèle et on

33
00:03:21,799 --> 00:03:26,569
veut le déployer: nous allons faire de l'ingénierie parce que maintenant il faut mettre ce modèle

34
00:03:26,569 --> 00:03:32,200
à l'échelle de possiblement des millions d'utilisateurs, ou peut-être qu'on va déployer ce modèle sur un téléphone

35
00:03:32,200 --> 00:03:37,640
portable. Premièrement, il faut se demander: "comment puis-je détecter et traiter les erreurs de prédiction?"

36
00:03:37,640 --> 00:03:43,850
Il n'est pas vrai que vous aurez une précision de 100% sur votre ensemble d'évaluation

37
00:03:43,850 --> 00:03:48,890
et que vous n'aurez jamais d'erreurs en production. Les erreurs font partie de

38
00:03:48,890 --> 00:03:54,470
la vraie vie: comment puis-je détecter ces erreurs avant de fournir les informations

39
00:03:54,470 --> 00:04:01,489
à mon utilisateur? Pour traiter cette erreur, peut-être que je peux utiliser un autre

40
00:04:01,489 --> 00:04:07,790
modèle qui est moins bon, mais que pour ce point de données, peut-être qu'il peut

41
00:04:07,790 --> 00:04:12,650
me fournir une prédiction différente qui n'est pas erronée. Je peux aussi utiliser des

42
00:04:12,650 --> 00:04:19,459
valeurs par défaut si c'est possible. L'autre point est qu'en ingénierie, peut-être que vous aurez

43
00:04:19,459 --> 00:04:23,300
d'autres contraintes qui doivent être respectées, par exemple la latence entre les

44
00:04:23,300 --> 00:04:28,729
moments quand je fais une demande et quand je reçois la réponse, l'utilisation de

45
00:04:28,729 --> 00:04:33,500
mémoire du modèle, ou la puissance de calcul utilisée. Ces contraintes sont

46
00:04:33,500 --> 00:04:37,550
super importantes, mais elles ne doivent pas être prises en compte durant la première

47
00:04:37,550 --> 00:04:41,180
itération du travail du scientifique des données. Le scientifique des données devrait

48
00:04:41,180 --> 00:04:46,940
être libre d'explorer et d'utiliser tous les modèles possibles. Quand on a trouvé un

49
00:04:46,940 --> 00:04:52,640
bon modèle, il est beaucoup plus facile de simplifier ce modèle en tenant compte de

50
00:04:52,640 --> 00:04:57,860
ces contraintes que de restreindre dès le départ les différents modèles que je peux

51
00:04:57,860 --> 00:05:07,130
utiliser dans mon protocole expérimental. On peut peut-être respecter cette contrainte

52
00:05:07,130 --> 00:05:13,340
dans une deuxième itération du protocole expérimental. Le dernier point est de savoir

53
00:05:13,340 --> 00:05:19,760
comment vraiment évaluer l'impact d'avoir un meilleur modèle en production. Peut-être

54
00:05:19,760 --> 00:05:25,430
que le modèle n'est qu'une petite caractéristique de mon système global: même si j'ai une très

55
00:05:25,430 --> 00:05:30,830
bonne mesure de performance, il se peut que mon utilisateur ne voie même pas la

56
00:05:30,830 --> 00:05:36,050
différence. Mesurer cet impact est important et si le système est très compliqué,

57
00:05:36,050 --> 00:05:41,060
les tests A / B sont peut-être le seul moyen de le mesurer: je déploierai

58
00:05:41,060 --> 00:05:45,800
un système avec mon ancien modèle et un système avec le nouveau, puis je compare si

59
00:05:45,800 --> 00:05:50,960
ça a un impact sur mon utilisateur. Si ça a un impact et je peux voir que j'ai un grand

60
00:05:50,960 --> 00:05:55,450
écart dans ma mesure de performance, ça signifie que la mesure de performance est

61
00:05:55,450 --> 00:06:01,250
corrélée à la façon que mon utilisateur la perçoit. Il faut le répéter plusieurs fois

62
00:06:01,250 --> 00:06:06,050
pour être sûr que la corrélation existe, mais ça signifie que vous avez une bonne

63
00:06:06,050 --> 00:06:11,540
métrique. Si elle est meilleure pour le nouveau modèle et que vous ne voyez aucune différence,

64
00:06:11,540 --> 00:06:18,760
alors il faut considérer l'utilisation d'une autre métrique de performance pour la prochaine itération.

