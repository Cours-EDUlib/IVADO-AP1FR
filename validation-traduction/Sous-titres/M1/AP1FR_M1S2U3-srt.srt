0
00:00:13,980 --> 00:00:20,990
so we will come back to hyperparameter tuning and model

1
00:00:20,990 --> 00:00:25,710
selection, but now that we have more tools, more concepts to understand these

2
00:00:25,710 --> 00:00:33,339
we will see it again, so the main question is can we compute, can we verify

3
00:00:33,339 --> 00:00:38,500
this, can we see if there is a difference between the real risk and the empirical

4
00:00:38,500 --> 00:00:43,690
risk, but we don't have the real data distribution: that's the main problem, and

5
00:00:43,690 --> 00:00:49,500
the validation set and the test set will be quantities that will estimate this distribution

6
00:00:49,500 --> 00:00:54,780
and the training set will be used to find the model that will minimize the

7
00:00:54,780 --> 00:01:01,090
empirical risk, so I don't want to rely on this estimation of this quantity

8
00:01:01,090 --> 00:01:07,259
because you see that h_S is the optimal point to minimize L_S and of course there

9
00:01:07,259 --> 00:01:12,890
is a h_D that will be the optimal model that will minimize L_D, but since we don't

10
00:01:12,890 --> 00:01:18,560
have D we cannot find it, but we will simulate it, we will use another set

11
00:01:18,560 --> 00:01:23,369
called validation set for hyperparameters, that we will use to estimate

12
00:01:23,369 --> 00:01:29,450
this quantity, and if the gap between the two is huge

13
00:01:29,450 --> 00:01:35,470
probably the gap between the real risk and this quantity is also important and

14
00:01:35,470 --> 00:01:42,479
so if you deploy your model the performance that you will observe will

15
00:01:42,479 --> 00:01:46,910
probably be closer to the performance on the validation set

16
00:01:46,910 --> 00:01:52,080
than the one that you obtained by choosing the model that minimized the empirical

17
00:01:52,080 --> 00:01:58,440
risk and so the validation set is defined similarly than the

18
00:01:58,440 --> 00:02:02,900
training set and the loss is computed exactly in the same way, you use the same

19
00:02:02,900 --> 00:02:10,360
loss function and you take the empirical average over the set V

20
00:02:10,360 --> 00:02:15,450
and since we have an iterative algorithm for the optimization, you will monitor

21
00:02:15,450 --> 00:02:24,250
the two values, you will monitor the empirical risk in red, which we will call

22
00:02:24,250 --> 00:02:29,950
the training error, and you will monitor an estimation of the risk with

23
00:02:29,950 --> 00:02:34,920
the validation set, that we will call after that the validation error and this

24
00:02:34,920 --> 00:02:39,650
is typically the shape of the two curves that you will observe in many of your

25
00:02:39,650 --> 00:02:43,609
experiments in deep learning, at the beginning the model is

26
00:02:43,609 --> 00:02:50,659
performing badly on the two sets, and at some point you will see the curve of the

27
00:02:50,659 --> 00:02:54,430
validation error that will increase: it means that at this point the model

28
00:02:54,430 --> 00:03:00,140
begins to memorize, to cheat, to use features that are only relevant in the

29
00:03:00,140 --> 00:03:05,969
training set to minimize the loss and so it will continue to decrease but these

30
00:03:05,969 --> 00:03:11,689
features are not useful to generalize and so the validation error will

31
00:03:11,689 --> 00:03:17,739
increase and what we probably want to do is to do several check points around this

32
00:03:17,739 --> 00:03:22,580
curve, so you will save the model parameters on your disk and you will

33
00:03:22,580 --> 00:03:28,250
stop the training before the validation error increases too much it's called

34
00:03:28,250 --> 00:03:34,230
early stopping and so you retrieve the model parameters evaluated here instead

35
00:03:34,230 --> 00:03:42,290
of retrieving the parameters only at the end of the training, and if we come back

36
00:03:42,290 --> 00:03:49,660
to our experimental protocol now I can define several model families, so maybe a

37
00:03:49,660 --> 00:03:55,739
one layer neural network, a two layer neural network, a convolutional neural

38
00:03:55,739 --> 00:04:01,049
network, etc. I will define different families of models, each family will have

39
00:04:01,049 --> 00:04:07,340
a set of parameters where the optimizer will navigate inside and I will find for

40
00:04:07,340 --> 00:04:13,219
each family, the model inside the space H that will minimize the empirical

41
00:04:13,219 --> 00:04:20,019
risk predictor here, and then I will evaluate each of these models on my

42
00:04:20,019 --> 00:04:25,150
validation set to retrieve the performance, to retrieve

43
00:04:25,150 --> 00:04:30,780
an estimation of the risk and so this one has the best performance on my

44
00:04:30,780 --> 00:04:37,810
validation set and again maybe because I'm optimizing, I'm looking for the best

45
00:04:37,810 --> 00:04:43,140
model according to the validation set over many many experiments maybe I can

46
00:04:43,140 --> 00:04:48,290
still overfit the validation set it's more tricky to understand but it can

47
00:04:48,290 --> 00:04:55,350
happen, I will use a final test set to be sure that the performance here is close

48
00:04:55,350 --> 00:05:00,690
to the performance here, if there is a huge gap then there may be a

49
00:05:00,690 --> 00:05:07,700
problem in your experimental protocol you should investigate more, so the

50
00:05:07,700 --> 00:05:11,690
take-home message of this presentation is that machine learning is very broad

51
00:05:11,690 --> 00:05:15,850
there's a lot of different techniques, different models, different algorithms in

52
00:05:15,850 --> 00:05:20,350
machine learning, the statistical learning framework helps us to

53
00:05:20,350 --> 00:05:25,450
understand more what is overfitting in terms of a probability distribution

54
00:05:25,450 --> 00:05:33,760
defined over the examples, a model can be too biased, too simple, not enough complexity

55
00:05:33,760 --> 00:05:38,170
or can have too much variance where there's a lot of complexity but because

56
00:05:38,170 --> 00:05:46,120
I don't have enough data, the decision boundaries are arbitrary, and we saw that

57
00:05:46,120 --> 00:05:52,860
data is necessary but not sufficient, and finally deep learning is a very

58
00:05:52,860 --> 00:05:58,910
efficient way to define complex hypothesis classes and we don't know

59
00:05:58,910 --> 00:06:03,940
still the answer, but deep learning empirically generalizes better than

60
00:06:03,940 --> 00:06:09,780
other models and this is a very active research topic today to understand why

61
00:06:09,780 --> 00:06:14,870
these architectures minimize the gap between the empirical risk and the real

62
00:06:14,870 --> 00:06:22,270
risk, and here I put some references so that you can have the math, but done

63
00:06:22,270 --> 00:06:30,300
properly, it would have required more than one hour to cover these concepts, but I

64
00:06:30,300 --> 00:06:35,920
used these books to to inspire myself for this talk, thank

65
00:06:35,920 --> 00:06:36,610
you very much

