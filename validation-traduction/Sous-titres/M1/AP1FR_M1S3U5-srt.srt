0
00:00:14,030 --> 00:00:18,150
D'accord maintenant, éloignons-nous des outils et regardons

1
00:00:18,150 --> 00:00:23,320
plus en détail le matériel. Comme vous le savez, l'apprentissage profond peut être très

2
00:00:23,320 --> 00:00:27,369
gourmand en ressources, alors nous allons passer en revue les différents matériels qui

3
00:00:27,369 --> 00:00:35,380
sont utiles lorsque nous faisons de l'apprentissage profond.  Commençons par l'ordinateur, les ordinateurs

4
00:00:35,380 --> 00:00:40,050
ont de nombreuses parties différentes. Nous allons nous concentrer principalement sur le CPU et le

5
00:00:40,050 --> 00:00:46,450
GPU. Ils ont tous deux été conçus avec des intentions différentes à l'esprit: le CPU est

6
00:00:46,450 --> 00:00:51,280
conçu pour gérer peu d'opérations rapidement et séquentiellement, et le GPU est

7
00:00:51,280 --> 00:00:55,440
conçu pour gérer de nombreuses opérations en parallèle. Il peut parfois

8
00:00:55,440 --> 00:01:00,880
être plus lent d'utiliser le GPU pour certaines opérations.

9
00:01:00,880 --> 00:01:05,379
Une belle analogie que j'aime utiliser, c'est de considérer le GPU comme beaucoup

10
00:01:05,379 --> 00:01:09,640
de tournevis et penser à un CPU comme une grosse perceuse. Supposons maintenant que

11
00:01:09,640 --> 00:01:13,450
vous avez une infinité de mains. Selon la tâche que vous pourriez avoir, beaucoup de tournevis

12
00:01:13,450 --> 00:01:18,250
peuvent être plus appropriés qu'une perceuse électrique. Un type de

13
00:01:18,250 --> 00:01:22,250
tâche est, par exemple, que vous avez beaucoup de petites vis partout. Il pourrait alors

14
00:01:22,250 --> 00:01:25,659
être très utile d'avoir de nombreux tournevis avec de nombreuses mains faisant

15
00:01:25,659 --> 00:01:29,390
tourner ces vis, comparé à aller avec votre seule perceuse géante une

16
00:01:29,390 --> 00:01:34,340
vis à la fois. Donc lorsque nous entraînons des modèles d'apprentissage profond, de nombreuses

17
00:01:34,340 --> 00:01:39,219
opérations sont indépendantes et beaucoup de ces opérations peuvent être parallélisées.

18
00:01:39,219 --> 00:01:43,810
Il est donc très utile dans le contexte de l'apprentissage profond d'utiliser ces GPU et

19
00:01:43,810 --> 00:01:47,320
ces calculs peuvent être effectués par lots. C'est ce qui est vraiment

20
00:01:47,320 --> 00:01:52,039
utile: lorsque vous les effectuez sur le GPU, le GPU pourrait effectuer ces nombreuses

21
00:01:52,039 --> 00:01:56,530
opérations en parallèle et de sorte que vous pouvez obtenir des accélérations importantes lors de l'entraînement

22
00:01:56,530 --> 00:02:03,330
sur un GPU. Un peu d'histoire: les GPU ont été inventés à l'origine

23
00:02:03,330 --> 00:02:08,580
par l'industrie du jeu. Ce sont les gens à Sony qui ont inventé le terme GPU

24
00:02:08,580 --> 00:02:12,760
pour la PlayStation 1 et ils se trouvent qu'ils sont très efficaces

25
00:02:12,760 --> 00:02:18,030
pour la multiplication matricielle et vectorielle. C'est pourquoi nous les aimons vraiment en

26
00:02:18,030 --> 00:02:22,310
apprentissage profond. L'apprentissage profond est essentiellement beaucoup d'algèbre linéaire et de calcul

27
00:02:22,310 --> 00:02:26,370
et ces GPU nous permettent de faire ces opérations très très efficacement.

28
00:02:26,370 --> 00:02:30,580
Le plus important, c'est que cela nous permet de les faire en parallèle.

29
00:02:30,580 --> 00:02:35,450
Ceux d'entre vous qui font de l'apprentissage profond, vous avez probablement utilisé beaucoup de

30
00:02:35,450 --> 00:02:41,550
GPUs dans votre vie qui ont été conçus principalement pour l'industrie du jeu.

31
00:02:41,550 --> 00:02:46,160
Alors regardons rapidement une vidéo que je trouve un peu amusante: les

32
00:02:46,160 --> 00:02:51,069
Mythbusters ont fait une démonstration pour montrer la différence

33
00:02:51,069 --> 00:02:56,209
entre un CPU et un GPU. Donc encore une fois, les Mythbusters aiment faire des choses

34
00:02:56,209 --> 00:03:02,299
de manière grandiose et j'apprécie vraiment vraiment cette démonstration. Donc, ils prennent une

35
00:03:02,299 --> 00:03:06,129
machine de paintball et maintenant ils imitent le CPU: le CPU fait

36
00:03:06,129 --> 00:03:09,459
une instruction à la fois et vous allez voir que nous pouvons imprimer

37
00:03:09,459 --> 00:03:14,180
le visage, mais cela prend du temps non? Chaque pixel est imprimé un à la fois. Ensuite,

38
00:03:14,180 --> 00:03:19,921
les Mythbusters décident évidemment d'implémenter le GPU, donc ici ils

39
00:03:19,921 --> 00:03:26,170
continuent avec leur analogie pour un GPU et maintenant nous avons un système massivement parallélisé.

40
00:03:26,170 --> 00:03:30,330
Chacun de ces tableaux est en fait un pistolet de paintball, d'accord? Ainsi , au lieu

41
00:03:30,330 --> 00:03:33,270
de n'avoir qu'un pistolet de paintball qui fait le tour et peint l'écran,

42
00:03:33,270 --> 00:03:38,590
nous avons maintenant un tableau parallèle de fusils de paintball. Nous allons voir ce qui se passe,

43
00:03:38,590 --> 00:03:41,909
On attendra un peu pour un effet dramatique, comme les Mythbusters aiment faire.

44
00:03:41,909 --> 00:03:46,430
Ils vont même faire un compte à rebours, alors essayez de penser à ce qui va se passer

45
00:03:46,430 --> 00:03:52,439
Alors, le suspense augmente et ça va être un

46
00:03:52,439 --> 00:03:57,939
résultat vraiment intéressant. Quand ils décident finalement de pousser le bouton,

47
00:03:57,939 --> 00:04:05,599
ils comptent, comptons dans nos têtes et soudainement,

48
00:04:05,599 --> 00:04:10,849
et bien sûr c'est disponible en mouvement lent. Voici donc cette grande démonstration d'un raison pour laquelle

49
00:04:10,849 --> 00:04:14,719
le GPU pourrait être plus adapté pour faire beaucoup d'opérations en parallèle.

50
00:04:14,719 --> 00:04:18,160
Vous pouvez voir à quel point c'était plus efficace: si vous deviez le faire

51
00:04:18,160 --> 00:04:24,250
un par un avec un pistolet de paintball, cela aurait pris beaucoup plus de temps.

52
00:04:24,250 --> 00:04:26,920
C'est donc une motivation, une beau petit exemple de pourquoi les

53
00:04:26,920 --> 00:04:31,970
GPU sont utiles. Aussi, la chose intéressante est que les cadres d'apprentissage profond sont

54
00:04:31,970 --> 00:04:36,390
déjà très optimisés pour paralléliser votre code, donc lorsque vous utilisez

55
00:04:36,390 --> 00:04:40,890
TensorFlow, lorsque vous utilisez PyTorch et que vous avez dit à TensorFlow et PyTorch:

56
00:04:40,890 --> 00:04:45,000
"utilisez ce GPU dont je dispose", il fait déjà

57
00:04:45,000 --> 00:04:49,430
cette parallélisation massive pour vous tant que vous avez écrit votre code de manière

58
00:04:49,430 --> 00:04:54,360
relativement saine. Il est généralement relativement facile de

59
00:04:54,360 --> 00:04:57,970
paralléliser votre code et il y a très peu d'intervention de la part de l'utilisateur

60
00:04:57,970 --> 00:05:02,380
pour vous assurer que votre code va aussi vite que possible sur votre GPU. Ici, j'ai

61
00:05:02,380 --> 00:05:06,360
quelques croquis différents juste pour montrer les différents cadres qui fonctionnent

62
00:05:06,360 --> 00:05:11,940
sous le capot pour rendre votre GPU plus rapide, donc des cadres comme TensorRT et cuDNN.

63
00:05:11,940 --> 00:05:15,490
Je trouve cette diapositive assez intéressante, je l'ai prise du site web de Nvidia.

64
00:05:15,490 --> 00:05:18,949
Elle vous montre la différence en mettant simplement à jour le logiciel qui

65
00:05:18,949 --> 00:05:22,960
parallélise ce code: vous pouvez déjà obtenir des accélérations en rendant simplement la

66
00:05:22,960 --> 00:05:26,890
parallélisation plus efficace. Ce n'est donc pas seulement le matériel mais, aussi la façon que

67
00:05:26,890 --> 00:05:30,590
c'est distribué et de plus, c'est déjà fait pour vous.

68
00:05:30,590 --> 00:05:34,240
Tant que vous vous assurez que vos versions sont à jour,

69
00:05:34,240 --> 00:05:40,050
vous tirerez le maximum possible de vos GPU. Nous devons également mentionner que récemment,

70
00:05:40,050 --> 00:05:45,740
Google a dévoilé leur TPU. Le GPU est une unité de traitement graphique et le TPU

71
00:05:45,740 --> 00:05:49,509
est une unité de traitement des tenseurs. Il ne devrait pas être surprenant que le matériel

72
00:05:49,509 --> 00:05:53,870
devienne de plus en plus spécialisé pour l'apprentissage profond. Le TPU n'est que

73
00:05:53,870 --> 00:05:58,500
disponible dans le nuage: vous ne pouvez pas réellement vous acheter un TPU. Par contre,

74
00:05:58,500 --> 00:06:03,530
ils ont été optimisés pour les réseaux de neurones et je recommande vraiment ce

75
00:06:03,530 --> 00:06:08,800
papier qui est caché en ce moment par cette chose, mais il est en bas ici,

76
00:06:08,800 --> 00:06:12,419
espérons qu'il deviendra visible. Je vous recommande d'aller lire ce document, il y a une

77
00:06:12,419 --> 00:06:17,819
comparaison vraiment approfondie entre l'utilisation de différents CPU, GPU, TPU et les avantages

78
00:06:17,819 --> 00:06:23,960
qu'ils pourraient avoir l'un par rapport à l'autre. Ensuite, parlons un peu des

79
00:06:23,960 --> 00:06:28,521
performances: ce n'est pas que parce que vous utilisez un GPU que vous devriez

80
00:06:28,521 --> 00:06:33,420
ignorer complètement votre CPU. En fait, ce que vous voulez faire, c'est utiliser le CPU

81
00:06:33,420 --> 00:06:36,949
tout en utilisant votre GPU, utiliser les ressources simultanément et les utiliser à

82
00:06:36,949 --> 00:06:40,400
leur propre avantage. Une chose que nous pouvons faire est, par exemple,

83
00:06:40,400 --> 00:06:43,490
une grande du prétraitement dont nous aurions besoin pour nos données, sur le

84
00:06:43,490 --> 00:06:48,530
CPU pendant que le GPU effectue certaines opérations. Voici un joli

85
00:06:48,530 --> 00:06:54,090
petit diagramme que j'ai pris sur le site web de TensorFlow qui vous montre comment

86
00:06:54,090 --> 00:06:57,599
ça fonctionne: pendant que vous entraînez un lot, vous voulez préparer votre prochain

87
00:06:57,599 --> 00:07:02,300
lot sur votre CPU. De cette façon, vous essayez de minimiser autant que possible

88
00:07:02,300 --> 00:07:05,740
le temps d'arrêt de vos composantes: vous voulez vraiment faire vos expériences le

89
00:07:05,740 --> 00:07:10,060
plus rapidement possible. En suivant les tutoriels sur leur site web

90
00:07:10,060 --> 00:07:13,629
et en suivant la documentation, vous pouvez vous assurer que votre code va aussi vite

91
00:07:13,629 --> 00:07:17,990
que possible. Si vous en avez besoin, ajoutez des CPU supplémentaires pour accélérer votre traitement.

92
00:07:17,990 --> 00:07:25,210
Vous ne voulez vraiment aucun goulot d'étranglement lorsque vous exécutez votre code.

93
00:07:25,210 --> 00:07:29,949
Parlons maintenant un peu de l'informatique en périphérie, alors qu'est-ce que

94
00:07:29,949 --> 00:07:34,479
l'informatique en périphérie? Ça signifie simplement d'être en mesure d'effectuer vos calculs

95
00:07:34,479 --> 00:07:38,550
pas dans le nuage. Vous pouvez donc être dans un contexte où vous déployez un certain

96
00:07:38,550 --> 00:07:42,550
produit et ce produit, il est essentiel qu'il puisse fonctionner même s'il

97
00:07:42,550 --> 00:07:45,640
n'est pas connecté à l'Internet. Généralement, nous appelons cela de

98
00:07:45,640 --> 00:07:51,690
l'informatique en périphérie. Vous pouvez penser à cela comme peut-être un RaspberryPi ou un téléphone, ou

99
00:07:51,690 --> 00:07:56,710
spécifiquement dans le contexte, disons, d'une salle d'opération. Si un chirurgien

100
00:07:56,710 --> 00:08:02,220
utilise une sorte d'algorithme d'apprentissage profond pour prendre des décisions,

101
00:08:02,220 --> 00:08:05,650
il est vraiment important qu'il puisse le faire sans avoir accès à une

102
00:08:05,650 --> 00:08:09,909
connexion Internet. Sinon, vous pouvez imaginer que si la connexion Internet est interrompue,

103
00:08:09,909 --> 00:08:13,830
vous ne pouvez pas le faire dépendre d'un processus critique comme celle-ci. Il se

104
00:08:13,830 --> 00:08:18,159
trouve que, tandis que les GPU sont vraiment très utiles pour l'entraînement et l'accélération de votre

105
00:08:18,159 --> 00:08:23,240
entraînement, les CPUs peuvent en fait être très utiles pour l'inférence.

106
00:08:23,240 --> 00:08:26,370
La différence ici est que lorsque vous entraînez vos algorithmes, c'est ce qui

107
00:08:26,370 --> 00:08:30,500
prend très longtemps: vous devez parcourir toutes vos données et je veux dire vous

108
00:08:30,500 --> 00:08:33,941
devez parcourir tout cela de nombreuses fois. Généralement, vous voulez donc parcourir

109
00:08:33,941 --> 00:08:39,680
toutes vos données par lots. Avec un processeur, vous ne pouvez généralement faire qu'une seule

110
00:08:39,680 --> 00:08:44,320
image à la fois. Avec un GPU, vous pouvez faire plusieurs images à la fois, mais quand il est temps de

111
00:08:44,320 --> 00:08:48,350
faire de l'inférence, donc d'utiliser l'informatique en périphérique, vous n'avez généralement qu'un

112
00:08:48,350 --> 00:08:51,320
seul exemple à la fois, surtout si vous êtes hors ligne. Si ce n'est

113
00:08:51,320 --> 00:08:54,500
qu'un utilisateur, il peut ne s'agir que d'un exemple à la fois:

114
00:08:54,500 --> 00:08:59,190
il se peut que votre CPU soit plus qu'assez pour exécuter du code en production pour

115
00:08:59,190 --> 00:09:03,330
l'inférence.D'habitude, c'est durant l'entraînement qu'on obtient une accélération plus forte lorsque

116
00:09:03,330 --> 00:09:07,130
nous utilisons des GPU et des TPU. Gardez cela à l'esprit lorsque vous

117
00:09:07,130 --> 00:09:11,700
concevez des produits: vous n'avez pas nécessairement besoin d'un GPU embarqué. Vous pourriez n'avoir besoin de

118
00:09:11,700 --> 00:09:15,080
votre GPU que pour l'entraînement, mais ce n'est peut-être pas le cas. Peut-être

119
00:09:15,080 --> 00:09:18,340
que vous avez vraiment besoin d'un GPU lorsque vous implémentez vos produits. Il

120
00:09:18,340 --> 00:09:23,450
garder cela à l'esprit lors de la conception de vos différentes infrastructures. Alors maintenant,

121
00:09:23,450 --> 00:09:27,510
parlons un peu de l'infonuagique: l'infonuagique est devenue très

122
00:09:27,510 --> 00:09:32,600
populaire ces dernières années. Elle a différents avantages et inconvénients. Le but de

123
00:09:32,600 --> 00:09:37,120
l'infonuagique est que vous n'avez plus besoin d'acheter vos propres GPU: vous pouvez les louer

124
00:09:37,120 --> 00:09:41,350
en ligne. Vous pouvez aller à un fournisseur de services nuagiques et ici je liste un tas de fournisseurs

125
00:09:41,350 --> 00:09:46,140
différents. Les plus gros sont Google Cloud, Microsoft Azure,

126
00:09:46,140 --> 00:09:51,240
AWS d'Amazon. Certains moins connus mais toujours très bons sont Paperspace, et vast.ai.

127
00:09:51,240 --> 00:09:55,870
Il y a différents fournisseurs et leur objectif est simplement de dire: "vous n'avez plus besoin d'acheter

128
00:09:55,870 --> 00:09:59,980
un GPU, vous pouvez simplement louer vos GPU en ligne." Cela peut être vraiment génial dans la mesure où vous

129
00:09:59,980 --> 00:10:03,570
pouvez accéder au matériel de pointe, car il peut être très coûteux d'acheter du

130
00:10:03,570 --> 00:10:06,740
nouveau matériel. Vous pouvez imaginer qu'après quelques années, votre matériel

131
00:10:06,740 --> 00:10:10,930
est démodé. Vous avez également un coût initial très faible: tout ce que vous avez vraiment besoin de

132
00:10:10,930 --> 00:10:15,310
faire est de vous inscrire. Vous n'avez pas besoin d'acheter quoi que ce soit à l'avance. Vous pouvez aussi

133
00:10:15,310 --> 00:10:20,620
augmenter et réduire votre utilisation en fonction de la demande: vous avez peut-être besoin de cinq GPUs aujourd'hui, mais dans

134
00:10:20,620 --> 00:10:24,330
trois mois, la recherche a ralenti, vous travaillez davantage sur l'implémentation de votre

135
00:10:24,330 --> 00:10:27,500
pipeline de production. Vous pouvez alors réduire l'utilisation des GPUs et vous

136
00:10:27,500 --> 00:10:31,430
ne les utiliserez que lorsque vous en aurez besoin. Le contre est que le coût peut être prohibitif:

137
00:10:31,430 --> 00:10:37,010
si vous utilisez des GPU 24/7 dans le nuage, votre facture augmentera. S'il vous

138
00:10:37,010 --> 00:10:40,530
arrive d'oublier de fermer votre instance et de la laisser allumée pendant une semaine,

139
00:10:40,530 --> 00:10:43,360
vous aurez une très mauvaise surprise sur votre carte de crédit à la

140
00:10:43,360 --> 00:10:50,390
fin du mois et je parle d'expérience. Maintenant, je veux juste peut-être comparer

141
00:10:50,390 --> 00:10:56,670
des cadres différents et des plates-formes différentes. Vous pouvez

142
00:10:56,670 --> 00:11:03,280
accéder à cette étude ici, c'est de DAWN de l'Université de Stanford et ce que

143
00:11:03,280 --> 00:11:05,890
vous voyez ici est une comparaison des différents modèles.

144
00:11:05,890 --> 00:11:08,160
Désolé c'est toujours le même modèle qui est en cours

145
00:11:08,160 --> 00:11:11,651
d'exécution. Dans ce cas-ci, c'est un ResNet50: nous allons regarder

146
00:11:11,651 --> 00:11:16,820
les ResNets mercredi. Par contre, ce qui est vraiment

147
00:11:16,820 --> 00:11:21,110
intéressant ici est le temps qu'il faut pour atteindre une précision de 93%, ainsi que le

148
00:11:21,110 --> 00:11:25,310
coût, le matériel qui a été utilisé et le cadre qui a été utilisé.

149
00:11:25,310 --> 00:11:29,870
Ce que je veux souligner, c'est tout d'abord comment le temps a chuté de

150
00:11:29,870 --> 00:11:33,590
manière significative, mais aussi les différences dans les coûts que vous pourriez encourir

151
00:11:33,590 --> 00:11:37,290
en fonction de comment vous voulez entraîner ces modèles. Par exemple, si vous vous

152
00:11:37,290 --> 00:11:44,250
entraîniez sur un TPU en avril 2018, cela pourrait vous avoir pris neuf heures et

153
00:11:44,250 --> 00:11:48,290
cela vous aurait coûté environ 60$. Aujourd'hui, ce temps passé sur un TPU chute

154
00:11:48,290 --> 00:11:52,230
à moins de trois heures et le coût a également chuté de manière significative à

155
00:11:52,230 --> 00:11:56,390
environ douze dollars et pour l'entraînement d'un ResNet50 sur ImageNet,

156
00:11:56,390 --> 00:12:00,450
une tâche non triviale à entraîner. Vous avez donc ces différentes comparaisons,

157
00:12:00,450 --> 00:12:06,230
c'est vraiment utile et encore une fois celles-ci changent tout le temps que ce soit Google

158
00:12:06,230 --> 00:12:10,560
ou Amazon. Ils sont tous les deux dans cette course aux armements de GPU où ils

159
00:12:10,560 --> 00:12:14,620
essaient d'obtenir les meilleurs GPU disponibles pour le moins cher possible, alors faites vos

160
00:12:14,620 --> 00:12:20,330
propres recherches, regardez les différents blogs. J'ai mis un lien ici à très joli article de blog,

161
00:12:20,330 --> 00:12:23,930
il s'appelle "maximizing your GPU dollars" («maximiser vos dollars GPU») et il contient

162
00:12:23,930 --> 00:12:27,120
cette belle étude sur l'entraînement de différents modèles avec différentes

163
00:12:27,120 --> 00:12:30,870
plates-formes, ainsi que les avantages et inconvénients en fonction du cadre que vous

164
00:12:30,870 --> 00:12:35,490
utilisez, que vous utilisiez TensorFlow, quelle version de TensorFlow, PyTorch etc.

165
00:12:35,490 --> 00:12:40,190
Ça aura également un impact énorme.

166
00:12:40,190 --> 00:12:43,890
Maintenant nous parlerons peut-être juste du pipeline de l'infonuagique,

167
00:12:43,890 --> 00:12:49,950
un pipeline typique et à quoi cela pourrait ressembler. Alors, vous iriez héberger vos données

168
00:12:49,950 --> 00:12:56,710
sur certains fournisseurs de services nuagiques comme AWS, Microsoft, Google, choisissez votre démon.

169
00:12:56,710 --> 00:13:00,730
Ici, vous avez votre base de données qui vit dans le nuage et en général,

170
00:13:00,730 --> 00:13:04,610
vous commencez d'abord par prototyper vos modèles afin d'entraîner tout un tas de

171
00:13:04,610 --> 00:13:09,250
modèles. Vous avez votre optimisation d'hyper-paramètre, vous évaluez et vous validez. Puis,

172
00:13:09,250 --> 00:13:14,170
vous avez enfin une sorte de processus de décision qui dit: "voici notre meilleur modèle".

173
00:13:14,170 --> 00:13:18,260
Ensuite, vous pouvez déployer votre modèle en production et ce à quoi cela pourrait

174
00:13:18,260 --> 00:13:22,730
ressembler, c'est que vous auriez une sorte d'API basée sur l'architecture REST

175
00:13:22,730 --> 00:13:27,570
où quelqu'un peut interroger votre API. Les données sont téléchargées sur votre serveur, vous faites de

176
00:13:27,570 --> 00:13:34,060
l'inférence sur votre serveur et vous les renvoyez aux utilisateurs.

177
00:13:34,060 --> 00:13:38,270
Certains outils très utiles pour faire ce genre d'APIs sont Django et

178
00:13:38,270 --> 00:13:43,500
Flask. Cela dépend vraiment de votre entreprise et des outils que vous utilisez

179
00:13:43,500 --> 00:13:46,750
déjà en interne, mais ce n'est qu'un aperçu de ce à quoi ça pourrait ressembler

180
00:13:46,750 --> 00:13:51,100
si vous vouliez faire de l'infonuagique en utilisant l'apprentissage profond dans le nuage.

