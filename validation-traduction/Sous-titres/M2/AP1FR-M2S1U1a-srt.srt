1
00:00:15,049 --> 00:00:23,199
D’accord, donc aujourd'hui, nous allons parler de l'apprentissage profond et seulement de l'apprentissage profond, ainsi

2
00:00:23,199 --> 00:00:28,070
hier, nous avons vu dans la conférence d'apprentissage automatique que nous devons choisir un

3
00:00:28,070 --> 00:00:32,839
espace d'hypothèse, vous vous souvenez de ce H majuscule, nous devons choisir un espace de

4
00:00:32,839 --> 00:00:40,150
modèles qui sera bon pour apprendre de nos données et nous voulons également implémenter

5
00:00:40,150 --> 00:00:45,329
cette hypothèse de variété que j'ai motivée avec un exemple en haute

6
00:00:45,329 --> 00:00:52,870
dimension avec l'ensemble de données MNIST. Aujourd'hui nous verrons comment créer un

7
00:00:52,870 --> 00:00:56,879
espace d'hypothèse, qu'est-ce que cela signifie, et comment créer une architecture en

8
00:00:56,879 --> 00:01:04,089
apprentissage profond pour avoir un bon espace d’hypothèse. Pour ce faire, je commencerai par

9
00:01:04,089 --> 00:01:12,570
l’intuition derrière l'apprentissage profond et j'utiliserai un exemple simple. Je poserai

10
00:01:12,570 --> 00:01:19,080
juste une question simple: qu'est-ce qu'une chaise? Nous essaierons de construire un

11
00:01:19,080 --> 00:01:23,661
algorithme tel que pour toute image que je donnerai en entrée à mon modèle,

12
00:01:23,661 --> 00:01:30,450
l’algorithme prédira si c'est une chaise ou non qui est dans l'image et en tant que

13
00:01:30,450 --> 00:01:35,751
scientifique des données, je ne sais peut-être pas comment le faire avec l’apprentissage automatique. Je

14
00:01:35,751 --> 00:01:39,300
vais commencer par regarder la définition d'une chaise dans le dictionnaire, alors ici je

15
00:01:39,300 --> 00:01:44,300
choisis le dictionnaire Merriam-Webster et la définition est: un siège

16
00:01:44,300 --> 00:01:51,129
ayant généralement quatre pieds et un dossier pour une personne. À partir de cette

17
00:01:51,129 --> 00:01:55,950
définition très abstraite qui est simple pour les humains car c'est en langage naturel, en

18
00:01:55,950 --> 00:01:59,390
anglais, je dois convertir cette définition en

19
00:01:59,390 --> 00:02:04,149
programme, par exemple en Python, qui va prendre une image et

20
00:02:04,149 --> 00:02:10,190
sortir 0 ou 1: 1 pour « chaise » ou 0 pour « pas chaise ». C'est ce que les gens

21
00:02:10,190 --> 00:02:18,840
essayaient de réaliser au début de l'IA. Ils se demandaient vraiment: « comment puis-je programmer un

22
00:02:18,840 --> 00:02:23,420
détecteur de caractéristiques dans les données qui implémentera la définition? »

23
00:02:23,420 --> 00:02:29,700
Ici, l'exemple est une chaise, mais vous pouvez penser à n'importe quel système complexe. Alors

24
00:02:29,700 --> 00:02:34,840
les programmeurs allaient voir l'expert pour un système donné et posaient

25
00:02:34,840 --> 00:02:40,620
beaucoup de questions pour récupérer suffisamment d'informations. Ils essayaient de

26
00:02:40,620 --> 00:02:44,670
coder toutes ces informations dans un programme, ils essayaient de programmer ces

27
00:02:44,670 --> 00:02:50,750
informations et l'exemple le plus célèbre est bien sûr les échecs. Les gens jouant aux échecs

28
00:02:50,750 --> 00:02:56,280
essayaient de coder l'expertise du joueur d'échecs pour construire des algorithmes

29
00:02:56,280 --> 00:03:04,980
pour jouer aux échecs. Essayons de concevoir un programme conçu à la main ici. Peut-être

30
00:03:04,980 --> 00:03:12,820
pouvons-nous supposer comme extracteur de caractéristiques : « y a-t-il quatre pattes? Y a-t-il un

31
00:03:12,820 --> 00:03:18,840
dossier? Pouvons-nous nous asseoir dessus? » À partir de ces trois questions que j'ai extraites

32
00:03:18,840 --> 00:03:25,120
de la définition, les deux premières semblent plus faciles: il est plus facile de coder si

33
00:03:25,120 --> 00:03:30,630
cette chaise a quatre pieds et un dossier que s’il est de s’asseoir dessus.

34
00:03:30,630 --> 00:03:36,000
C'est quelque chose qui est si facile pour nous, on peut juste penser: « puis-je m'asseoir sur cet objet? »

35
00:03:36,000 --> 00:03:40,540
Nous le faisons dans notre vie quotidienne, mais pour le coder dans un programme, nous devons

36
00:03:40,540 --> 00:03:47,370
avoir un modèle de la physique derrière cette image et nous devons faire une simulation pour

37
00:03:47,370 --> 00:03:56,230
répondre à cette dernière question de pouvoir s’asseoir dessus. En fait, nous regarderons

38
00:03:56,230 --> 00:04:08,000
les deux premières questions ici, la dernière utilise d'autres techniques que vous verrez vendredi

39
00:04:08,000 --> 00:04:13,599
qui sont reliées à l'apprentissage par renforcement. Pour l'extracteur de caractéristiques

40
00:04:13,599 --> 00:04:19,530
« est-ce qu'il y a quatre pieds? », est-ce vraiment une bonne définition? Si nous prenons ce modèle de

41
00:04:19,530 --> 00:04:26,100
chaise, nous pouvons clairement voir que c'est une chaise, mais elle n'a pas quatre pieds.

42
00:04:26,100 --> 00:04:31,441
C'est ce qui rend cette chaise spéciale: c'est une chaise célèbre apparemment, mais elle

43
00:04:31,441 --> 00:04:37,620
n'a pas quatre pieds pour nous soutenir. « Y a-t-il un dossier? » Celle-ci a un dossier,

44
00:04:37,620 --> 00:04:42,949
mais c'est un très grand dossier. Je suis sûr que nous sommes très à l'aise dans cette chaise, mais ce n'est

45
00:04:42,949 --> 00:04:50,770
clairement pas le même dossier que cette chaise. Alors, le programme que je

46
00:04:50,770 --> 00:04:55,319
dois créer doit être robuste aux facteurs de variation de

47
00:04:55,319 --> 00:05:01,260
l'objet. Je viens de mentionner deux facteurs de variation, mais pensez

48
00:05:01,260 --> 00:05:05,889
à une chaise: vous verrez qu'il y a une multitude de différents types de chaises dans

49
00:05:05,889 --> 00:05:12,129
l'univers. Le programme devrait pouvoir fonctionner avec n'importe laquelle de ces variations, il

50
00:05:12,129 --> 00:05:17,870
devrait être robuste aux facteurs de variation. De plus, notre programme devrait être

51
00:05:17,870 --> 00:05:23,740
robuste à l'entourage de la chaise dans une image. Nous ne voulons pas que notre programme

52
00:05:23,740 --> 00:05:30,430
prédise si c'est une chaise ou non en regardant où se trouve la chaise.

53
00:05:30,430 --> 00:05:35,080
Nous voulons qu'il soit robuste, car si la chaise n'est pas dans un endroit commun, ici

54
00:05:35,080 --> 00:05:41,909
elle est dans la nature, nous voulons que notre algorithme prédise bien. Nous ne voulons pas que la

55
00:05:41,909 --> 00:05:46,509
performance de notre algorithme diminue parce que l'environnement n'est pas quelque chose

56
00:05:46,509 --> 00:05:54,901
que nous voyons très souvent, qui n'est pas courant. Alors, programmer tout le modèle

57
00:05:54,901 --> 00:05:59,259
à la main, concevoir le programme à la main semble très complexe à cause de tout ces

58
00:05:59,259 --> 00:06:07,080
facteurs de variation d'une chaise. C'est probablement pourquoi le système basé sur des règles,

59
00:06:07,080 --> 00:06:14,710
les gens y ont beaucoup travaillé, mais en fin de compte ça n'a pas été très réussi.

60
00:06:14,710 --> 00:06:21,869
La deuxième tentative pour résoudre ce genre de problème a été, au lieu d'écrire tout

61
00:06:21,869 --> 00:06:26,999
le code, tout le programme, d’essayer de trouver de bonnes caractéristiques dans les images. Nous essaierons

62
00:06:26,999 --> 00:06:34,389
d'extraire les caractéristiques pertinentes que nous pouvons donner à un modèle d'apprentissage automatique simple

63
00:06:34,389 --> 00:06:41,589
comme un régresseur linéaire. À partir de ce modèle d'apprentissage automatique simple,

64
00:06:41,589 --> 00:06:46,800
nous agrégerons toutes les caractéristiques extraites pour prédire si c'est une

65
00:06:46,800 --> 00:06:54,629
chaise ou non. Nous allons essayer de faire cet exercice: j'utiliserai un extracteur de caractéristiques

66
00:06:54,629 --> 00:06:59,210
super simple qui n'est pas utilisé en pratique, car cela ne fonctionnerait que pour les chaises. En

67
00:06:59,210 --> 00:07:01,710
vision par ordinateur, les gens essayaient de produire

68
00:07:01,710 --> 00:07:04,909
des caractéristiques conçues à la main qui sont universelles, qui

69
00:07:04,909 --> 00:07:09,979
peuvent être utilisées pour n'importe quelle tâche en vision. Il y avait beaucoup de recherches en

70
00:07:09,979 --> 00:07:17,300
vision avant l'apprentissage profond. Les plus récents et

71
00:07:17,300 --> 00:07:25,150
complexes extracteurs de caractéristiques avaient des performances qui n'étaient pas si mauvaises, mais vous

72
00:07:25,150 --> 00:07:29,030
verrez pourquoi l'apprentissage profond a dominé par la suite.