1
00:00:14,200 --> 00:00:20,990
et la manière moderne de représenter ce type d'architecture consiste à n'utiliser

2
00:00:20,990 --> 00:00:29,620
que de petites cases qui représentent la transformation effectuée sur les données. Au

3
00:00:29,620 --> 00:00:34,230
lieu de représenter des unités de traitement avec des cercles et des connexions, vous pouvez voir

4
00:00:34,230 --> 00:00:39,270
que cela peut prendre beaucoup de temps pour dessiner cette architecture même si elle est plus

5
00:00:39,270 --> 00:00:45,620
belle par rapport à cette simple série de boîtes en raison de la complexité de

6
00:00:45,620 --> 00:00:53,500
l’architecture que nous développons, nous dirons simplement que nous donnons un vecteur de N nombres

7
00:00:53,500 --> 00:00:58,680
et nous savons comment ce vecteur a été généré par les données parce que nous connaissons le

8
00:00:58,680 --> 00:01:02,190
pré-traitement fait sur les données. Alors, ce vecteur sera donné

9
00:01:02,190 --> 00:01:08,380
à un module appelé dense qui est vraiment une transformation linéaire. Le module dense,

10
00:01:08,380 --> 00:01:14,829
ce sont les connexions ici: la multiplication des paramètres avec

11
00:01:14,829 --> 00:01:19,420
la valeur associée aux unités de traitement. Alors, on utilise une

12
00:01:19,420 --> 00:01:26,729
activation: ce module concerne l'opération sur les neurones cachés puis on

13
00:01:26,729 --> 00:01:31,399
a un autre module dense qui est associé à ces connexions.

14
00:01:31,399 --> 00:01:38,940
Activation, dense, activation et on obtient un vecteur en sortie, nous

15
00:01:38,940 --> 00:01:44,939
représentons donc la même chose, mais de manière beaucoup plus simplifiée et le nom de ces

16
00:01:44,939 --> 00:01:50,399
modules est important parce que maintenant les bibliothèques que vous utilisez,

17
00:01:50,399 --> 00:01:56,119
Tensorflow et PyTorch, elles ont standardisé le nom des opérations qui

18
00:01:56,119 --> 00:02:02,770
auparavant n'étaient pas définies avec un nom, il était juste donné

19
00:02:02,770 --> 00:02:09,390
par ce type d'architecture. Regardons le module dense: c’est donc une

20
00:02:09,390 --> 00:02:17,730
multiplication vecteur-vecteur, ce petit diagramme je vais le décomposer en cette

21
00:02:17,730 --> 00:02:21,360
opération et nous verrons qu'avec un module je peux aussi

22
00:02:21,360 --> 00:02:27,151
faire des opérations plus efficacement. Alors comment puis-je implémenter le module dense?

23
00:02:27,151 --> 00:02:33,870
J’utiliserai un vecteur ligne pour un exemple. Ici, j'ai seulement quatre caractéristiques

24
00:02:33,870 --> 00:02:39,110
comme l'iris, puis j'aurai quatre connexions, elles représenteront

25
00:02:39,110 --> 00:02:44,130
ma forme, je fais le produit scalaire, je multiplierai composante par composante,

26
00:02:44,130 --> 00:02:52,420
puis j’additionnerai et j'aurai un scalaire qui sera la prochaine représentation

27
00:02:52,420 --> 00:02:58,340
qui représentera si la forme a été trouvé dans l'exemple.

28
00:02:58,340 --> 00:03:04,849
Si j'ai plusieurs formes, je peux empiler les formes sur les colonnes. Alors maintenant,

29
00:03:04,849 --> 00:03:11,780
j'ai une matrice: c'est un vecteur 1 par 4, c'est une matrice 4 par 3. Si je

30
00:03:11,780 --> 00:03:17,659
multiplie les deux, j'obtiendrai un vecteur 1 par 3 pour ma représentation. Alors maintenant, je ne

31
00:03:17,659 --> 00:03:25,409
fais que la multiplication matrice-vecteur et je représente l'opération effectuée

32
00:03:25,409 --> 00:03:34,920
ici. Puisque j'ai encore un degré de liberté pour faire de l'algèbre linéaire, je peux

33
00:03:34,920 --> 00:03:41,650
ajouter des exemples ici pour passer d'un vecteur à une matrice. Donc ici, j'ai

34
00:03:41,650 --> 00:03:49,730
deux exemples que j'ai empilés: c’est une matrice 2 par 4 fois une matrice 4 par 3,

35
00:03:49,730 --> 00:03:55,599
cela me donnera une matrice 2 par 3 et parce que je fais une multiplication

36
00:03:55,599 --> 00:04:01,709
matrice-matrice, nous savons que les bibliothèques d'algèbre linéaire qui l’implémentent,

37
00:04:01,709 --> 00:04:09,500
tel que l'API BLAS, sont très très efficaces. Elles sont super efficaces pour effectuer ce

38
00:04:09,500 --> 00:04:15,480
genre de calcul et sur GPU, c'est encore plus rapide et le module dense sera utilisé

39
00:04:15,480 --> 00:04:23,970
partout dans notre architecture: c'est pourquoi nous utilisons le GPU, nous avons implémenté cette

40
00:04:23,970 --> 00:04:30,570
étape simple dans le réseau neuronal artificiel comme multiplication matrice-matrice, de

41
00:04:30,570 --> 00:04:40,161
sorte que l'avantage sur les diagrammes classiques, les données et les modèles sont maintenant stockés dans des

42
00:04:40,161 --> 00:04:47,360
tenseurs. Nous utilisons donc des tableaux à n dimensions (n-d arrays), et des opérations vecteur-matrice.

43
00:04:47,360 --> 00:04:51,702
Maintenant que les opérations d'apprentissage profond seront encapsulées dans des modules, nous verrons donc

44
00:04:51,702 --> 00:04:56,120
différents modules pendant le cours, mais vraiment dans les bibliothèques, vous pouvez simplement

45
00:04:56,120 --> 00:05:02,070
prendre un module, l'utiliser dans votre architecture et lier la sortie d'un

46
00:05:02,070 --> 00:05:07,511
module à l'entrée d'un autre module. Vous ne traiterez donc cette architecture

47
00:05:07,511 --> 00:05:13,530
qu'en utilisant des modules où toute la complexité de l'opération a été

48
00:05:13,530 --> 00:05:18,020
encapsulée et programmée pour vous très efficacement. Aussi,

49
00:05:18,020 --> 00:05:27,420
les modules peuvent traiter plusieurs exemples en parallèle, c'est super important

50
00:05:27,420 --> 00:05:34,720
pour entraîner le modèle efficacement. C’est implémenté avec l'algèbre linéaire sur le GPU

51
00:05:34,720 --> 00:05:40,540
et nous utiliserons cette approche modulaire pour définir les architectures. Donc, si vous

52
00:05:40,540 --> 00:05:47,340
communiquez votre travail à quelqu'un d'autre, vous pouvez simplement dire « j’utilise le modèle vgg »,

53
00:05:47,340 --> 00:05:53,960
et vgg est assez standard, il a des milliers de citations, c'est un papier qui

54
00:05:53,960 --> 00:05:58,500
présente une nouvelle architecture et il fonctionne très bien sur différents tests de vision,

55
00:05:58,500 --> 00:06:04,130
et les gens savent ce q’est vgg dans la communauté. C'est vraiment un

56
00:06:04,130 --> 00:06:10,220
modèle très célèbre, mais à l'intérieur de vgg, il y a beaucoup de convolutions avec des couches de mise en commun par maximum (max pool), et

57
00:06:10,220 --> 00:06:15,720
différentes opérations qui elles-mêmes sont composées d'autres opérations. Vous pouvez

58
00:06:15,720 --> 00:06:22,800
voir que tout est encapsulé ici, mais aussi ces modules peuvent

59
00:06:22,800 --> 00:06:30,160
encapsuler la complexité jusqu'à la multiplication, l’addition et le produit

60
00:06:30,160 --> 00:06:35,520
scalaire unique. Au bas de cette hiérarchie, vous aurez les

61
00:06:35,520 --> 00:06:39,810
opérations élémentaires.

62
00:06:39,810 --> 00:06:44,400
La fonction d'activation peut aussi être encapsulée dans des modules. Ici,

63
00:06:44,400 --> 00:06:51,560
j'utilisais ces points jaunes pour représenter que ce module a des paramètres, mais la

64
00:06:51,560 --> 00:06:56,000
fonction d'activation n'a pas de paramètres. Les paramètres du

65
00:06:56,000 --> 00:07:05,940
module dense sont les valeurs des poids sur les connexions, mais une

66
00:07:05,940 --> 00:07:12,020
fonction d'activation n'a pas de paramètres. Un module est donc un

67
00:07:12,020 --> 00:07:16,770
bloc de construction élémentaire que nous pouvons combiner avec d'autres blocs pour créer de nouveaux

68
00:07:16,770 --> 00:07:23,640
modules et c'est quelque chose qui est très efficace parce que maintenant, je peux créer

69
00:07:23,640 --> 00:07:29,020
le module « Inception »qui n'est pas toute l'architecture « Inception », c'est seulement une

70
00:07:29,020 --> 00:07:35,780
sous-partie de celui-ci et peut-être que ce module peut être intéressant pour capturer certaines

71
00:07:35,780 --> 00:07:39,620
caractéristiques et créer une représentation intéressante. Ce module « Inception »

72
00:07:39,620 --> 00:07:45,920
peut être utilisé par une autre architecture. Il y a aussi le modèle « Inception » qui

73
00:07:45,920 --> 00:07:52,620
est beaucoup plus complexe, mais peut également être encapsulé dans un seul module.

74
00:07:52,620 --> 00:07:58,600
Voici un exemple, c'est le modèle « Inception » et ici ils

75
00:07:58,600 --> 00:08:06,850
n'utilisent pas beaucoup d'encapsulation: chaque petite boîte est une opération élémentaire

76
00:08:06,850 --> 00:08:12,440
et les données entrent ici. Vous pouvez voir comment toutes les données sont

77
00:08:12,440 --> 00:08:18,620
traitées jusqu'à la sortie du modèle. Nous avons donc une sortie ici une

78
00:08:18,620 --> 00:08:25,670
sortie ici et une autre ici, donc c'est super complexe, mais comme exercice,

79
00:08:25,670 --> 00:08:30,610
vous pouvez essayer de trouver des motifs dans cette architecture et définir un module qui

80
00:08:30,610 --> 00:08:38,779
encapsulera une certaine complexité et reliera ces modules entre eux. C'est

81
00:08:38,779 --> 00:08:42,800
un exercice intéressant parce que vous ne voulez pas programmer

82
00:08:42,800 --> 00:08:48,570
dans une fonction tous ces minuscules modules. Vous voulez créer une classe qui sera

83
00:08:48,570 --> 00:08:53,620
un sous-module et vous voulez réutiliser ces sous-modules encore et encore pour réduire la

84
00:08:53,620 --> 00:09:00,680
longueur de votre code. Une autre question est de savoir: comment pouvons-nous construire un

85
00:09:00,680 --> 00:09:07,180
modèle probabiliste avec ces architectures complexes? Quand je dis que je veux

86
00:09:07,180 --> 00:09:11,440
reconnaître s'il y a un chat ou non dans l'image, vraiment ce que je veux prédire est

87
00:09:11,440 --> 00:09:15,310
la probabilité d'un chat, car c’est beaucoup plus flexible.

88
00:09:15,310 --> 00:09:23,030
Au lieu de dire seulement « chat » ou « pas de chat », mon modèle peut dire que je suis sûr à 90% que c'est un chat. Cela

89
00:09:23,030 --> 00:09:26,740
nous donne beaucoup plus d'informations, surtout si vous faites un

90
00:09:26,740 --> 00:09:34,710
apprentissage actif: si le modèle dit à 50% qu’il y a un chat pour un point de données sans étiquette, vous pouvez demander

91
00:09:34,710 --> 00:09:39,880
à l'annotateur de donner l'étiquette parce que le modèle est incertain. Alors, comment

92
00:09:39,880 --> 00:09:45,090
pouvons-nous créer une probabilité? Pour la classification binaire c'est vraiment facile: je

93
00:09:45,090 --> 00:09:52,000
peux utiliser n'importe quel modèle aussi complexe que je veux qui pré-traitera les données et à

94
00:09:52,000 --> 00:09:57,690
la fin je viens ajouter un seul module qui est le module sigmoïde. Le

95
00:09:57,690 --> 00:10:04,580
sigmoïde renverra une valeur entre 0 et 1 et cette valeur représentera la

96
00:10:04,580 --> 00:10:11,940
probabilité P de « y a-t-il un chat ou non dans l'image? » Ce sera le paramètre de

97
00:10:11,940 --> 00:10:18,650
ma distribution de Bernoulli, donc P est la probabilité de classe 1, et 1 moins P est

98
00:10:18,650 --> 00:10:25,630
la probabilité de classe 0: je viens de convertir mon architecture très complexe

99
00:10:25,630 --> 00:10:32,740
en modèle probabiliste en ajoutant un sigmoïde. Si je veux faire une

100
00:10:32,740 --> 00:10:38,341
classification multi-étiquettes,  rappelez-vous que nous avions cette image-ci où nous voulons trouver tous les

101
00:10:38,341 --> 00:10:47,279
objets dans l'image. J'ai ici dans mon dictionnaire six concepts: « chat », « voiture »,

102
00:10:47,279 --> 00:10:51,700
« personne »,  « arbre »,  « chien » et « vélo ». Pour chacun des concepts, mon

103
00:10:51,700 --> 00:10:59,860
modèle doit prédire: « l'objet est-il dans l'image ou non? » La prédiction est faite

104
00:10:59,860 --> 00:11:05,630
concept par concept. Ici, le modèle est à peu près certain qu'il y a des personnes dans

105
00:11:05,630 --> 00:11:12,510
l'image: il n'y a pas de chat dans cette image et le modèle est à soixante pour cent sûr

106
00:11:12,510 --> 00:11:19,050
qu’il y a un vélo dans cette image. Comment puis-je le modéliser? Je peux utiliser à nouveau le

107
00:11:19,050 --> 00:11:28,370
module sigmoïde, mais maintenant dans ce module je dois prendre la dernière représentation et je

108
00:11:28,370 --> 00:11:34,240
dois utiliser un module dense pour convertir une représentation d'une taille arbitraire au

109
00:11:34,240 --> 00:11:42,650
nombre de concepts que je veux prédire. Donc si j'ai un vecteur de taille 800

110
00:11:42,650 --> 00:11:48,560
comme représentation, j'utilise le dense pour convertir 800 en 6, puis j'utilise une

111
00:11:48,560 --> 00:11:55,140
fonction sigmoïde sur mon vecteur de 6 pour avoir une probabilité par composante du

112
00:11:55,140 --> 00:12:03,300
vecteur. Donc, pour la prédiction multi-étiquette, maintenant je dois

113
00:12:03,300 --> 00:12:10,740
identifier un et un seul concept dans l'image: ça doit être un chat ou un chien ou un

114
00:12:10,740 --> 00:12:15,460
cheval ou un tigre ou un pingouin, mais ça ne peut pas être deux concepts à la fois. C'est

115
00:12:15,460 --> 00:12:24,860
un seul parmi les n concepts ici: mon modèle est sûr à 90% qu'il s'agit d'un chat, mais aussi

116
00:12:24,860 --> 00:12:31,370
qu'il s’agit peut être d’un tigre avec 8% et d’un chien avec 2%.

117
00:12:31,370 --> 00:12:36,210
Ce que nous voulons réaliser ici, c'est que chaque nombre est compris entre zéro et un, et

118
00:12:36,210 --> 00:12:44,190
que la somme est 1. Donc, ici j'ai 90 plus 8 plus 2, j'aurai 100% de

119
00:12:44,190 --> 00:12:50,650
prédiction sur tous les objets. Ça représente que l'objet qui est dans

120
00:12:50,650 --> 00:12:56,790
l’image doit être un de ces concepts ici, ça ne peut pas être autre chose.

121
00:12:56,790 --> 00:13:02,110
Si vous voulez, vous pouvez également ajouter une autre composante ici pour dire «autre» et peut-être

122
00:13:02,110 --> 00:13:07,980
que le modèle peut utiliser « autre » lorsque le concept dans l'image n'est pas dans le

123
00:13:07,980 --> 00:13:14,730
dictionnaire. Alors, comment pouvons-nous modéliser cette probabilité en multiclasse? Nous utiliserons le

124
00:13:14,730 --> 00:13:24,940
module softmax, donc le softmax est défini ici: pour le vecteur de taille n,

125
00:13:24,940 --> 00:13:29,029
la probabilité associée à une composante sera l'exponentielle de cette

126
00:13:29,029 --> 00:13:34,960
composante et je diviserai par la somme de l'exponentielle sur tous les

127
00:13:34,960 --> 00:13:39,079
composantes, donc ça s'appelle module « softmax ».
