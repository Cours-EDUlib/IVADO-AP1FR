1
00:00:14,010 --> 00:00:19,210
Maintenant nous allons passer en revue le calcul différentiel et intégral pour comprendre clairement qu’est-ce

2
00:00:19,210 --> 00:00:23,840
que le gradient. Nous ferons beaucoup d'approximations de fonction, mais sur un

3
00:00:23,840 --> 00:00:29,280
graphe de calcul. Donc pour commencer, nous allons juste revoir la définition de la

4
00:00:29,280 --> 00:00:33,520
dérivée: j'ai une fonction qui prend comme argument

5
00:00:33,520 --> 00:00:42,650
x, f(x), donc ici c'est la valeur de f pour différents x. Je vais essayer de calculer

6
00:00:42,650 --> 00:00:48,720
une approximation linéaire au point a et pour ce faire, j'utiliserai cette quantité. Disons que j'ai

7
00:00:48,720 --> 00:00:55,340
un pas h où je vais un peu devant le point a. Je diminuerai ce h à

8
00:00:55,340 --> 00:01:02,691
0 et je comparerai la valeur de f évaluée à x + h moins f (x) et je

9
00:01:02,691 --> 00:01:07,870
diviserai par la longueur du pas: ceci est la dérivée de f

10
00:01:07,870 --> 00:01:15,260
évaluée au point x, ou ici ce sera au point a. Il ne s’agit que d’une information de

11
00:01:15,260 --> 00:01:19,900
la pente, c'est une pente instantanée comme je teste ici, mais ensuite

12
00:01:19,900 --> 00:01:25,570
j'irai jusqu'à très près de moi et j'ai une information

13
00:01:25,570 --> 00:01:32,620
instantanée, très locale de cette pente près de moi. En

14
00:01:32,620 --> 00:01:37,970
physique, c'est aussi une vitesse: si x est le temps, c'est comme avoir la

15
00:01:37,970 --> 00:01:43,750
vitesse instantanée au temps t. L'approximation linéaire que je vais construire est si je

16
00:01:43,750 --> 00:01:51,340
veux calculer la valeur de f au point a + h à ce point je peux construire cette

17
00:01:51,340 --> 00:01:56,280
approximation linéaire en disant que cette valeur est approximativement égale à la

18
00:01:56,280 --> 00:02:02,280
valeur de f (a) plus le produit scalaire. C’est le produit scalaire, mais il est utilisé

19
00:02:02,280 --> 00:02:07,490
d’une manière différente dans une dimension: ce ne serait que le produit de

20
00:02:07,490 --> 00:02:15,120
la dérivée ici évalué au point a fois la longueur du

21
00:02:15,120 --> 00:02:20,210
pas. Cette approximation linéaire est représentée

22
00:02:20,210 --> 00:02:27,540
comme cette ligne-ci et si h est trop long, mon approximation linéaire

23
00:02:27,540 --> 00:02:32,209
ne sera pas très bonne pour une fonction non linéaire comme celle-ci. Nous aurons une

24
00:02:32,209 --> 00:02:40,109
erreur, l'erreur sera ce point moins ce point: ces deux ne seront pas

25
00:02:40,109 --> 00:02:47,099
proches si h est trop loin. Si je me rapproche de a en diminuant h, alors les deux

26
00:02:47,099 --> 00:02:52,569
lignes correspondent et ceci est une bonne approximation locale. Alors maintenant nous allons dans

27
00:02:52,569 --> 00:03:01,500
l’espace à n dimensions afin que je puisse le calculer pour mon réseau de neurones. Habituellement,

28
00:03:01,500 --> 00:03:06,849
vous pouvez penser à f comme la fonction de perte. La fonction de perte prendra n

29
00:03:06,849 --> 00:03:11,639
paramètres, par exemple 1 million de paramètres, et me retournera la perte

30
00:03:11,639 --> 00:03:17,650
moyenne sur tous les exemples. Maintenant, je veux prendre la dérivée de f par rapport à

31
00:03:17,650 --> 00:03:23,459
un paramètre et donc je vais le noter avec cette notation. Je choisis le paramètre

32
00:03:23,459 --> 00:03:28,360
i et je calcule la dérivée partielle de ma fonction de perte.

33
00:03:28,360 --> 00:03:33,499
Disons que dans mon espace de paramètres, je n'ai que trois dimensions:

34
00:03:33,499 --> 00:03:40,150
je choisirais une dimension et je vais tester la pente dans cette dimension. Si

35
00:03:40,150 --> 00:03:47,260
j'ai une autre dimension, je teste dans cette direction, c'est donc le e_i ici.

36
00:03:47,260 --> 00:03:55,389
Je le fais pour toute la dimension ici, pour toute la direction N et en faisant

37
00:03:55,389 --> 00:04:01,790
ainsi, j'ai un scalaire qui est la pente très localement où je suis. Chaque

38
00:04:01,790 --> 00:04:07,989
scalaire associé à une dimension, je les empilerai dans un vecteur: c'est

39
00:04:07,989 --> 00:04:12,859
ce que nous appellerons le gradient. Il me donne une information de la pente

40
00:04:12,859 --> 00:04:20,449
pour toute les directions dans mon espace. Nous pouvons voir ce que cette chose

41
00:04:20,449 --> 00:04:26,260
calcule: si je déplace un paramètre, combien la perte change-t-elle

42
00:04:26,260 --> 00:04:32,250
localement avec cette approximation linéaire?

43
00:04:32,250 --> 00:04:39,130
Donc, une fonction dérivable est une fonction où cette approximation linéaire

44
00:04:39,130 --> 00:04:47,400
ici en est une bonne, où si je prends l'étape h et que je la diminue à zéro,

45
00:04:47,400 --> 00:04:55,120
l'erreur que je fais près du point a ici sera nulle. Je

46
00:04:55,120 --> 00:05:02,620
peux écrire cette formule que je peux approximer --ma fonction -- par une

47
00:05:02,620 --> 00:05:11,780
approximation linéaire et cela est valide si h n'est pas trop grand. Donc ici nous pouvons

48
00:05:11,780 --> 00:05:18,860
voir que l'erreur converge vers zéro: c'est ce qu’est une fonction dérivable. Voici

49
00:05:18,860 --> 00:05:24,870
quelques exemples de fonction dérivables et leurs dérivées. Donc la dérivée d'une

50
00:05:24,870 --> 00:05:31,560
fonction linéaire, si c'est un scalaire et je la multiplie par W et j'ajoute le biais

51
00:05:31,560 --> 00:05:40,850
b, la dérivée par rapport à X ne sera que W. Pour la sigmoïde, c'est

52
00:05:40,850 --> 00:05:46,850
un peu plus intéressant, cette dérivée du sigmoïde est égal au

53
00:05:46,850 --> 00:05:51,940
sigmoïde fois un moins le sigmoïde. Donc la dérivée de la sigmoïde est

54
00:05:51,940 --> 00:05:59,470
exprimée en fonction de la sigmoïde elle-même. La ReLU est un peu plus

55
00:05:59,470 --> 00:06:04,380
délicate: rappelez-vous que la définition de la dérivée

56
00:06:04,380 --> 00:06:10,800
ici, c'est la limite de h allant à 0, mais je peux prendre la limite avec h de

57
00:06:10,800 --> 00:06:17,760
gauche à droite, donc je prends un h de négatif à positif ou un h

58
00:06:17,760 --> 00:06:26,070
de positif à négatif, et la valeur de ma dérivée devrait être la même.

59
00:06:26,070 --> 00:06:32,360
Cela ne dépendra pas de la direction du h que je prends, donc ici j'ai la

60
00:06:32,360 --> 00:06:37,810
fonction qui transforme toute valeur négative à 0 de

61
00:06:37,810 --> 00:06:42,910
sorte que la dérivée sera 0 0 0 et quand j'arrive au point 0. La

62
00:06:42,910 --> 00:06:48,210
dérivée est 0, et ici c'est la fonction d'identité: la dérivée de la

63
00:06:48,210 --> 00:06:53,501
fonction d' identité est 1 et j'ai donc 1 1 1 1 1 et lorsque j'arrive à 0, j'ai deux

64
00:06:53,501 --> 00:06:59,050
valeurs différentes, ce qui signifie que ReLU n'est pas dérivable à 0. Nous ne nous en

65
00:06:59,050 --> 00:07:06,650
soucions pas en apprentissage profond: empiriquement,

66
00:07:06,650 --> 00:07:10,750
ce problème n'a pas vraiment

67
00:07:10,750 --> 00:07:17,780
d’impact sur l'apprentissage. Même si nous atterrissons exactement à 0, choisir

68
00:07:17,780 --> 00:07:23,930
l'une des deux valeurs est correcte,

69
00:07:23,930 --> 00:07:30,210
cela n'affectera pas beaucoup le processus d'apprentissage. Donc cette fonction est définie en deux parties,

70
00:07:30,210 --> 00:07:36,640
1 pour cette partie et 0 pour cette partie. Une autre propriété de la dérivée

71
00:07:36,640 --> 00:07:41,940
qui est très utile en apprentissage profond est la composition de fonctions. J'aurai

72
00:07:41,940 --> 00:07:49,140
une fonction f qui prendra x et ensuite je donnerai ce résultat à g et

73
00:07:49,140 --> 00:07:56,700
g sera évalué au point f (x). Si je prends la dérivée de ce terme-ci, on

74
00:07:56,700 --> 00:08:00,970
obtient la dérivée de g évaluée à f (x) fois la

75
00:08:00,970 --> 00:08:11,480
dérivée de f évaluée à x. Je suis sûr que vous vous

76
00:08:11,480 --> 00:08:17,220
souvenez quand vous étiez au collège, vous faisiez cela à la main sur beaucoup de

77
00:08:17,220 --> 00:08:22,110
fonctions différentes, c'était douloureux et vous vous demandiez: « pourquoi je fais ça,

78
00:08:22,110 --> 00:08:28,330
quand est-ce que cela sera utile dans ma vie? » C'était pour aujourd'hui, votre professeur

79
00:08:28,330 --> 00:08:33,120
aurait dû vous dire que c'était seulement pour le cours d'apprentissage profond, mais vraiment

80
00:08:33,120 --> 00:08:38,339
la composition de fonction est quelque chose qui est partout dans ces

81
00:08:38,339 --> 00:08:44,450
graphes de calcul parce que, quand je donne mes données x à une fonction f, le module

82
00:08:44,450 --> 00:08:48,270
me retournera la sortie et cette sortie sera donnée à un autre

83
00:08:48,270 --> 00:08:53,440
module. Alors, quand je relie des modules comme ceci, vraiment

84
00:08:53,440 --> 00:08:57,180
ce qui se passe est une composition de fonction.

85
00:08:57,180 --> 00:09:02,150
Les modules réutilisent toujours le résultat de la fonction précédente pour calculer

86
00:09:02,150 --> 00:09:03,940
le suivant.
