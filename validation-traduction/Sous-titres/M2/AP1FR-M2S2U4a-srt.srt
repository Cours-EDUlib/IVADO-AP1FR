1
00:00:10,170 --> 00:00:22,070
Nous allons donc essayer d’implémenter un algorithme pour calculer ce gradient de z, le Nabla

2
00:00:22,070 --> 00:00:28,470
z. C'est donc la définition d’une composante de ce vecteur. Ceci est

3
00:00:28,470 --> 00:00:35,710
un vecteur, ceci est un vecteur. Donc un composant du vecteur, le composant j

4
00:00:35,710 --> 00:00:41,940
est égal à ceci. D'abord, ce que je dois faire pour évaluer la

5
00:00:41,940 --> 00:00:48,400
dérivée de g_i est de calculer la somme ici pour obtenir la valeur de mon y. Si je

6
00:00:48,400 --> 00:00:54,489
n'ai pas la valeur de mon y, je ne peux pas calculer cette fonction. Donc je sommerai,

7
00:00:54,489 --> 00:00:59,100
je ferai une propagation avant: je vais partir de ces nœuds ici et je

8
00:00:59,100 --> 00:01:06,220
vais calculer les valeurs de "y" ici, donc c'est ce que ce terme est.

9
00:01:06,220 --> 00:01:12,630
J'ai les valeurs de ma variable y ici et je peux stocker ce

10
00:01:12,630 --> 00:01:20,350
résultat intermédiaire dans le nœud et je peux continuer. Je peux aussi calculer z, mais

11
00:01:20,350 --> 00:01:26,670
pour calculer la valeur que je recherche pour ce gradient, je n'ai pas besoin de z

12
00:01:26,670 --> 00:01:33,960
ici dans cette formule. Alors maintenant je vais commencer à la fin du

13
00:01:33,960 --> 00:01:39,700
graphe de calcul et je vais revenir en arrière. Alors maintenant, je peux évaluer la

14
00:01:39,700 --> 00:01:47,409
dérivée de ma fonction g_i à y_i, c'est la valeur de y qui a été

15
00:01:47,409 --> 00:01:53,450
calculeé avant lors de la propagation avant et je peux stocker le résultat intermédiaire:

16
00:01:53,450 --> 00:02:01,100
cette partie, je vais la stocker sur le lien, puis je continue. Je peux calculer

17
00:02:01,100 --> 00:02:06,610
cette quantité ici qui sera associée à un lien, c'est la

18
00:02:06,610 --> 00:02:14,360
dérivée de la fonction f du nœud j à i évaluée à son

19
00:02:14,360 --> 00:02:22,200
j et je peux le dire ici. Ce que nous pouvons voir est que cette expression est le

20
00:02:22,200 --> 00:02:30,370
produit de ce terme qui a été calculé et stocké sur cette arête avec ce

21
00:02:30,370 --> 00:02:35,180
terme qui a été calculé et stocké sur cette arête. Donc, je fais le produit de

22
00:02:35,180 --> 00:02:38,409
la valeur de cette arête avec cette arête.

23
00:02:38,409 --> 00:02:45,239
Pour calculer cette composante du gradient, je peux convertir le

24
00:02:45,239 --> 00:02:49,290
problème du calcul de cette approximation linéaire du gradient

25
00:02:49,290 --> 00:02:56,019
en un algorithme de traversée de graphe.Jje vais partir de la fin et je vais calculer

26
00:02:56,019 --> 00:03:00,409
ces expressions bord par bord, puis je dois multiplier ces expressions

27
00:03:00,409 --> 00:03:06,599
ensemble et les additionner pour que cette expression soit le produit de la valeur associée

28
00:03:06,599 --> 00:03:11,060
à cette arête avec cette arête plus la valeur de cette arête avec cette arête plus

29
00:03:11,060 --> 00:03:17,200
la valeur de cette arête avec cette arête. Y a-t-il un moyen de faire ceci plus

30
00:03:17,200 --> 00:03:23,790
efficacement? Je n'utiliserai pas l'équation ici, je voudrais juste ajouter une

31
00:03:23,790 --> 00:03:31,750
couche à ce graphique et donc pour calculer le gradient de la sortie par

32
00:03:31,750 --> 00:03:38,060
rapport à une entrée, c'est un problème de traversée de graphe: je dois parcourir tous les

33
00:03:38,060 --> 00:03:45,019
arêtes de ce nœud à ce nœud et donc je peux calculer le résultat intermédiaire sur

34
00:03:45,019 --> 00:03:50,020
cette arête, puis je fais le produit de ces trois valeurs. Si nous regardons

35
00:03:50,020 --> 00:03:56,079
attentivement, après avoir calculé le gradient par rapport à ce nœud, alors je

36
00:03:56,079 --> 00:04:00,800
calculerai le gradient avec ce nœud, mais la valeur associée à ces deux

37
00:04:00,800 --> 00:04:07,890
nœuds a déjà été calculée avant. Au lieu de simplement calculer une fois

38
00:04:07,890 --> 00:04:16,420
puis en effacer les informations, je devrais réutiliser ces informations d'une

39
00:04:16,420 --> 00:04:23,660
certaine manière et la rétropropagation est un algorithme très efficace pour le faire.

40
00:04:23,660 --> 00:04:29,700
Alors, au lieu de suivre toutes les arêtes de ce noeud à ce noeud sans regarder les

41
00:04:29,700 --> 00:04:34,730
autres noeuds, je vais le faire de manière systématique basée sur la programmation dynamique. Donc, à

42
00:04:34,730 --> 00:04:39,920
partir de ce noeud, je vais calculer les valeurs de la dérivée de la fonction

43
00:04:39,920 --> 00:04:48,530
évaluée aux valeurs de ces variables. Je le ferai pour chaque

44
00:04:48,530 --> 00:04:54,420
arête provenant de ce nœud, puis je passe à la couche suivante. Je peux

45
00:04:54,420 --> 00:05:01,450
calculer toutes les informations ici et je peux déjà faire mon produit pour que

46
00:05:01,450 --> 00:05:07,570
je puisse calculer le produit de cette arête avec cette arête ici. À ce point-ci, j’ai

47
00:05:07,570 --> 00:05:12,670
toutes les informations: je n'ai pas à regarder en arrière à l'étape précédente pour implémenter

48
00:05:12,670 --> 00:05:18,580
cette idée de programmation dynamique. Maintenant, j'ai la dernière couche que je peux faire

49
00:05:18,580 --> 00:05:23,670
et donc je réutilise toutes les informations. Je n'efface pas et je ne recalcule aucune

50
00:05:23,670 --> 00:05:32,110
information, cela est fait de manière efficace. La rétropropagation a été

51
00:05:32,110 --> 00:05:41,490
proposée par Seppo Linnainmaa en 1970 et elle a été proposée dans la

52
00:05:41,490 --> 00:05:46,910
communauté de la dérivation automatique. Les physiciens étaient fatigués de

53
00:05:46,910 --> 00:05:51,880
calculer leur dérivés à la main, ils voulaient que les programmes le fassent pour eux, donc

54
00:05:51,880 --> 00:05:57,180
il y avait beaucoup de travail sur la dérivation automatisée et cet

55
00:05:57,180 --> 00:06:03,690
étudiant de maîtrise a trouvé cet algorithme pour le faire efficacement

56
00:06:03,690 --> 00:06:09,410
basé sur la programmation dynamique. La première fois que ça a été appliqué à l’entraînement d'un

57
00:06:09,410 --> 00:06:16,870
réseau de neurons, c'était par Rumelhart et Hinton (et Williams) en 1986 et il y a donc eu

58
00:06:16,870 --> 00:06:20,990
beaucoup de temps entre les deux pour vraiment comprendre que le réseau neuronal comme nous

59
00:06:20,990 --> 00:06:21,990
l’avons

60
00:06:21,990 --> 00:06:26,100
vu dans la première conférence peut être interprété comme un

61
00:06:26,100 --> 00:06:31,270
graphe de calcul. Donc, la rétropropagation sera utile pour calculer

62
00:06:31,270 --> 00:06:39,330
le gradient: il y avait 16 ans entre les deux publications, et maintenant ce que nous

63
00:06:39,330 --> 00:06:43,120
disons, c'est que nous faisons une programmation différentielle lorsque nous

64
00:06:43,120 --> 00:06:49,080
créons ce graphique qui est entièrement dérivable, où nous pouvons calculer

65
00:06:49,080 --> 00:06:54,380
l'impact de tout paramètre sur la fonction de perte à la sortie en utilisant cette

66
00:06:54,380 --> 00:07:02,180
technique et en utilisant les dérivées. Maintenant chaque fois que nous programmons, nous avons

67
00:07:02,180 --> 00:07:08,930
cette notion sous-jacente de « dérivable » à l'architecture et nous pouvons

68
00:07:08,930 --> 00:07:14,490
avoir un modèle très complexe comme le ResNet ici qui est

69
00:07:14,490 --> 00:07:15,430
dérivable de bout en bout.
