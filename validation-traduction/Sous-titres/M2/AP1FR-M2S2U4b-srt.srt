1
00:00:14,150 --> 00:00:18,160
Bon alors maintenant nous allons revoir un peu plus notre approche modulaire de

2
00:00:18,160 --> 00:00:23,930
l’apprentissage profond et nous allons définir un module nous-mêmes et qu'est-ce que cela signifie de

3
00:00:23,930 --> 00:00:29,760
définir un module en termes de programmation. La première chose à faire est

4
00:00:29,760 --> 00:00:35,320
de définir les paramètres de ce module donc ici je veux implémenter

5
00:00:35,320 --> 00:00:41,170
le module dense que nous avons déjà vu. C’est une transformation linéaire, ici

6
00:00:41,170 --> 00:00:48,610
je vais avoir ma matrice W. Mon module prendra x comme entrée, je

7
00:00:48,610 --> 00:00:56,040
prendrai la multiplication matricielle de W et x plus ce biais b.

8
00:00:56,040 --> 00:01:00,400
Ce sera la transformation, ce sera comment je prends une donnée en entrée et comment je la

9
00:01:00,400 --> 00:01:05,229
transforme en sortie. Au début on ne sait pas quelles

10
00:01:05,229 --> 00:01:10,300
seront les valeurs W, donc on va initialiser ce module à des valeurs aléatoires et on va laisser

11
00:01:10,300 --> 00:01:14,080
le modèle apprend les valeurs de W et b.

12
00:01:14,080 --> 00:01:16,659
W et b sont mes paramètres.

13
00:01:16,659 --> 00:01:21,280
Je définis comment les informations seront transformées dans une

14
00:01:21,280 --> 00:01:28,280
direction, donc nous prendrons cet exemple simple où je construis un

15
00:01:28,280 --> 00:01:35,370
graphe de calcul très simple. Ici, je représente ces images comme de petits

16
00:01:35,370 --> 00:01:40,340
vecteurs à 4 dimensions, c'est seulement pour l'exercice. Ce vecteur sera

17
00:01:40,340 --> 00:01:44,630
traité par un module dense, j'obtiendrai une autre représentation d'une

18
00:01:44,630 --> 00:01:50,700
longueur différente. Cette représentation sera traitée par une

19
00:01:50,700 --> 00:01:56,940
fonction d'activation: ici j'ai choisi un ReLU. J'obtiens donc un nouvelle représentation et celle-

20
00:01:56,940 --> 00:02:03,729
ci sera donnée à un module dense qui enverra cette représentation à la bonne

21
00:02:03,729 --> 00:02:10,340
sortie. Ici, je veux juste détecter est-ce un 2 ou pas: je ne veux qu'un scalaire

22
00:02:10,340 --> 00:02:15,620
à la sortie et le dense peut le faire, car je veux prédire la probabilité

23
00:02:15,620 --> 00:02:20,590
de 2. J’utiliserai la fonction sigmoïde. Ici, j'aurai ma probabilité, je la

24
00:02:20,590 --> 00:02:23,799
donnerai à un module appelé la fonction de perte et ici,

25
00:02:23,799 --> 00:02:27,920
c'est différent de la métrique de performance: la métrique de performance doit être

26
00:02:27,920 --> 00:02:33,900
utilisable pour tout type d'algorithme, tout type de modèle. Ici, nous utilisons une

27
00:02:33,900 --> 00:02:38,590
fonction de perte qui sera dérivable et toutes les métriques de performance ne sont pas

28
00:02:38,590 --> 00:02:44,480
dérivables, donc nous utilisons un substitut. Si votre métrique de performance est

29
00:02:44,480 --> 00:02:47,879
dérivable, utilisez-la, c'est vraiment la bonne chose à faire

30
00:02:47,879 --> 00:02:53,970
parce que vous optimiserez la métrique réelle. Si votre

31
00:02:53,970 --> 00:02:58,969
métrique de performance n'est pas dérivable, gardez votre métrique de performance et essayez de trouver un

32
00:02:58,969 --> 00:03:04,099
substitut qui l'estimera bien, qui l'approximera bien, mais

33
00:03:04,099 --> 00:03:08,879
qui sera dérivable. Ce module ce module de perte aura deux

34
00:03:08,879 --> 00:03:15,309
entrées: la prédiction de mon modèle après le sigmoïde et la cible réelle. Il

35
00:03:15,309 --> 00:03:24,370
me retournera une valeur de perte pour cet exemple spécifique. Alors maintenant je suis intéressé

36
00:03:24,370 --> 00:03:28,890
à changer les valeurs de mes paramètres. Maintenant, nous avons juste un

37
00:03:28,890 --> 00:03:34,209
paramètre par défaut. Peut-être que je l'initialise avec une distribution uniforme, ce ne sont que des

38
00:03:34,209 --> 00:03:39,840
variables aléatoires et le modèle ne fonctionnera pas bien: c'est juste une transformation aléatoire. Je veux

39
00:03:39,840 --> 00:03:45,040
changer les paramètres de mon modèle pour minimiser la perte et nous utiliserons

40
00:03:45,040 --> 00:03:50,499
ce concept de dérivée pour essayer de calculer, en changeant un

41
00:03:50,499 --> 00:03:55,650
paramètre, si ma perte augmente ou diminue. Avec cette information, je

42
00:03:55,650 --> 00:04:00,849
changerai petit à petit tous mes paramètres pour minimiser la perte

43
00:04:00,849 --> 00:04:06,590
sur l'ensemble d'entraînement. Alors, le changement sur ce paramètre, si je l’augmente, est-ce 

44
00:04:06,590 --> 00:04:11,959
que ça aura un impact positif ou négatif sur cet exemple d’entraînement? C'est une question à laquelle

45
00:04:11,959 --> 00:04:14,169
nous voulons répondre.

46
00:04:14,169 --> 00:04:21,780
Une chose que nous pouvons faire est de ne pas utiliser la rétropropagation.

47
00:04:21,780 --> 00:04:27,010
Le calcul de la dérivée semble trop complexe, nous venons d’implémenter la

48
00:04:27,010 --> 00:04:32,930
façon dont une entrée sera transformée en sortie et nous pensons que cela suffit pour

49
00:04:32,930 --> 00:04:38,139
calculer le changement de ces paramètres sur la sortie. Il y a un moyen de le faire:

50
00:04:38,139 --> 00:04:44,159
je prendrai ma fonction de perte pour cet exemple avec les

51
00:04:44,159 --> 00:04:49,580
paramètres actuels de tout mon réseau, je vais choisir ce paramètre ici je vais

52
00:04:49,580 --> 00:04:54,720
l’augmenter de 10 à la puissance de moins 6 juste pour voir si je bouge

53
00:04:54,720 --> 00:04:59,699
un peu ce paramètre comment ma perte va changer. Je vais le diviser par la

54
00:04:59,699 --> 00:05:04,389
taille de la modification de ce paramètre et cela me donnera un valeur de, si je

55
00:05:04,389 --> 00:05:10,150
change par Epsilon i, combien ma perte va augmenter ou diminuer. C'est un

56
00:05:10,150 --> 00:05:14,960
algorithme valide et en fait c'est vraiment une méthode de différence finie pour

57
00:05:14,960 --> 00:05:23,740
vous qui êtes familier avec le calcul de ce genre de chose en physique. Si vous

58
00:05:23,740 --> 00:05:28,250
voulez le faire correctement, vous devriez probablement utiliser cette formule, c’est la perte

59
00:05:28,250 --> 00:05:34,379
pour les paramètres moins Epsilon i et nous le comparons aux mêmes paramètres, mais

60
00:05:34,379 --> 00:05:39,430
avec celui-ci plus epsilon i et nous le divisons par 2 Epsilon i. C’est une

61
00:05:39,430 --> 00:05:43,640
méthode de différence finie pour approximer la dérivée partielle de la

62
00:05:43,640 --> 00:05:51,210
perte par rapport à epsilon i. Lorsque je programmais la rétropropagation, j’utilisais

63
00:05:51,210 --> 00:05:57,639
la méthode de différence finie pour tester mon implémentation. Si je prends un

64
00:05:57,639 --> 00:06:05,409
Epsilon i très petit, comme 10 à la puissance de moins 6, la sortie de

65
00:06:05,409 --> 00:06:10,200
l'algorithme de rétropropagation devrait être très proche de celui-ci et 

66
00:06:10,200 --> 00:06:16,189
vice-versa parce que celui-ci est une approximation. Alors, quelle est

67
00:06:16,189 --> 00:06:26,590
la complexité de cette méthode de différence finie? Si j'ai n paramètres,

68
00:06:26,590 --> 00:06:31,830
pour la différence finie j'ai besoin de deux propagations avant. J'ai besoin d'un « avant »

69
00:06:31,830 --> 00:06:35,490
où je changerai les paramètres. Ici je prendrai

70
00:06:35,490 --> 00:06:39,939
l'image, je propagerai en avant pour récupérer la perte, puis je ferai la

71
00:06:39,939 --> 00:06:45,520
même chose mais en augmentant par Epsilon i au lieu de supprimer Epsilon i.

72
00:06:45,520 --> 00:06:47,610
Il y a donc deux propagations avant dans mon

73
00:06:47,610 --> 00:06:50,300
modèle et je dois le faire pour chaque

74
00:06:50,300 --> 00:06:55,759
paramètre. Comme je l'ai dit, en apprentissage profond, nous avons des centaines de millions de

75
00:06:55,759 --> 00:07:01,449
paramètres dans les modèles plus complexes. Donc, il y a deux fois des millions de propagations avant

76
00:07:01,449 --> 00:07:02,449
pour

77
00:07:02,449 --> 00:07:07,830
avoir juste savoir si je change l'un des paramètres comment la perte va

78
00:07:07,830 --> 00:07:13,120
changer. Je n'ai pas encore mis à jour de paramètres, donc il est super lent d'utiliser la

79
00:07:13,120 --> 00:07:21,039
méthode des différences finies - pour voir l’impact d'un paramètre. Donc, à la place, nous

80
00:07:21,039 --> 00:07:27,870
utiliserons la rétropropagation. Avec la rétropropagation, vous verrez que c'est seulement

81
00:07:27,870 --> 00:07:35,660
une propagation avant et une rétroporgation. Pour cet exemple, la complexité en termes de

82
00:07:35,660 --> 00:07:42,960
la propagation avant ne dépend pas du nombre de paramètres: il ne faut que deux traversées du graphe

83
00:07:42,960 --> 00:07:47,650
au lieu de millions de traversées du graphe, c'est pourquoi la rétropropagation est

84
00:07:47,650 --> 00:07:48,419
si importante.
