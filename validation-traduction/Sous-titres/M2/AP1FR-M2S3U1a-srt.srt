1
00:00:14,120 --> 00:00:20,070
La prochaine conférence porte sur l'optimisation. Dans la conférence précédente,

2
00:00:20,070 --> 00:00:26,480
nous avons vu l'intuition derrière l'apprentissage profond, ce qu’est un graphe de calcul, puis

3
00:00:26,480 --> 00:00:29,750
comment calculer efficacement le gradient qui nous donnera les

4
00:00:29,750 --> 00:00:35,970
informations sur la façon de modifier les paramètres pour minimiser la perte. Par contre, nous avons encore besoin

5
00:00:35,970 --> 00:00:43,399
d’un autre concept, l'optimisation. Comment puis-je utiliser ces informations de gradient

6
00:00:43,399 --> 00:00:49,289
pour modifier les paramètres aussi efficacement que possible pour minimiser la perte sur

7
00:00:49,289 --> 00:00:56,980
l’ensemble d’entraînement? Un rappel: les ingrédients pour l'apprentissage profond supervisé, nous avons vu

8
00:00:56,980 --> 00:01:02,570
ces deux ce matin et maintenant, nous examinerons l'optimiseur de paramètres. Pourquoi

9
00:01:02,570 --> 00:01:06,430
parlons-nous d'optimisation? C'est parce que nous avons tout d'abord cette

10
00:01:06,430 --> 00:01:11,680
métrique d'évaluation: nous voulons maximiser les performances de notre modèle et nous avons vu

11
00:01:11,680 --> 00:01:15,410
que nous utilisons une perte qui est dérivable, un substitut pour cette

12
00:01:15,410 --> 00:01:21,470
métrique de performance. Nous voudrions minimiser la perte par rapport aux

13
00:01:21,470 --> 00:01:26,610
paramètres. Quand nous disons minimiser maximiser, il y a un

14
00:01:26,610 --> 00:01:31,290
problème d'optimisation sous-jacent et il y a beaucoup de littérature sur

15
00:01:31,290 --> 00:01:35,750
l’optimisation, c'est tout un domaine des mathématiques appliquées. Nous verrons

16
00:01:35,750 --> 00:01:41,660
quels concepts nous pouvons tirer de cette littérature pour construire un optimiseur pour les

17
00:01:41,660 --> 00:01:47,710
réseaux d'apprentissage profonds. Les concepts que nous verrons dans cette présentation

18
00:01:47,710 --> 00:01:54,920
sont le préconditionnement de momentum et aussi quel genre d'optimisation on fait,

19
00:01:54,920 --> 00:02:01,060
et nous verrons que c'est de l’optimisation stochastique. Je vais donc commencer par un

20
00:02:01,060 --> 00:02:06,030
problème d'optimisation très simple et j'expliquerai les difficultés que nous

21
00:02:06,030 --> 00:02:10,700
pouvons avoir avec des modèles qui ont beaucoup de paramètres. Nous verrons aussi des solutions à

22
00:02:10,700 --> 00:02:16,450
ces problèmes. Je vais d'abord devoir définir qu’est-ce qu’un minimum. Il y avait une

23
00:02:16,450 --> 00:02:19,520
question sur le minimum local et le minimum global,

24
00:02:19,520 --> 00:02:27,200
définissons-les ici. Donc quand j'utilise f, vous pouvez l'interpréter comme la fonction de perte,

25
00:02:27,200 --> 00:02:33,840
c'est la perte moyenne sur tous mes exemples d’entraînement. Cette perte prendra donc

26
00:02:33,840 --> 00:02:37,200
les valeurs des paramètres de mon graphe de calcul

27
00:02:37,200 --> 00:02:42,260
me retournera un seul scalaire pour me dire en moyenne si mon modèle

28
00:02:42,260 --> 00:02:48,489
fonctionne bien ou non. Ce sera donc une entrée de cette fonction, ce sont les

29
00:02:48,489 --> 00:02:54,760
paramètres. On voit qu'on a un optimum global si, pour tout autres

30
00:02:54,760 --> 00:03:00,819
paramètres dans l'espace des paramètres, la valeur de la perte pour ce

31
00:03:00,819 --> 00:03:06,030
minimum global est la meilleure: il n'y a pas d'autre modèle qui peut minimiser plus cette

32
00:03:06,030 --> 00:03:13,299
perte moyenne. Alors, pour tous les modèles dans cet espace des paramètres, ma perte est le minimum

33
00:03:13,299 --> 00:03:19,739
globalement. Généralement, le minimiseur global, c'est trop demandant:

34
00:03:19,739 --> 00:03:24,749
nos réseaux de neurones profonds ont des millions de paramètres. Pouvons-nous redéfinir le

35
00:03:24,749 --> 00:03:29,290
minimiseur global qui minimiserait la perte moyenne sur l'ensemble d'entraînement?

36
00:03:29,290 --> 00:03:35,170
Ce que nous considérerons est plus le minimum local: j'aurai donc

37
00:03:35,170 --> 00:03:41,019
un modèle ici avec les valeurs des paramètres qui ont été définis, je

38
00:03:41,019 --> 00:03:47,370
définirai une boule autour d'ici, un petit quartier. Pour tous les modèles à l'intérieur de

39
00:03:47,370 --> 00:03:53,079
ce quartier, ce modèle a les meilleures performances avec la perte minimale.

40
00:03:53,079 --> 00:03:57,659
Ce n'est donc pas global ,c'est vraiment local autour de ce modèle: ça s'appelle un

41
00:03:57,659 --> 00:04:04,439
minimum local. Il existe un epsilon, un rayon tel que j'ai cette

42
00:04:04,439 --> 00:04:12,059
inégalité. Voyons un exemple en 1D: je n'ai qu'un seul paramètre et c'est la

43
00:04:12,059 --> 00:04:19,070
perte pour toutes les valeurs de ce paramètre. Je vais identifier mon minimum,

44
00:04:19,070 --> 00:04:22,860
alors j'ai un minimum global ici: vous pouvez voir qu'il n'y a pas d'autre point de

45
00:04:22,860 --> 00:04:28,600
f (x) qui ira plus bas que ce point. J'ai aussi un

46
00:04:28,600 --> 00:04:33,490
minimum local ici, car je peux prendre le quartier avec Epsilon

47
00:04:33,490 --> 00:04:39,759
fixe pour limiter cela à cette plage. Vous pouvez voir que dans cette plage,

48
00:04:39,759 --> 00:04:44,460
ce point est le minimum, mais ce n'est pas le minimum global: c'est seulement dans ce

49
00:04:44,460 --> 00:04:49,400
quartier que c'est le minimum. Nous allons donc revenir à notre

50
00:04:49,400 --> 00:04:53,830
concept d'approximation linéaire: vous voyez qu'une approximation linéaire pour une

51
00:04:53,830 --> 00:04:59,229
fonction est cette ligne ici, donc nous avons cette ligne qui se

52
00:04:59,229 --> 00:05:06,430
rapprochera bien de notre fonction de perte dans ce quartier. Cependant, nous avons une

53
00:05:06,430 --> 00:05:12,180
erreur de prédiction si h devient trop grand parce que ma fonction n’est pas linéaire. 

54
00:05:12,180 --> 00:05:19,470
Regardons alors la dérivée ou le gradient de la perte

55
00:05:19,470 --> 00:05:23,930
par rapport aux paramètres au minimum global et local: ils sont

56
00:05:23,930 --> 00:05:29,650
plats, la dérivée est nulle et c'est donc peut-être un bon moyen de trouver le

57
00:05:29,650 --> 00:05:35,139
minimum local ou le minimum global. Vous pouvez voir que mon approximation linéaire est plate

58
00:05:35,139 --> 00:05:40,849
ici, donc dans mon approximation linéaire, cela signifie que ce terme est nul. Cela signifie que

59
00:05:40,849 --> 00:05:45,830
si je me déplace un peu à gauche ou à droite, ma valeur ne changera pas

60
00:05:45,830 --> 00:05:52,099
beaucoup. C'est tout ce que cette équation nous dit: suffit-il donc d'avoir la

61
00:05:52,099 --> 00:05:58,220
dérivée égale à zéro pour avoir un minimum local ou un minimum global? La réponse est

62
00:05:58,220 --> 00:06:04,580
« certainement pas » parce que nous avons une pente égale à zéro pour le maximum global et pour

63
00:06:04,580 --> 00:06:10,699
le maximum local: ici, je peux avoir un quartier autour de ce point et

64
00:06:10,699 --> 00:06:16,580
c'est le plus haut, mais je peux aussi avoir un point de selle. Un point de selle n'est pas un

65
00:06:16,580 --> 00:06:22,099
minimum ou un maximum pour n'importe quelle taille du quartier, c'est seulement un plateau où

66
00:06:22,099 --> 00:06:26,379
la perte ne change pas par rapport à ce x.

67
00:06:26,379 --> 00:06:30,700
Donc, en général, nous dirons qu'il y a des points critiques: ce sont les points

68
00:06:30,700 --> 00:06:37,430
où la dérivée évaluée à ces points est égale à zéro. Nous avons donc

69
00:06:37,430 --> 00:06:44,169
un vieux théorème disant que si nous avons un extremum, alors c'est un point critique.

70
00:06:44,169 --> 00:06:49,889
Donc pour tout maximum ou minimum, la dérivée sera égale à zéro.

71
00:06:49,889 --> 00:06:55,349
C’est une bonne chose: je peux caractériser le minimum et le maximum avec cette

72
00:06:55,349 --> 00:07:00,949
propriété, mais si j'ai une dérivée égale à zéro, alors je ne peux pas conclure avec

73
00:07:00,949 --> 00:07:07,860
certitude que j'ai un global ou un minimum. Cela pourrait être un point de selle ici:

74
00:07:07,860 --> 00:07:12,840
c'est seulement pour vous montrer quelques concepts. Vous pouvez voir que je considère un paramètre,

75
00:07:12,840 --> 00:07:17,729
en apprentissage profond, nous avons des millions de paramètres. Juste pour être un

76
00:07:17,729 --> 00:07:24,319
peu impressionné par la complexité, je vous montre ces images où les gens inventaient

77
00:07:24,319 --> 00:07:28,590
un moyen très intelligent de projeter le paysage d'optimisation en millions

78
00:07:28,590 --> 00:07:33,719
de dimension en quelque chose que nous pouvons visualiser. Cela a été fait pour les

79
00:07:33,719 --> 00:07:43,500
modèles VGG-56 et VG-110: vous pouvez voir à quel point il est difficile de naviguer dans ce

80
00:07:43,500 --> 00:07:48,000
paysage et ce n'est pas la situation réelle parce que la situation réelle vit

81
00:07:48,000 --> 00:07:54,560
dans 1 million de dimensions, mais ils utilisaient de bonnes propriétés pour générer cela.

82
00:07:54,560 --> 00:08:02,219
Nous pouvons voir à quel point il peut être complexe de partir de n'importe quel point et de trouver un

83
00:08:02,219 --> 00:08:07,990
minimum global ou local. Interprétons un peu plus les

84
00:08:07,990 --> 00:08:13,979
minimums local et global: c'est lié à la question de ce matin où vous avez demandé

85
00:08:13,979 --> 00:08:20,599
si nous pouvions atterrir dans un mauvais minimum local. Une façon d'éviter un mauvais minimum local:

86
00:08:20,599 --> 00:08:27,830
tout d'abord, si j'initialise mon modèle de manière incorrecte, je peux atterrir quelque part où je

87
00:08:27,830 --> 00:08:32,770
n'ai aucun gradient, alors peut-être que j'atterrirai ici.

88
00:08:32,770 --> 00:08:37,060
Sans gradient, je ne sais pas si je dois aller à droite ou à gauche, car

89
00:08:37,060 --> 00:08:41,140
il n'y a pas de gradient du tout: il n'y a pas de direction qui me dira

90
00:08:41,140 --> 00:08:46,370
comment je peux changer les paramètres pour améliorer mes performances. Ce point

91
00:08:46,370 --> 00:08:51,500
serait donc une mauvaise initialisation, car la perte est élevée et je n'ai pas de

92
00:08:51,500 --> 00:08:56,829
gradient. Ici, je peux avoir des difficultés car c'est similaire: je peux

93
00:08:56,829 --> 00:09:02,000
commencer ici, je peux avoir un certain gradient où je vais voir mon modèle apprendre et

94
00:09:02,000 --> 00:09:07,990
quand j'arrive au point de selle, mon gradient va à zéro. Alors maintenant, je peux penser que

95
00:09:07,990 --> 00:09:12,610
mon modèle n'apprend pas. Peut-être que je peux penser que c'est un minimum local, mais nous

96
00:09:12,610 --> 00:09:17,370
ne pouvons pas le savoir avec certitude parce que nous n'avons pas de conditions pour tester et voir

97
00:09:17,370 --> 00:09:21,650
si c'est un point de selle ou un minimum local. Donc ici nous aurons

98
00:09:21,650 --> 00:09:27,110
des difficultés. Ici, c'est le minimum global. Est-ce une bonne chose

99
00:09:27,110 --> 00:09:34,010
d’avoir la perte minimale sur l'ensemble d'entraînement? Peut-être pas parce que nous devons également

100
00:09:34,010 --> 00:09:39,260
surveiller la perte sur l'ensemble de validation. Si la perte de mon

101
00:09:39,260 --> 00:09:45,500
ensemble d'entraînement est dix à la moins douze ou encore plus petit que cela, on

102
00:09:45,500 --> 00:09:52,440
aura généralement surappris, car pour obtenir quelque chose d'aussi proche de zéro, le

103
00:09:52,440 --> 00:09:56,690
modèle devra à un moment donné mémoriser les exemples qui sont très

104
00:09:56,690 --> 00:10:01,380
difficiles dans l'ensemble d’entraînement. Lorsque le modèle mémorisera ces exemples, il

105
00:10:01,380 --> 00:10:05,380
ne généralisera pas aussi bien: nous devons nous rappeler probablement de l’arrêt précoce qu’on a vu

106
00:10:05,380 --> 00:10:10,000
hier: nous devrions probablement arrêter l’entraînement à un point où il n'a pas

107
00:10:10,000 --> 00:10:16,410
convergé au minimum global en termes d'optimisation. Nous disons ici que

108
00:10:16,410 --> 00:10:21,110
c'est un bon modèle, car maintenant, nous ne pouvons pas le voir sur cette image, mais c'est un

109
00:10:21,110 --> 00:10:27,220
minimum local, c'est relativement bas et je peux confirmer avec mes informations sur mon

110
00:10:27,220 --> 00:10:31,820
ensemble de validation que je n'ai pas de surapprentissage. Je suis donc satisfait de la

111
00:10:31,820 --> 00:10:37,420
mesure de performance. Nous ne voulons donc pas nécessairement trouver un minimum global,

112
00:10:37,420 --> 00:10:44,269
les bon minima locaux sont mieux. Aussi, pour répondre à la question sur le mauvais

113
00:10:44,269 --> 00:10:50,290
minimum local, il y a eu beaucoup de recherches et les gens pensaient vraiment

114
00:10:50,290 --> 00:10:53,970
que l'un des problèmes de l'apprentissage profond était les mauvais minima locaux, mais

115
00:10:53,970 --> 00:10:59,730
apparemment c'est vraiment une question de point de selle: à un moment donné, nous

116
00:10:59,730 --> 00:11:04,519
arriverons quelque part dans l'espace des paramètres où le gradient diminuera

117
00:11:04,519 --> 00:11:09,540
à zéro et en fait, c'est juste parce que les paramètres n'ont pas

118
00:11:09,540 --> 00:11:14,550
d’impact sur la fonction de perte. Supposons que vous avez une bonne initialisation

119
00:11:14,550 --> 00:11:19,220
et que pour une raison quelconque, tous les sigmoïdes de votre modèle saturent. Alors, c'est un

120
00:11:19,220 --> 00:11:25,350
point de selle: vous pouvez diminuer peut-être certains paramètres et en augmenter d'autres

121
00:11:25,350 --> 00:11:30,100
et vous dépasserez le point de selle. Par contre, le problème principal n'est pas l’existence

122
00:11:30,100 --> 00:11:36,510
de mauvais minima locaux, c'est plus les point de selle. Voyons maintenant comment nous pouvons classifier 

123
00:11:36,510 --> 00:11:42,820
notre point critique: nous avons besoin d'une autre information, un terme de second ordre ici.

124
00:11:42,820 --> 00:11:46,440
Nous avons donc notre approximation linéaire, c'est un produit scalaire entre le

125
00:11:46,440 --> 00:11:52,670
gradient et le pas h. Nous allons calculer cette quantité f ' ‘ de a,

126
00:11:52,670 --> 00:11:58,140
c’est la hessienne de la fonction: la hessienne est une matrice ou chaque

127
00:11:58,140 --> 00:12:02,560
composant de cette matrice est la deuxième dérivée partielle des

128
00:12:02,560 --> 00:12:09,730
variables. Nous verrons comment cette matrice est organisée et nous faisons donc

129
00:12:09,730 --> 00:12:14,199
un autre type d'approximation de notre fonction. Nous verrons ce que c'est et nous

130
00:12:14,199 --> 00:12:19,310
utilisons ce terme. Je multiplie donc ma matrice avec un vecteur et je prends le

131
00:12:19,310 --> 00:12:24,890
vecteur résultant. Je prends le produit scalaire avec h et nous désignerons cette

132
00:12:24,890 --> 00:12:32,480
fonction comme q(h) ici. Si c'est un minimum, alors cette nouvelle approximation,

133
00:12:32,480 --> 00:12:37,550
qui est meilleure que l'approximation linéaire, ne sera que pour

134
00:12:37,550 --> 00:12:42,420
approximer la fonction de f évaluée à x * + h, elle sera approximativement

135
00:12:42,420 --> 00:12:50,259
égal à f évalué à x * plus un sur deux et q (h)

136
00:12:50,259 --> 00:12:54,310
et l'approximation que nous utilisons maintenant avec ce

137
00:12:54,310 --> 00:13:00,949
terme de second ordre est la fonction quadratique. Vous pouvez donc voir que pour ce minimum local,

138
00:13:00,949 --> 00:13:04,950
cette approximation est bien meilleure que l'approximation linéaire. L'approximation linéaire

139
00:13:04,950 --> 00:13:11,089
n'est plate qu’ici. Donc, si je bouge un peu à droite ou à

140
00:13:11,089 --> 00:13:15,519
gauche, l'approximation linéaire n'est pas bonne pour cette

141
00:13:15,519 --> 00:13:22,240
approximation de second ordre. J'ai cette fonction quadratique qui s’ajustera à mon paysage et

142
00:13:22,240 --> 00:13:27,570
je peux donc étudier du point de vue de l'optimisation ce qui se passe dans un bassin

143
00:13:27,570 --> 00:13:35,779
où, dans cette région, pour mon optimiseur, je ne veux pas étudier les

144
00:13:35,779 --> 00:13:40,019
performances de mon optimiseur sur tout ce paysage: c'est une

145
00:13:40,019 --> 00:13:45,050
fonction non linéaire, c'est très difficile à comprendre sur toute la plage des

146
00:13:45,050 --> 00:13:51,060
paramètres. Nous allons donc juste étudier les performances de notre optimiseur sur cette

147
00:13:51,060 --> 00:13:55,630
région qui peut être très bien approximée par une fonction quadratique et

148
00:13:55,630 --> 00:14:00,329
c'est ce que les gens ont fait en optimisation convexe pendant de nombreuses années,

149
00:14:00,329 --> 00:14:06,660
ils étudiaient les optimiseurs sur les fonctions quadratiques.
