1
00:00:14,059 --> 00:00:15,389
Nous l'appellerons

2
00:00:15,389 --> 00:00:17,920
matrice de préconditionnement. Au lieu

3
00:00:17,920 --> 00:00:22,160
d’utiliser simplement la diagonale, nous pouvons utiliser tous

4
00:00:22,160 --> 00:00:23,980
les coefficients de la matrice afin que

5
00:00:23,980 --> 00:00:28,251
je puisse peut-être faire tourner l'ellipse dans

6
00:00:28,251 --> 00:00:29,939
la direction opposée:

7
00:00:29,939 --> 00:00:32,900
je peux redimensionner l'axe et à la fin,

8
00:00:32,900 --> 00:00:36,470
cette matrice aura pour effet

9
00:00:36,470 --> 00:00:39,620
de changer cette fonction, qui est complexe à

10
00:00:39,620 --> 00:00:42,260
optimiser, avec cette fonction. C’est comme si je

11
00:00:42,260 --> 00:00:44,400
faisais une descente de gradient

12
00:00:44,400 --> 00:00:47,110
sur une fonction isotropique en utilisant ce

13
00:00:47,110 --> 00:00:50,680
préconditionnement puis en utilisant cette fonction.

14
00:00:50,680 --> 00:00:53,449
Si je n'utilise pas ce préconditionnement,

15
00:00:53,449 --> 00:00:55,890
donc ce M est seulement là pour faciliter le travail,

16
00:00:55,890 --> 00:00:59,630
mais M a comme dimensions le nombre

17
00:00:59,630 --> 00:01:01,440
de paramètres fois le nombre de

18
00:01:01,440 --> 00:01:04,210
paramètres. Dans l'apprentissage profond, le nombre

19
00:01:04,210 --> 00:01:06,530
de paramètres d'un modèle est environ 1

20
00:01:06,530 --> 00:01:10,200
million. De nombreux algorithmes

21
00:01:10,200 --> 00:01:12,780
de la communauté d'optimisation, en fait,

22
00:01:12,780 --> 00:01:16,049
ils calculent une autre matrice avec

23
00:01:16,049 --> 00:01:19,829
la même dimension: ils rempliront donc un

24
00:01:19,829 --> 00:01:22,420
million par 1 million de valeurs de paramètres

25
00:01:22,420 --> 00:01:24,859
dans cette matrice et ils prendront

26
00:01:24,859 --> 00:01:27,549
l’inverse de cette matrice.

27
00:01:27,549 --> 00:01:30,940
Il est vraiment difficile de stocker une matrice de 1

28
00:01:30,940 --> 00:01:32,090
million par 1 million

29
00:01:32,090 --> 00:01:36,399
en mémoire, mais de plus la plupart des

30
00:01:36,399 --> 00:01:39,840
optimiseurs de la communauté d'optimisation

31
00:01:39,840 --> 00:01:42,640
doivent faire l'inverse, car

32
00:01:42,640 --> 00:01:45,640
généralement je veux diviser ces valeurs par

33
00:01:45,640 --> 00:01:49,189
un facteur d'échelle, et je

34
00:01:49,189 --> 00:01:51,960
dois inverser cela. En apprentissage profond,

35
00:01:51,960 --> 00:01:54,200
nous ne pourrons pas utiliser un très

36
00:01:54,200 --> 00:01:56,770
bon optimiseur qui s'appuiera sur la

37
00:01:56,770 --> 00:01:59,119
matrice de préconditionnement complète à

38
00:01:59,119 --> 00:02:01,479
cause de la taille de cette matrice. De plus,

39
00:02:01,479 --> 00:02:03,899
le calcul de l’inverse, que de nombreux

40
00:02:03,899 --> 00:02:07,499
algorithmes nécessitaient, l'inverse d'une

41
00:02:07,499 --> 00:02:10,459
matrice diagonale est juste l'inverse des

42
00:02:10,459 --> 00:02:12,610
éléments de la diagonale: c'est beaucoup

43
00:02:12,610 --> 00:02:14,790
plus facile à calculer et c'est ce que nous

44
00:02:14,790 --> 00:02:17,069
allons utiliser pour définir un

45
00:02:17,069 --> 00:02:21,060
meilleur optimiseur. Alors maintenant, nous avons notre

46
00:02:21,060 --> 00:02:24,670
algorithme de descente de gradient préconditionné

47
00:02:24,670 --> 00:02:28,439
ici, mais j'ai encore besoin de spécifier quelles

48
00:02:28,439 --> 00:02:30,819
seront les valeurs numériques que je

49
00:02:30,819 --> 00:02:36,780
veux utiliser dans M. M est diagonal, donc

50
00:02:36,780 --> 00:02:40,849
un premier algorithme d'optimisation qui est

51
00:02:40,849 --> 00:02:43,909
beaucoup utilisé dans la communauté est RMSprop.

54
00:02:48,702 --> 00:02:49,702
Le moins un fois 

55
00:02:49,702 --> 00:02:51,400
le gradient me fournira une

56
00:02:51,400 --> 00:02:57,040
direction de descente. Je suis sûr que si je me déplace dans la

57
00:02:57,040 --> 00:02:59,610
direction négative du gradient, la perte

58
00:02:59,610 --> 00:03:02,950
diminuera. Peut-être que je dois me déplacer très,

59
00:03:02,950 --> 00:03:05,340
très lentement, car la fonction est

60
00:03:05,340 --> 00:03:08,670
très non linéaire. Par contre, il existe un Eta

61
00:03:08,670 --> 00:03:12,230
suffisamment petit qui me

62
00:03:12,230 --> 00:03:14,989
garantira que la perte diminuera

63
00:03:14,989 --> 00:03:17,030
si je me déplace dans la direction du

64
00:03:17,030 --> 00:03:19,989
gradient. Maintenant, je multiplie ce

65
00:03:19,989 --> 00:03:22,560
gradient par une matrice et peut-être que je n'ai

66
00:03:22,560 --> 00:03:25,159
plus de direction de descente, peut-être que je

67
00:03:25,159 --> 00:03:29,780
perdrai cette propriété que le

68
00:03:29,780 --> 00:03:32,460
gradient est une direction de descente. Donc, pour être

69
00:03:32,460 --> 00:03:36,680
sûr que je n'inverse pas la

70
00:03:36,680 --> 00:03:39,409
direction de descente, je dois être sûr que M est

71
00:03:39,409 --> 00:03:42,450
définitive positive. Pour être sûr que M

72
00:03:42,450 --> 00:03:44,434
est définitive positive, je peux ne prendre que le

73
00:03:44,434 --> 00:03:45,434
carré des valeurs pour être sûr

74
00:03:45,434 --> 00:03:46,434
qu'elle est positive et sur la

75
00:03:46,434 --> 00:03:47,434
diagonale, je dois juste m'assurer que toutes

76
00:03:47,434 --> 00:03:48,740
mes valeurs sont positives, qu’elles n'ont pas

77
00:03:48,740 --> 00:03:50,900
des signes différents, et je serai sûr de

78
00:03:50,900 --> 00:03:52,609
préserver la propriété que j'ai une

79
00:03:52,609 --> 00:03:56,890
direction de descente. Donc, cette diagonale doit

80
00:03:56,890 --> 00:03:59,120
être positive.

81
00:03:59,120 --> 00:04:01,430
Une façon simple de redimensionner les

82
00:04:01,430 --> 00:04:04,690
paramètres est de diviser par le carré du

83
00:04:04,690 --> 00:04:07,069
gradient: cette équation doit

84
00:04:07,069 --> 00:04:10,249
être interprétée composant par composant.

85
00:04:10,249 --> 00:04:13,599
Je maintiendrai une variable auxiliaire

86
00:04:13,599 --> 00:04:18,310
VT et je mettrai à jour VT avec Beta. Beta

87
00:04:18,310 --> 00:04:24,170
est un hyperparamètre, fois la valeur de V

88
00:04:24,170 --> 00:04:28,530
au temps t moins 1 plus 1 moins bêta.

89
00:04:28,530 --> 00:04:31,860
Je multiplierai cela avec

90
00:04:31,860 --> 00:04:34,590
le carré du gradient,

91
00:04:34,590 --> 00:04:36,770
mais pour la composante relative à un

92
00:04:36,770 --> 00:04:37,770
paramètre.

97
00:04:41,770 --> 00:04:42,770
Ici, pour mettre à jour mes paramètres,

98
00:04:42,770 --> 00:04:43,770
ce serait l'ancienne version de mes

99
00:04:43,770 --> 00:04:47,449
paramètres plus 1 divisé par la racine carrée

100
00:04:47,449 --> 00:04:51,639
de cette valeur qui sera positive,

101
00:04:51,639 --> 00:04:54,509
puis je multiplie cela par le

102
00:04:54,509 --> 00:04:57,410
gradient et cela réduira l'échelle et 

103
00:04:57,410 --> 00:05:00,530
en quelque sorte l'axe, alors le

104
00:05:00,530 --> 00:05:03,890
nombre de transition soit meilleur en utilisant

105
00:05:03,890 --> 00:05:06,350
cette technique simple. Donc, notre RMSprop

106
00:05:06,350 --> 00:05:09,000
fonctionne généralement mieux que juste

107
00:05:09,000 --> 00:05:11,790
la descente de gradient classique.

108
00:05:11,790 --> 00:05:15,060
En termes d'optimisation, c’est plus rapide pour

109
00:05:15,060 --> 00:05:17,639
minimiser la fonction de perte sur

110
00:05:17,639 --> 00:05:18,289
l’ensemble d'entraînement.