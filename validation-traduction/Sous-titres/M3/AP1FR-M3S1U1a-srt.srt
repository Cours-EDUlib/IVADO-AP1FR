1
00:00:13,070 --> 00:00:17,820
Je vais commencer par un petit avertissement: vous avez les diapositives disponibles en

2
00:00:17,820 --> 00:00:22,130
ligne au format PDF. J'ai préparé beaucoup d'animations pour ces diapositives.

3
00:00:22,130 --> 00:00:28,050
Malheureusement les gifs ne peuvent pas être rendus en PDF et toutes les diapositives où vous pourriez

4
00:00:28,050 --> 00:00:31,949
voir une figure vierge, il y a un lien pour accéder à l'animation appropriée.

5
00:00:31,949 --> 00:00:35,519
Toutes les animations que vous verrez aujourd'hui, vous pourrez également les voir à la maison.

6
00:00:35,519 --> 00:00:40,260
Le code que j'utilise pour générer ces figures est disponible aussi si vous êtes intéressé.

7
00:00:40,260 --> 00:00:48,019
Commençons par le contenu d'aujourd'hui.

8
00:00:48,019 --> 00:00:52,280
Nous allons d'abord commencer par une introduction et une intuition aux réseaux de neurones convolutifs.

9
00:00:52,280 --> 00:00:56,339
En passant, le terme « réseau de neurones convolutif » est un peu long,

10
00:00:56,339 --> 00:00:59,670
il y a beaucoup de syllabes, donc je vais utiliser les termes

11
00:00:59,670 --> 00:01:03,720
CNN ou RNC de manière interchangeable. Alors, si je dis CNN ou RNC, je parle vraiment de

12
00:01:03,720 --> 00:01:07,950
réseaux de neurones convolutifs. 
Nous allons parler des opérations mathématiques

13
00:01:07,950 --> 00:01:11,670
dans les CNN: comprendre ce qu'est une convolution et

14
00:01:11,670 --> 00:01:15,000
comment elle s'intègre dans le contexte d'un réseau de neurones convolutif.

15
00:01:15,000 --> 00:01:18,780
Nous allons ensuite parler des motivations pour utiliser les CNN: pourquoi sont-ils si utiles?

16
00:01:18,780 --> 00:01:22,350
Où se situent-ils dans le pipeline d'apprentissage profond?

17
00:01:22,350 --> 00:01:26,820
Nous verrons comment un CNN peut être considéré comme un bloc de construction. 

18
00:01:26,820 --> 00:01:31,910
L'image que nous avons ici, espérons-le, à la fin de cet exposé, commencera à avoir du sens.

19
00:01:31,910 --> 00:01:37,620
Commençons par une introduction aux CNN.

20
00:01:37,620 --> 00:01:42,360
Les réseaux de neurones convolutifs sont un type très spécifique de réseau de neurones.

21
00:01:42,360 --> 00:01:47,400
Ils ont été vraiment conçus pour traiter des données avec une structure en forme de grille. Qu'est-ce qu’’une

22
00:01:47,400 --> 00:01:51,720
structure en forme de grille signifie? One entend que les données ont un aspect temporel, par

23
00:01:51,720 --> 00:01:58,200
exemple, comme le signal que nous voyons ici qui est un signal ECG. Vous pourriez donc avoir

24
00:01:58,200 --> 00:02:02,250
une sorte d'appareil qui mesure à une certaine fréquence à un intervalle très spécifique.

25
00:02:02,250 --> 00:02:05,970
Il prendra des mesures et vous pouvez lire ces mesures et les

26
00:02:05,970 --> 00:02:12,450
Interpréter. Nous pouvons également avoir une structure de grille spatiale, par exemple une

27
00:02:12,450 --> 00:02:17,580
image où chaque pixel d'une image représentera une sorte d'étendue spatiale.

28
00:02:17,580 --> 00:02:21,030
Il se pourrait donc que la différence entre deux pixels soit

29
00:02:21,030 --> 00:02:24,390
effectivement traduite à une certaine distance dans le monde physique, mais il est très

30
00:02:24,390 --> 00:02:27,689
important que nous essayions d'exploiter le fait que la

31
00:02:27,689 --> 00:02:31,200
distance entre deux points sur les signaux qui mesuraient est constante.

32
00:02:31,200 --> 00:02:38,790
Les CNN étaient en fait quelques-uns des premiers modèles d'apprentissage profond à

33
00:02:38,790 --> 00:02:44,040
généraliser à des tâches utiles. L'apprentissage profond a explosé au cours des cinq

34
00:02:44,040 --> 00:02:48,299
à dix dernières années, mais les CNN ont été utilisés pendant longtemps. En fait, à la fin des années 1990,

35
00:02:48,299 --> 00:02:53,370
environ 10% de tous les chèques émis aux États-Unis

36
00:02:53,370 --> 00:02:57,750
étaient automatiquement traités par un réseau de neurones convolutif. Bien que

37
00:02:57,750 --> 00:03:01,680
l'apprentissage profond soit encore nouveau dans l'industrie et qu'il

38
00:03:01,680 --> 00:03:06,209
devienne de plus en plus populaire, l'apprentissage profond a en fait été utilisé en production depuis

39
00:03:06,209 --> 00:03:10,560
très longtemps. 
Le problème est qu'en dehors de

40
00:03:10,560 --> 00:03:14,819
la reconnaissance optique de caractères sur une tâche comme les chèques, il était très difficile d’entraîner

41
00:03:14,819 --> 00:03:21,629
des modèles d'apprentissage profond sur d'autres tâches utiles jusqu'en 2012, plus ou moins: 

42
00:03:21,629 --> 00:03:26,040
c'était un point énorme pour l'apprentissage profond lorsque nous avions ce modèle appelé AlexNet.

43
00:03:26,040 --> 00:03:30,239
Nous allons parler d'AlexNet beaucoup plus tard dans la journée, nous explorerons

44
00:03:30,239 --> 00:03:34,109
En profondeur ce qu'était AlexNet et ce qu'était ImageNet, une

45
00:03:34,109 --> 00:03:38,400
compétition qu'AlexNet a gagnée, et pourquoi c'était si important. C'est vraiment en 2012 que

46
00:03:38,400 --> 00:03:42,150
les gens ont commencé à prêter plus d’attention aux réseaux de neurones convolutifs:

47
00:03:42,150 --> 00:03:46,169
c'est à ce moment-là qu'ils ont commencé à réaliser le potentiel que les CNN ont sur des

48
00:03:46,169 --> 00:03:50,459
images et sur des tâches utiles, et que le matériel était maintenant à jour et aligné

49
00:03:50,459 --> 00:03:54,449
avec l’entraînement de ces modèles.
Nous allons passer en revue tous

50
00:03:54,449 --> 00:04:00,690
les détails de ce réseau plus tard dans la journée. 
Alors pourquoi utilisons-nous

51
00:04:00,690 --> 00:04:05,430
des réseaux de neurones convolutifs? L’intuition derrière est vraiment de faire en sorte que le réseau se concentre sur

52
00:04:05,430 --> 00:04:10,829
les caractéristiques locales - vous pouvez penser à cela comme des zones - dans une image.

53
00:04:10,829 --> 00:04:14,790
Une esquisse mentale à laquelle vous pourriez penser est de dire que vous avez un algorithme qui

54
00:04:14,790 --> 00:04:18,030
essaie de détecter un visage. Eh bien, vous pourriez essayer de le considérer comme quelque chose qui

55
00:04:18,030 --> 00:04:23,520
cherche des caractéristiques ressemblant à des nez, des yeux et des oreilles en combinaison,

56
00:04:23,520 --> 00:04:27,180
de telle sorte qu'en combinant ces caractéristiques dans un ordre spécifique,

57
00:04:27,180 --> 00:04:31,710
que vous puissiez ensuite reconstruire un visage.
Cela ressemble à

58
00:04:31,710 --> 00:04:35,190
ce que Gaétan présentait avec l'exemple de chaise hier: nous ne

59
00:04:35,190 --> 00:04:38,639
savons pas nécessairement comment abstraire une chaise, mais nous pourrions vouloir chercher certaines

60
00:04:38,639 --> 00:04:42,509
caractéristiques locales qui pourraient représenter la chaise. Cela est vrai pour un visage, cela est

61
00:04:42,509 --> 00:04:48,719
vrai pour les voitures. Prenons un exemple ici, disons que nous recherchons une voiture.

62
00:04:48,719 --> 00:04:53,699
Notre réseau pourrait d'abord se concentrer sur un ensemble de caractéristiques qui ressemblent à

63
00:04:53,699 --> 00:04:57,960
des roues, puis cela se transmet èa travers notre réseau, puis nous

64
00:04:57,960 --> 00:05:01,710
pouvons éventuellement prendre une décision basée sur les probabilités pour savoir si c'est

65
00:05:01,710 --> 00:05:05,279
vraiment une voiture que nous regardons. Le CNN peut également prendre les informations et dire:

66
00:05:05,279 --> 00:05:09,029
« Eh bien, il y a quelque chose qui ressemble à un miroir, quelque chose qui ressemble à une

67
00:05:09,029 --> 00:05:12,960
caractéristique d’une voiture. » Combinant toutes ces différentes caractéristiques ensemble, nous pouvons ensuite

68
00:05:12,960 --> 00:05:17,009
faire notre prédiction avec plus de précision. Encore une fois, tout l'intérêt des CNN était

69
00:05:17,009 --> 00:05:24,050
de forcer le réseau à se concentrer sur les zones  locales et les combiner de manière intelligente.