1
00:00:13,179 --> 00:00:20,060
Maintenant que nous avons examiné la plupart des opérations courantes dans les RNC, regardons les

2
00:00:20,060 --> 00:00:25,700
différents éléments constitutifs. Les RNC utiliseront généralement

3
00:00:25,700 --> 00:00:29,599
ces opérations de nombreuses façons, vous pouvez penser à cela comme des pièces LEGO

4
00:00:29,599 --> 00:00:34,129
et ce concept reviendra encore et encore. Vous verrez dans les architectures

5
00:00:34,129 --> 00:00:37,220
que nous étudierons plus tard aujourd'hui qu'il y a beaucoup de

6
00:00:37,220 --> 00:00:43,010
pièces qui sont très similaires d'une architecture à une autre.

7
00:00:43,010 --> 00:00:48,920
Commençons par revenir à la raison pour laquelle nous utilisons des RNC: notre RNC peut

8
00:00:48,920 --> 00:00:53,360
être utilisé pour mapper notre entrée, donc par exemple une image. Disons que nous avons l'image

9
00:00:53,360 --> 00:00:57,469
d'un chien et nous voulons la mapper à une certaine distribution de probabilité. Si

10
00:00:57,469 --> 00:01:01,039
nous savons qu'il y a une certaine quantité de catégories dans lesquelles nous pourrions classer ce

11
00:01:01,039 --> 00:01:05,899
chien, nous voulons être sûrs que notre réseau a compris que cette image

12
00:01:05,899 --> 00:01:13,759
représente un chien. Idéalement, nous avons notre image en entrée, nous avons une

13
00:01:13,759 --> 00:01:20,960
sorte de boîte noire RNC et elle retourne une distribution de probabilités.

14
00:01:20,960 --> 00:01:26,479
Si nous considérons le cas d'une image en couleur, nous avons donc 256 pixels par 256 pixels,

15
00:01:26,479 --> 00:01:31,009
multiplions cela par trois canaux, ce qui représente un total d'environ 200 000 entrées.

16
00:01:31,009 --> 00:01:35,450
Nous avons environ 200 000 entrées et nous voulons

17
00:01:35,450 --> 00:01:39,500
réduire à trois entrées. Comment allons-nous ces 200 000 entrées à

18
00:01:39,500 --> 00:01:43,610
environ trois entrées? Il s'avère qu'il existe de nombreux

19
00:01:43,610 --> 00:01:48,920
blocs fonctionnels utiles que nous pouvons utiliser pour le faire efficacement. Voici à quoi 

20
00:01:48,920 --> 00:01:51,860
ressemble le plus commun ensemble de blocs de construction et nous allons lexplorer

21
00:01:51,860 --> 00:01:57,920
en détail. Alors regardez cette image, il y a beaucoup d'opérations en cours, mais

22
00:01:57,920 --> 00:02:02,450
encore une fois, la plupart des architectures que vous utiliserez ressemblent généralement à ceci:

23
00:02:02,450 --> 00:02:06,860
une sorte de convolution, une sorte d'activation non linéaire et une sorte de

24
00:02:06,860 --> 00:02:11,480
mise en commun. Décomposons-le en détails: le premier est d'appliquer la convolution.

25
00:02:11,480 --> 00:02:17,919
Nous prenons notre noyau, nous prenons notre image et nous produisons une sorte de carte topologique.

26
00:02:17,919 --> 00:02:22,790
Nous prenons ensuite cette carte topologique et nous appliquons notre activation non linéaire.

27
00:02:22,790 --> 00:02:26,770
Cest ce qui donne à notre réseau de neurones convolutif sa puissance, c'est qu'il est capable de

28
00:02:26,770 --> 00:02:31,450
faire ces opérations non linéaires sur nos cartes topologiques. Elles sont généralement

29
00:02:31,450 --> 00:02:36,060
Appliquées élément par élément, vous avez donc vu le ReLU hier et il y a différentes

30
00:02:36,060 --> 00:02:40,060
fonctions d'activation. La plupart des frameworks d'apprentissage profond implémentent la plupart des

31
00:02:40,060 --> 00:02:43,510
activations populaires qui sont utilisés, mais généralement dans le cas des

32
00:02:43,510 --> 00:02:50,350
réseaux de neurones convolutifs , vous verrez ReLU ou tanh utilisé. Nous allons ensuite faire

33
00:02:50,350 --> 00:02:55,170
lopération de mise en commun , donc nous avons vu l'opération de mise en commun maximale, vous pouvez faire la mise en commun moyenne, mise en commun

34
00:02:55,170 --> 00:02:59,170
minimale, c'est une sorte de décision architecturale. La mise

35
00:02:59,170 --> 00:03:04,180
en commun vous permettra de réduire la taille de votre réseau et c'est vraiment je dirais

36
00:03:04,180 --> 00:03:07,720
le bloc de construction le plus commun que vous verrez dans les

37
00:03:07,720 --> 00:03:13,540
réseaux de neurones convolutifs. Ce qui devient votre responsabilité pendant que vous construisez

38
00:03:13,540 --> 00:03:16,660
ces architectures est de décider comment vous allez les assembler.

39
00:03:16,660 --> 00:03:21,280
Vous pouvez avoir pleins de convolutions et d'activations enchaînées avant de

40
00:03:21,280 --> 00:03:25,600
faire votre mise en commun. Ici, par exemple, vous pouvez décider de faire une convolution,

41
00:03:25,600 --> 00:03:30,040
une activation non linéaire, une autre convolution, une autre activation non linéaire, autant de

42
00:03:30,040 --> 00:03:33,850
fois que vous le souhaitez avant de faire la mise en commun. Vous pouvez également décidez de la

43
00:03:33,850 --> 00:03:37,570
Profondeur: vous pouvez décider combien de noyaux vous voulez à chaque étape et c'est là que

44
00:03:37,570 --> 00:03:43,060
le type d'architecture que vous avez va changer. Non seulement

45
00:03:43,060 --> 00:03:46,360
cela, mais nous pouvons avoir un grand nombre de ces blocs enchaînés: vous pouvez faire

46
00:03:46,360 --> 00:03:50,470
votre ensembles de convolutions et de mises en commun, vous obtenez une sorte de volume de sortie

47
00:03:50,470 --> 00:03:55,570
et cela devient votre entrée dans votre prochain ensemble de blocs.

48
00:03:55,570 --> 00:04:00,160
Il suffit de les enchaîner de plus en plus et ensuite vous pouvez construire

49
00:04:00,160 --> 00:04:07,540
des réseaux de neurones de plus en plus profonds, ce qui nous permet de passer d'une

50
00:04:07,540 --> 00:04:13,510
dimension arbitraire à une autre dimension arbitraire. Si nous reprenons notre image, nous faisons une série de

51
00:04:13,510 --> 00:04:18,310
convolutions et d'activations non linéaires que nous mettons en commun et nous faisons ensuite à nouveau une

52
00:04:18,310 --> 00:04:22,419
série d'activations et de mise en communs non linéaires autant de fois que vous le souhaitez.

53
00:04:22,419 --> 00:04:27,250
Finalement, vous obtenez ce volume en sortie, donc nous appelons généralement cela une

54
00:04:27,250 --> 00:04:31,750
carte topologique et dans cette carte topologique, si votre réseau a appris correctement, vous avez une

55
00:04:31,750 --> 00:04:35,070
représentation de haut niveau de votre image qui contient beaucoup d'informations riches,

56
00:04:35,070 --> 00:04:37,750
qui est vraiment difficile pour nous d'interpréter, mais que

57
00:04:37,750 --> 00:04:43,150
votre réseau pourrait alors prendre des décisions en fonctions de. Maintenant, si nous voulons obtenir notre

58
00:04:43,150 --> 00:04:47,140
distribution des probabilités, nous devons passer d'un volume à un

59
00:04:47,140 --> 00:04:52,900
vecteur unidimensionnel. Une manière de le faire est d'aplatir votre volume, puis d'utiliser un

60
00:04:52,900 --> 00:04:58,600
Perceptron multi-couches à la fin. C'est un scénario très typique dans certaines architectures que

61
00:04:58,600 --> 00:05:02,440
nous verrons aujourd'hui, nous faisons beaucoup de ces blocs de construction que je viens de vous montrer,

62
00:05:02,440 --> 00:05:07,570
nous obtenons une sorte de volume comme sortie , nous aplatissons notre carte topologique et nous

63
00:05:07,570 --> 00:05:12,250
utilisons un MLP. Nous utilisons quelques couches entièrement connectées et nous pouvons décider

64
00:05:12,250 --> 00:05:16,980
ce quest la probabilité dans ce cas que nous ayons un chien, un ours, un chat, etc.

65
00:05:16,980 --> 00:05:21,010
Ces couches entièrement connectées sont généralement suivies d'un softmax simple

66
00:05:21,010 --> 00:05:25,120
afin que nous puissions obtenir notre distribution de probabilité. Vous avez vu le

67
00:05:25,120 --> 00:05:29,110
softmax hier dans le tutoriel et cela nous permet simplement de

68
00:05:29,110 --> 00:05:44,429
garantir que ces résultats sadditionnent à un comme une distribution de probabilité devrait.
