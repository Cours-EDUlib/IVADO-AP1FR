1
00:00:13,209 --> 00:00:18,500
Nous avons mentionné ImageNet plus tôt et juste pour récapituler,

2
00:00:18,500 --> 00:00:23,090
ImageNet joue un rôle important dans la communauté d'apprentissage profond. Il a réellement façonné

3
00:00:23,090 --> 00:00:27,920
où nous en sommes aujourd'hui en termes de réseaux neuronaux convolutifs.

4
00:00:27,920 --> 00:00:31,640
À mon opinion, c'est l'une des raisons pour lesquelles l'apprentissage profond a explosé dans les

5
00:00:31,640 --> 00:00:36,110
dernières années. Pour vous rappeler, elle consiste

6
00:00:36,110 --> 00:00:39,230
en des millions d'images qui ont été étiquetées manuellement dans des milliers de

7
00:00:39,230 --> 00:00:45,379
catégories. ILSVRC est la reconnaissance visuelle à grande échelle d'ImageNet.

8
00:00:45,379 --> 00:00:49,129
C'est un défi qui a été lancé quelques années après qu'ImageNet ait été lancé

9
00:00:49,129 --> 00:00:53,750
et l'idée était de réduire le nombre de catégories à un millier et

10
00:00:53,750 --> 00:01:00,079
il y avait environ 1,2 million d'images à entraîner sur ces catégories.

11
00:01:00,079 --> 00:01:04,280
Le concours était relativement simple. Il s'agissait de savoir qui pouvait entraîner le modèle qui classifie

12
00:01:04,280 --> 00:01:11,479
le plus de catégories avec précision. Voici quelques mots de Yann Lecun et

13
00:01:11,479 --> 00:01:14,300
ça provient en fait d’un forum sur Reddit « demandez-moi n’importe quoi ».

14
00:01:14,300 --> 00:01:19,369
Il dit : « Ne vous laissez pas berner par des gens qui prétendent avoir trouvé une solution

15
00:01:19,369 --> 00:01:23,689
à l’AGI (intelligence générale artificielle). Demandez-leur quel est leur taux d'erreur

16
00:01:23,689 --> 00:01:28,640
sur MNIST ou ImageNet. ». Ça démontre l'importance de l’analyse comparative

17
00:01:28,640 --> 00:01:32,600
sur des ensembles de données connus. Si vous affirmez que votre modèle est meilleur, c'est une chose,

18
00:01:32,600 --> 00:01:36,890
mais à quel point est-il vraiment meilleur que d'autres modèles pour ces tâches similaires?

19
00:01:36,890 --> 00:01:45,229
Nous devons comparer les modèles de manière équitable. Alors ImageNet et le concours ILSVRC

20
00:01:45,229 --> 00:01:50,840
ont ouvert la voie à la recherche sur la vision par ordinateur et, précisément,

21
00:01:50,840 --> 00:01:54,409
à l’apprentissage profond. La raison est qu’ils ont permis aux chercheurs

22
00:01:54,409 --> 00:01:58,430
de comparer de manière objective les différents algorithmes qu'ils proposaient.

23
00:01:58,430 --> 00:02:04,579
ILSVRC était un concours avec mille catégories et

24
00:02:04,579 --> 00:02:12,799
1,2 millions d'images. Donc en 2011, l'algorithme le plus utilisé était présenté ici,

25
00:02:12,799 --> 00:02:19,850
l’algorithme XRCE, et celui-ci était principalement basé sur l'extraction de caractéristiques.

26
00:02:19,850 --> 00:02:24,350
Pour que vous compreniez bien, nous avons ici une chronologie linéaire au fil des années

27
00:02:24,350 --> 00:02:28,130
des différents modèles qui ont été proposés. Ici, nous avons sur l'axe des y

28
00:02:28,130 --> 00:02:33,020
les cinq erreurs de classification les plus importantes. Il s'agit donc d'une mesure qui a été utilisée pour

29
00:02:33,020 --> 00:02:37,610
évaluer les modèles les uns par rapport aux autres et ce que vous faites est dans presque tous les

30
00:02:37,610 --> 00:02:42,020
ces modèles, ils font la sortie de la distribution des probabilités de l'ensemble des 1 000 catégories

31
00:02:42,020 --> 00:02:47,480
et vous prenez les cinq catégories qui se sont classées le plus haut et

32
00:02:47,480 --> 00:02:50,810
si l'objet que vous essayiez de classer se trouve dans l'une de ces cinq catégories principales,

33
00:02:50,810 --> 00:02:55,480
c’est considéré comme une estimation correcte. Nous pouvons alors évaluer

34
00:02:55,480 --> 00:02:58,550
l’erreur de classification totale, ou l’erreur de classification par critère top-5, pour tous ces

35
00:02:58,550 --> 00:03:02,750
différents modèles et celui qui obtient le plus faible score est le plus performant. Nous cherchons donc à

36
00:03:02,750 --> 00:03:08,720
minimiser l'erreur de classification. En 2011, il y a eu une soumission

37
00:03:08,720 --> 00:03:13,910
qui a obtenu un score de 26. Mais l’événement révolutionnaire dans

38
00:03:13,910 --> 00:03:19,760
la communauté d'apprentissage profond s'est produit en 2012, qui est considéré comme un tournant important

39
00:03:19,760 --> 00:03:25,250
pour la communauté d’apprentissage profond. Nous avons cette publication qui porte sur la classification ImageNet

40
00:03:25,250 --> 00:03:29,450
avec des réseaux neuronaux convolutifs profonds. Il s'agit d'une publication de

41
00:03:29,450 --> 00:03:33,500
Alex Krizhevsky, Ilya Sutskever et Geoffrey Hinton. Ils ont tous eu

42
00:03:33,500 --> 00:03:37,850
d'illustres carrières. Par la suite, Geoffrey Hinton a été lauréat du prix Turing.

43
00:03:37,850 --> 00:03:43,370
Ilya travaille actuellement pour open AI et Alex a fait toutes sortes de choses différentes

44
00:03:43,370 --> 00:03:46,940
dans beaucoup d'entreprises. Donc c’est un article très important

45
00:03:46,940 --> 00:03:50,780
pour le domaine de l'apprentissage profond. Alors examinons ce qui s'est passé et pourquoi nous

46
00:03:50,780 --> 00:03:56,690
avons pu obtenir le bond soudain d’une différence de près de dix points

47
00:03:56,690 --> 00:04:00,700
pour cette erreur de classification.