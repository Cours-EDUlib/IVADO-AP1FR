1
00:00:13,179 --> 00:00:19,939
L'architecture de AlexNet reste très proche du style de LeNet.

2
00:00:19,939 --> 00:00:25,220
Nous avons vu l'architecture originale de LeNet et elle suit un patron de conception très similaire.

3
00:00:25,220 --> 00:00:28,250
J'ai mentionné les composantes de base, et nous les voyons en ici encore.

4
00:00:28,250 --> 00:00:34,940
Nous avons donc cinq couches convolutives et chacune d'entre elles

5
00:00:34,940 --> 00:00:40,400
est suivie d'une fonction d'activation non linéaire ReLU. Elles ne sont pas toutes suivies

6
00:00:40,400 --> 00:00:44,329
d’une mise en commun par maximum. Par exemple, vous voyez que nous avons trois fois

7
00:00:44,329 --> 00:00:49,520
une couche convolutive et à l'extrémité de notre réseau, similaire à l'architecture de LeNet,

8
00:00:49,520 --> 00:00:54,489
nous avons des couches connectées qui sont suivies d'une couche softmax pour nous donner notre distribution de probabilité.

9
00:00:54,489 --> 00:00:58,670
Donc si nous examinons l'architecture, nous voyons qu'elle

10
00:00:58,670 --> 00:01:05,539
commence avec une convolution 11 par 11. La taille de notre carré de noyau était de 11 par 11

11
00:01:05,539 --> 00:01:12,770
et ils utilisent une profondeur de 96 et un pas de 4. Ici, ils utilisent

12
00:01:12,770 --> 00:01:16,759
une mise en commun par maximum de 3 par 3. Je vous rappelle que vous pouvez aller lire le document original pour l'implémentation.

13
00:01:16,759 --> 00:01:20,960
Ce qui est vraiment intéressant, c'est l’échelle du nombre de paramètres.

14
00:01:20,960 --> 00:01:24,950
Donc si vous vous rappelez, pour LeNet, nous avions affaire à des images

15
00:01:24,950 --> 00:01:31,850
qui étaient en échelle de gris, donc de 784 pixels, 28 par 28. Maintenant nous avons affaire à ces

16
00:01:31,850 --> 00:01:38,420
images RVB. Rappellons-nous qu'ImageNet avait des images de 256 par 256 et pour des raisons pratiques,

17
00:01:38,420 --> 00:01:43,640
ils ont décidé de recadrer ces images au format de 224 par 224, mais avec des canaux RVB,

18
00:01:43,640 --> 00:01:47,329
et donc beaucoup plus d'informations qui arrivent dans notre réseau et beaucoup plus de

19
00:01:47,329 --> 00:01:51,140
paramètres. En fait, si vous calculez le nombre de paramètres utilisés

20
00:01:51,140 --> 00:01:55,100
pour LeNet en comparaison avec AlexNet, vous voyez que

21
00:01:55,100 --> 00:02:00,500
la différence d'ampleur dans le nombre de paramètres utilisés est multipliée par mille.

22
00:02:00,500 --> 00:02:06,649
Ce qui est aussi intéressant, c'est qu'AlexNet a utilisé l’extinction de neurones dans les couches entièrement connectées.

23
00:02:06,649 --> 00:02:12,290
Qu’est-ce que l'extinction de neurones? C'est une technique relativement nouvelle, donc l'extinction de neurones est

24
00:02:12,290 --> 00:02:17,180
vraiment intéressante dans la mesure où ce que nous faisons dans les couches entièrement connectées est que

25
00:02:17,180 --> 00:02:21,949
pendant l’entraînement, nous désactivons au hasard certaines connexions avec

26
00:02:21,949 --> 00:02:25,260
utilisant une probabilité de 0,5. Nous avons ces couches entièrement connectées

27
00:02:25,260 --> 00:02:29,670
et à chaque fois que nous allons passer par notre réseau,

28
00:02:29,670 --> 00:02:34,170
nous tirons à pile ou face et nous décidons au hasard que certaines

29
00:02:34,170 --> 00:02:39,030
de ces connexions vont être désactivées. On peut le considérer comme une

30
00:02:39,030 --> 00:02:43,950
forme de régularisation, ce qui oblige notre réseau à

31
00:02:43,950 --> 00:02:48,540
ne pas être trop dépendant de toutes les informations qui sont là. J'aime vraiment la façon dont

32
00:02:48,540 --> 00:02:52,620
ils l'ont mentionné dans l'article original. Ils le perçoivent un peu comme un moyen

33
00:02:52,620 --> 00:02:56,670
d'entraîner de nombreux modèles en même temps. En fait, l’entraînement du modèle AlexNet original

34
00:02:56,670 --> 00:03:00,840
a pris quelques semaines, vous pouvez donc imaginer que si vous vouliez entraîner

35
00:03:00,840 --> 00:03:04,260
de nombreux modèles différents, cela prendrait beaucoup de temps. Et comme ils ne disposaient pas

36
00:03:04,260 --> 00:03:08,819
d’autant de temps ni de ressources, ils ont proposé

37
00:03:08,819 --> 00:03:11,760
cette technique d'extinction de neurones et ils ont constaté qu'elle faisait fortement converger leurs modèles

38
00:03:11,760 --> 00:03:20,069
et a aussi réduit l'impact du surapprentissage.

39
00:03:20,069 --> 00:03:24,930
Alexnet est un modèle très important dans les réseaux neuronaux convolutifs et

40
00:03:24,930 --> 00:03:30,180
vous pouvez voir, rien qu'à partir des citations de Google Scholar, que l’article a été cité plus de 43 000 fois.

41
00:03:30,180 --> 00:03:36,000
C'est un nombre très élevé pour un article,

42
00:03:36,000 --> 00:03:39,540
et il est cité dans presque tous les articles que vous pouvez lire sur les réseaux neuronaux convolutifs.

43
00:03:39,540 --> 00:03:42,810
C'était vraiment l'un des piliers du domaine et les gens ont commencé à y prêter attention

44
00:03:42,810 --> 00:03:46,980
une fois qu'ils ont vu que les réseaux neuronaux convolutifs pouvaient converger

45
00:03:46,980 --> 00:03:52,260
pour donner de meilleures performances que les caractéristiques étiquetées à la main d'ImageNet.

46
00:03:52,260 --> 00:03:58,739
J’aime aussi beaucoup cette citation d'Alex Krizhevsky et al.

47
00:03:58,739 --> 00:04:02,880
qui dit que « toutes nos expériences suggèrent que nos résultats peuvent être améliorés simplement

48
00:04:02,880 --> 00:04:07,319
en attendant que des GPU plus rapides et des ensembles de données plus importants soient disponibles. »

49
00:04:07,319 --> 00:04:11,849
Alors ils avaient déjà cette idée que les GPU

50
00:04:11,849 --> 00:04:15,329
sur lesquels ils s'entraînaient étaient tout simplement trop lents. En fait, ils devaient faire

51
00:04:15,329 --> 00:04:20,760
toutes sortes de manipulations pour faire converger leurs modèles. Ils ont été l'un des premiers à

52
00:04:20,760 --> 00:04:24,960
excécuter leurs modèles d'apprentissage profond sur les GPU. Ils ont eu cette intuition

53
00:04:24,960 --> 00:04:29,250
que le GPU était optimisé pour faire des calculs matriciels,

54
00:04:29,250 --> 00:04:32,820
mais à l’époque, TensorFlow et de PyTorch n'existaient pas,

55
00:04:32,820 --> 00:04:36,960
et vous ne pouviez pas simplement dire « device equals GPU » et passer votre modèle au GPU.

56
00:04:36,960 --> 00:04:41,070
Ils ont en fait implémenté toutes les opérations du GPU dans CUDA juste pour cet article

57
00:04:41,070 --> 00:04:45,270
et ils ont aussi publié le code. Il y avait aussi des détails techniques.

58
00:04:45,270 --> 00:04:49,680
Le modèle original d'AlexNet ne tenait même pas sur un seul GPU,

59
00:04:49,680 --> 00:04:53,639
donc ils étaient déjà en train de trouver des trucs pour entraîner leur modèle sur deux

60
00:04:53,639 --> 00:04:57,449
GPU séparés simultanément. Juste pour vous rappeler, tous ces détails sont dans l’article original.

61
00:04:57,449 --> 00:05:01,199
C'est un travail très impressionnant, puisque vous pouvez imaginer

62
00:05:01,199 --> 00:05:04,320
le temps qui a dû être consacré à déterminer s'ils étaient sur la bonne voie

63
00:05:04,320 --> 00:05:11,509
sans même savoir si le projet serait fructueux ou non. En 2012,

64
00:05:11,509 --> 00:05:15,900
les gens commençaient vraiment à s’y intéresser et ils se demandaient

65
00:05:15,900 --> 00:05:19,590
ce qu’était l'apprentissage profond et pourquoi ça fonctionnait aussi bien.

66
00:05:19,590 --> 00:05:23,910
Il y avait une différence majeure dans les performances, si bien que tout d'un coup, on assistait presque à une course aux armements

67
00:05:23,910 --> 00:05:29,520
pour trouver les meilleurs modèles d'apprentissage profond. C’était un défi,

68
00:05:29,520 --> 00:05:33,150
surtout du point de vue technique, comme le matériel comporte des limites,

69
00:05:33,150 --> 00:05:36,720
ça prend beaucoup de temps pour former ces modèles. Je pense que c'était l'ordre de

70
00:05:36,720 --> 00:05:41,760
quelques semaines pour entraîner AlexNet et d'un point de vue algorithmique, vous avez

71
00:05:41,760 --> 00:05:44,880
toutes sortes de problèmes qui commencent à surgir dès que vous commencez à entraîner votre réseau.

72
00:05:44,880 --> 00:05:48,599
Vous avez des problèmes de surapprentissage, vous avez des problèmes de sous-apprentissage,

73
00:05:48,599 --> 00:05:52,500
vous avez des problèmes liés au fait que vous avez beaucoup d'hyperparamètres,

74
00:05:52,500 --> 00:05:55,380
donc toute la structure de votre réseau. Tout d'un coup, nous avons ce réseau qui fonctionne

75
00:05:55,380 --> 00:05:58,979
mais rien ne peut vraiment nous indiquer que c'est le meilleur réseau qui existe.

76
00:05:58,979 --> 00:06:03,479
Il pourrait y avoir beaucoup d'autres réseaux différents. Comme je l'ai mentionné,

77
00:06:03,479 --> 00:06:06,389
c’était un tournant important pour l'apprentissage profond et nous allons maintenant

78
00:06:06,389 --> 00:06:09,949
examiner les différents modèles qui ont suivi.