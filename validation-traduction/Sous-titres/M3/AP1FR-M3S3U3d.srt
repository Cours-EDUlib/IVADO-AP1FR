1
00:00:01,070 --> 00:00:12,339
[Musique]

2
00:00:13,179 --> 00:00:19,430
C’était en 2014  et nous sommes à

3
00:00:17,270 --> 00:00:21,289
environ 6,7 d’erreur de classification par critère top-5

4
00:00:19,430 --> 00:00:22,869
et c'est là encore que

5
00:00:21,289 --> 00:00:25,820
quelque chose de vraiment intéressant se produit :

6
00:00:22,869 --> 00:00:27,949
vous voyez que nous avons ici

7
00:00:25,820 --> 00:00:29,750
l’erreur humaine de classification par critère top-5,

8
00:00:27,949 --> 00:00:32,480
obtenue de la production participative qui faisait appel à de nombreuses personnes et

9
00:00:29,750 --> 00:00:36,050
qui les mobilisait à réaliser le défi ImageNet.

10
00:00:32,480 --> 00:00:38,300
C’était le score que

11
00:00:36,050 --> 00:00:41,390
la plupart des personnes obtenaient, qui était d'environ 5,0.

12
00:00:38,300 --> 00:00:43,940
En 2015, nous aurons enfin le modèle ResNet

13
00:00:41,390 --> 00:00:47,149
qui bat l'exactitude humaine sur

14
00:00:43,940 --> 00:00:48,980
ImageNet. Donc maintenant que ce modèle est sorti,

15
00:00:47,149 --> 00:00:51,379
qui est appelée apprentissage profond résiduel

16
00:00:48,980 --> 00:00:53,570
pour la reconnaissance d'images,

17
00:00:51,379 --> 00:00:55,609
et c'est un travail novateur effectué par

18
00:00:53,570 --> 00:00:57,649
Microsoft Research. Vous verrez que

19
00:00:55,609 --> 00:01:02,269
leurs contributions se sont réellement propagé

20
00:00:57,649 --> 00:01:05,119
dans toute la littérature.

21
00:01:02,269 --> 00:01:08,149
Bien entendu, ils sont allés plus en profondeur,

22
00:01:05,119 --> 00:01:10,070
mais ils ont voulu aller encore plus en profondeur. Alors avec ResNet,

23
00:01:08,149 --> 00:01:12,860
ce qu'ils voulaient faire, c'était savoir jusqu’où

24
00:01:10,070 --> 00:01:15,259
il était possible d’aller. Ils ont dit : d'accord,

25
00:01:12,860 --> 00:01:17,000
c'est beau 8 couches, 20 couches,

26
00:01:15,259 --> 00:01:20,000
mais jusqu'où pouvons-nous vraiment pousser cette chose?

27
00:01:17,000 --> 00:01:21,530
Et je vous dévoile le secret,

28
00:01:20,000 --> 00:01:23,450
ils ont décidé d'utiliser 152.

29
00:01:21,530 --> 00:01:26,630
Alors ils ont décidé de pousser

30
00:01:23,450 --> 00:01:29,060
ce réseau aussi en profondeur que possible.

31
00:01:26,630 --> 00:01:31,130
J’aime bien cette citation qui dit :

32
00:01:29,060 --> 00:01:33,590
« une question se pose : l’apprentissage de meilleurs

33
00:01:31,130 --> 00:01:35,869
réseaux est-il aussi simple que l'empilement de plus de couches? »

34
00:01:33,590 --> 00:01:38,180
Donc le réseau sera-t-il meilleur

35
00:01:35,869 --> 00:01:41,270
plus on va en profondeur?

36
00:01:38,180 --> 00:01:44,119
Il s’avère

37
00:01:41,270 --> 00:01:45,829
que la réponse est non.

38
00:01:44,119 --> 00:01:47,960
En fait, ils avaient beaucoup de difficultés.

39
00:01:45,829 --> 00:01:49,850
Lorsque les gens essayaient d'entraîner naïvement

40
00:01:47,960 --> 00:01:52,040
sur des réseaux plus profonds, ils constataient qu’ils

41
00:01:49,850 --> 00:01:55,790
atteignaient ce seuil. Par exemple,

42
00:01:52,040 --> 00:01:57,890
supposons que vous avez pris un réseau, peut-être

43
00:01:55,790 --> 00:01:59,689
un réseau qui ressemble à VGG, où l'on garde

44
00:01:57,890 --> 00:02:01,579
la résolution spatiale et on augmente

45
00:01:59,689 --> 00:02:04,070
le nombre de couches. Ce qu'ils ont découvert,

46
00:02:01,579 --> 00:02:06,110
c’est qu’à un moment donné, quand vous allez passer,

47
00:02:04,070 --> 00:02:08,329
par exemple de 20 couches à 56 couches,

48
00:02:06,110 --> 00:02:11,120
votre zone d'évaluation commence à diminuer

49
00:02:08,329 --> 00:02:12,769
et l'intuition est que

50
00:02:11,120 --> 00:02:14,600
si votre réseau est en mesure de généraliser

51
00:02:12,769 --> 00:02:17,690
les 20 couches, en supposant que vous conservez

52
00:02:14,600 --> 00:02:19,190
la résolution spatiale,

53
00:02:17,690 --> 00:02:21,410
alors votre réseau devrait pouvoir apprendre

54
00:02:19,190 --> 00:02:23,419
l’application identité. Il devrait être en mesure

55
00:02:21,410 --> 00:02:24,560
d’apprendre à ne plus faire

56
00:02:23,419 --> 00:02:26,450
de calculs dans les couches suivantes

57
00:02:24,560 --> 00:02:28,430
si la profondeur entrave

58
00:02:26,450 --> 00:02:29,989
l'expérience. Alors vous vous attendez

59
00:02:28,430 --> 00:02:31,970
au moins à ce que ces réseaux aient

60
00:02:29,989 --> 00:02:34,010
la même performance. Mais ils ont constaté

61
00:02:31,970 --> 00:02:36,140
qu’il y avait une sorte de goulot d'étranglement de l'information.

62
00:02:34,010 --> 00:02:39,500
Les informations se dégradent plus vous allez en profondeur, 

63
00:02:36,140 --> 00:02:43,220
et il s'avère que l’empilement

64
00:02:39,500 --> 00:02:46,040
de plus de couches n'était pas si simple. Alors c’était

65
00:02:43,220 --> 00:02:48,470
la principale contribution derrière ResNet.

66
00:02:46,040 --> 00:02:51,170
Ils ont introduit la notion de composantes de base résiduelles,

67
00:02:48,470 --> 00:02:54,380
donc d'où le nom de ResNet (RESidual blocks).

68
00:02:51,170 --> 00:02:57,019
Cela permet au réseau d'aller

69
00:02:54,380 --> 00:02:58,430
beaucoup plus en profondeur tout en évitant

70
00:02:57,019 --> 00:03:01,190
le problème que nous venons de mentionner,

71
00:02:58,430 --> 00:03:04,160
qui est la dégradation de l’exactitude.

72
00:03:01,190 --> 00:03:07,130
Le concept principal est qu'à

73
00:03:04,160 --> 00:03:10,700
toutes les quelques couches, nous avons ce que l'on appelle une

74
00:03:07,130 --> 00:03:12,049
connexion saute-couche. Alors nous avons notre entrée,

75
00:03:10,700 --> 00:03:14,150
nous la faisons passer par une série de couches de poids

76
00:03:12,049 --> 00:03:16,370
ou des convolutions mais ensuite nous prenons l'entrée

77
00:03:14,150 --> 00:03:19,040
de notre réseau et nous l'ajoutons

78
00:03:16,370 --> 00:03:22,069
à la sortie finale. À chaque fois,

79
00:03:19,040 --> 00:03:24,140
il y a un mélange entre l'application

80
00:03:22,069 --> 00:03:26,390
identité et les caractéristiques du réseau.

81
00:03:24,140 --> 00:03:28,370
Alors le réseau peux décider d’ignorer

82
00:03:26,390 --> 00:03:30,170
complètement, par exemple mettre tous

83
00:03:28,370 --> 00:03:31,790
les poids à zéro et l’application identité

84
00:03:30,170 --> 00:03:34,100
ne ferait que ramener l’entrée originale.

85
00:03:31,790 --> 00:03:36,380
C'était donc un moyen de forcer l'information

86
00:03:34,100 --> 00:03:40,549
à travers le réseau, au fur et à mesure que nous allons

87
00:03:36,380 --> 00:03:43,760
de plus en plus en profondeur. Voici à quoi leur

88
00:03:40,549 --> 00:03:48,769
modèle ressemble. Ici, nous avons VGG19 et

89
00:03:43,760 --> 00:03:50,840
un autre équivalent, un modèle simple VGG de 34 couches.

90
00:03:48,769 --> 00:03:53,780
Et nous avons maintenant un modèle résiduel

91
00:03:50,840 --> 00:03:56,420
avec 34 couches. Il est donc très similaire

92
00:03:53,780 --> 00:03:58,730
en termes d'architecture aux convolutions

93
00:03:56,420 --> 00:04:01,100
et le type de convolutions

94
00:03:58,730 --> 00:04:04,280
qu'ils utilisent sont très similaires mais, là encore, c'est

95
00:04:01,100 --> 00:04:06,260
vraiment le concept de connexions saute-couches

96
00:04:04,280 --> 00:04:07,430
qui étaient nouveau dans cet article

97
00:04:06,260 --> 00:04:11,150
et qui leur a permis de pousser leurs

98
00:04:07,430 --> 00:04:13,280
modèles beaucoup plus en profondeur. Encore une fois,

99
00:04:11,150 --> 00:04:15,920
ce sont ces blocs très similaires qui sont utilisés

100
00:04:13,280 --> 00:04:17,810
et une autre chose intéressante est

101
00:04:15,920 --> 00:04:19,729
qu'ils n'ont pas vraiment utilisé de

102
00:04:17,810 --> 00:04:21,650
mise en commun maximale. Ils ont utilisé des convolutions

103
00:04:19,729 --> 00:04:24,169
avec une foulée de 2 pour faire leur propre

104
00:04:21,650 --> 00:04:26,210
sous-échantillonnage, sauf pour les premières couches et

105
00:04:24,169 --> 00:04:28,039
les dernières couches

106
00:04:26,210 --> 00:04:30,050
où ils utilisent une mise en commun moyenne à la fin

107
00:04:28,039 --> 00:04:31,639
et une mise en commun maximale au début.

108
00:04:30,050 --> 00:04:33,380
Mais à part ça, ils utilisent simplement

109
00:04:31,639 --> 00:04:35,950
la foulée de leur convolution pour pouvoir

110
00:04:33,380 --> 00:04:35,950
faire le sous-échantillonage.

111
00:04:38,630 --> 00:04:44,520
ResNet a donc aussi introduit le concept

112
00:04:41,280 --> 00:04:45,960
de la normalisation par lots.

113
00:04:44,520 --> 00:04:47,310
En fait, ils ont plutôt utilisé

114
00:04:45,960 --> 00:04:48,810
la normalisation par lots, mais ils n'ont pas

115
00:04:47,310 --> 00:04:51,060
introduit le concept

116
00:04:48,810 --> 00:04:54,540
de normalisation par lots. La normalisation par lots

117
00:04:51,060 --> 00:04:57,840
a été suggérée par

118
00:04:54,540 --> 00:04:59,850
les auteurs de l'article de GoogLeNet.

119
00:04:57,840 --> 00:05:02,910
Elle a été introduite comme une forme de

120
00:04:59,850 --> 00:05:04,740
régularisation du réseau et

121
00:05:02,910 --> 00:05:06,960
l'idée était qu'on voulait empêcher

122
00:05:04,740 --> 00:05:10,080
le réseau d’être bloqué dans ce que l’on appelle

123
00:05:06,960 --> 00:05:12,300
des modes saturés. Donc si vos valeurs

124
00:05:10,080 --> 00:05:13,770
commencent à devenir très grandes

125
00:05:12,300 --> 00:05:16,170
ou si vos valeurs commencent à devenir très petites,

126
00:05:13,770 --> 00:05:18,240
votre réseau se retrouve

127
00:05:16,170 --> 00:05:19,620
coincé dans ce qui est connu

128
00:05:18,240 --> 00:05:21,810
sous le nom de mode saturé,

129
00:05:19,620 --> 00:05:25,260
où l'information ne se propage pas de manière adéquate.

130
00:05:21,810 --> 00:05:28,380
Ils ont donc suggéré cette opération de normaliser par lots,

131
00:05:25,260 --> 00:05:30,510
que nous appelons généralement

132
00:05:28,380 --> 00:05:33,210
normalisation par lots. C'est un

133
00:05:30,510 --> 00:05:35,910
algorithme où ce que vous faites est que

134
00:05:33,210 --> 00:05:38,130
pour chaque chaîne que vous avez,

135
00:05:35,910 --> 00:05:41,550
et pour chaque mini lot que vous avez,

136
00:05:38,130 --> 00:05:44,160
vous calculez la moyenne de

137
00:05:41,550 --> 00:05:46,710
vos caractéristiques, vous calculez

138
00:05:44,160 --> 00:05:49,200
l'écart type, puis vous redimensionnez

139
00:05:46,710 --> 00:05:50,820
ou vous remodelez la carte des caractéristiques en fonction

140
00:05:49,200 --> 00:05:52,950
de la moyenne et de l'écart-type.

141
00:05:50,820 --> 00:05:54,750
Cependant, ils ont introduit quelques

142
00:05:52,950 --> 00:05:56,700
trucs intéressants car ils ont dû

143
00:05:54,750 --> 00:05:58,020
rendre le tout dérivable.

144
00:05:56,700 --> 00:05:59,760
Un truc intéressant qu'ils ont introduit,

145
00:05:58,020 --> 00:06:01,740
c'est qu'ils ont ajouté ces paramètres

146
00:05:59,760 --> 00:06:03,330
que le réseau pouvait apprendre afin que

147
00:06:01,740 --> 00:06:05,910
celui-ci puisse déterminer

148
00:06:03,330 --> 00:06:08,550
à quel point agrandir ou réduire les caractéristiques

149
00:06:05,910 --> 00:06:12,210
parce qu'ils se sont rendus compte

150
00:06:08,550 --> 00:06:14,970
qu’en réduisant de vos caractéristiques à

151
00:06:12,210 --> 00:06:17,220
une moyenne de 0

152
00:06:14,970 --> 00:06:19,770
ne s'avérait pas

153
00:06:17,220 --> 00:06:21,300
nécessairement utile et ils ont trouvé un moyen

154
00:06:19,770 --> 00:06:23,340
de le formuler afin que le réseau

155
00:06:21,300 --> 00:06:26,010
puisse apprendre à différentes couches à quel point

156
00:06:23,340 --> 00:06:28,770
il doit agrandir ou réduire les moyennes

157
00:06:26,010 --> 00:06:30,540
des cartes de caractéristiques.

158
00:06:28,770 --> 00:06:31,980
ResNet a aussi a utilisé ce concept de normalisation par lots

159
00:06:30,540 --> 00:06:34,380
et il s'avère qu'il s'agit

160
00:06:31,980 --> 00:06:37,080
de moyens de régularisation très utiles et

161
00:06:34,380 --> 00:06:38,940
vous mettrez en œuvre votre propre

162
00:06:37,080 --> 00:06:44,310
normalisation par lots dans les tutoriels

163
00:06:38,940 --> 00:06:46,260
plus tard. Alors examinons à nouveau

164
00:06:44,310 --> 00:06:47,880
des résultats. ResNet

165
00:06:46,260 --> 00:06:50,010
a atteint une meilleure exactitude avec

166
00:06:47,880 --> 00:06:51,100
beaucoup plus de profondeur.

167
00:06:50,010 --> 00:06:53,410
Ils ont pu se rendre à

168
00:06:51,100 --> 00:06:55,540
152 couches et vous verrez que

169
00:06:53,410 --> 00:06:57,430
le nombre de paramètres est très similaires

170
00:06:55,540 --> 00:06:59,350
au nombre de paramètres utilisés dans AlexNet.

171
00:06:57,430 --> 00:07:02,050
Donc, une fois de plus, c'était une

172
00:06:59,350 --> 00:07:03,970
contribution très intéressante car maintenant,

173
00:07:02,050 --> 00:07:05,470
non seulement ont-ils réussi à aller beaucoup plus loin,

174
00:07:03,970 --> 00:07:07,390
ils ont utilisé beaucoup moins

175
00:07:05,470 --> 00:07:09,250
de paramètres et ils ont également

176
00:07:07,390 --> 00:07:10,180
démontré qu'ils ont pu obtenir

177
00:07:09,250 --> 00:07:13,240
une plus grande exactitude.

178
00:07:10,180 --> 00:07:15,310
ResNet est un modèle

179
00:07:13,240 --> 00:07:17,430
que vous voyez souvent être utilisé dans la

180
00:07:15,310 --> 00:07:17,430
littérature.

181
00:07:17,830 --> 00:07:27,399
[Musique]

182
00:07:30,540 --> 00:07:40,999
[Musique]

183
00:07:44,150 --> 00:07:46,820
[Musique]