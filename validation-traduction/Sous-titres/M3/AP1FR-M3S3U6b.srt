1
00:00:13,000 --> 00:00:17,750
Examinons maintenant quelques architectures qui nous permettent de faire tous ces différents types de

2
00:00:17,750 --> 00:00:23,419
détections d’objets et de segmentations d’objets. Faster R-CNN (R-RNC plus rapide)

3
00:00:23,419 --> 00:00:28,070
est un modèle assez populaire qui a été proposé il y a quelques années pour la détection d'objets et

4
00:00:28,070 --> 00:00:33,379
il utilise à nouveau l'idée d'un modèle pré-entraîné, donc dans le cas de Faster R-CNN,

5
00:00:33,379 --> 00:00:38,000
ils ont pris un réseau VGG pré-entraîné sur ImageNet et ils l'ont figé.

6
00:00:38,000 --> 00:00:42,890
Ils ont décidé que ce réseau VGG ne servira que d'extracteur de caractéristiques et

7
00:00:42,890 --> 00:00:47,300
ils ont ensuite entraîné un réseau de proposition de région supplémentaire qui suggère

8
00:00:47,300 --> 00:00:53,030
des boîtes englobantes candidates pour des objets spécifiques dans une image.

9
00:00:53,030 --> 00:00:57,379
Donc ils prennent leurs cartes de caractéristiques et ce réseau séparé apprend à dire :

10
00:00:57,379 --> 00:01:01,699
je pense qu'il y a des boîtes d'intérêt ici, ici, ici, ici et ici. Ensuite, nous pouvons classer

11
00:01:01,699 --> 00:01:06,860
ce qui se trouve dans ces boîtes, faire la rétropropagation

12
00:01:06,860 --> 00:01:10,700
et entraîner un réseau de proposition de région pour qu'il soit devienne de plus en plus efficace à

13
00:01:10,700 --> 00:01:14,360
suggérer ces boîtes englobantes de candidats. Il s'avère que cette approche fonctionne très bien,

14
00:01:14,360 --> 00:01:20,210
donc je recommande vraiment d'aller lire cet article sur Faster R-CNN,

15
00:01:20,210 --> 00:01:24,950
c’était publié dans une série d'articles qui ont conduit à cet article sur Faster R-CNN.

16
00:01:24,950 --> 00:01:30,829
C'est un assez bon modèle pour la détection d'objets. Après l'arrivée de Faster R-CNN,

17
00:01:30,829 --> 00:01:35,180
un autre article appelé Mask R-CNN (Masque R-RNC) a été publié. Nous avons vu plus tôt dans la journée quelques exemples des applications de Mask R-CNN.

18
00:01:35,180 --> 00:01:39,649
Si vous vous souvenez des personnes qui marchaient et nous avions les différentes

19
00:01:39,649 --> 00:01:44,509
boîtes englobantes et la segmentation par pixel.

20
00:01:44,509 --> 00:01:49,250
En fait, c'était Mask R-CNN qui a été utilisé pour générer ces images. Donc Mask R-CNN est très populaire,

21
00:01:49,250 --> 00:01:52,670
par exemple, pour la segmentation d’instances. C'est un modèle qui existe depuis un moment

22
00:01:52,670 --> 00:01:57,649
et depuis, toutes sortes d'optimisations et d'améliorations ont été apportées à Mask R-CNN.

23
00:01:57,649 --> 00:02:01,549
Mais c'est un travail assez important en matière de segmentation d’instances

24
00:02:01,549 --> 00:02:06,409
et il fonctionne donc en prédisant en parallèle, un peu comme ce que

25
00:02:06,409 --> 00:02:11,329
Faster R-CNN faisait, les régions d'intérêt, ou les boîtes englobantes,

26
00:02:11,329 --> 00:02:15,350
mais les classes auxquelles elles appartiennent. Mais aussi pour ces boîtes englobantes, 

27
00:02:15,350 --> 00:02:20,239
il y a cette série de couches convolutives en parallèle qui prédisent un masque.

28
00:02:20,239 --> 00:02:23,480
Ce masque déterminera à nouveau le masque de segmentation pour 

29
00:02:23,480 --> 00:02:26,689
chacune de ces boîtes, ce qui constitue l'objet et

30
00:02:26,689 --> 00:02:33,140
ce qui constitue l'arrière-plan. C'est un modèle très intéressant

31
00:02:33,140 --> 00:02:37,069
avec beaucoup d'astuces pour le faire fonctionner et je recommande fortement

32
00:02:37,069 --> 00:02:43,579
de lire cet article. UNet est un autre modèle de segmentation sémantique

33
00:02:43,579 --> 00:02:48,319
et beaucoup plus facile à mettre en œuvre que Mask R-CNN.

34
00:02:48,319 --> 00:02:53,000
Il apporte aussi beaucoup d'idées intéressantes, donc il a donc été introduit dans le contexte de la segmentation d’images biomédicales,

35
00:02:53,000 --> 00:02:58,159
afin de déterminer où se trouvent les cellules dans des tranches de

36
00:02:58,159 --> 00:03:04,579
certains types de tissus. Il y avait un problème de suivi de cellules.

37
00:03:04,579 --> 00:03:08,090
Donc ce qu'il faisait, c'est qu'il trouvait un moyen de préserver les convolutions de niveau supérieur

38
00:03:08,090 --> 00:03:12,290
et de les ajouter aux caractéristiques de niveau inférieur que nous obtenons en descendant.

39
00:03:12,290 --> 00:03:16,669
On l'appelle donc UNet parce que le modèle peut être mieux visualisé

40
00:03:16,669 --> 00:03:22,730
comme une sorte de U, donc ici, en suivant ce chemin, c'est notre pipeline typique de

41
00:03:22,730 --> 00:03:26,659
réseau neuronal convolutif que nous voyons habituellement. Alors nous avons notre image d'entrée

42
00:03:26,659 --> 00:03:32,000
et nous faisons une série de convolutions. Donc en entrant ici, nous faisons des convolutions,

43
00:03:32,000 --> 00:03:38,659
nous faisons ensuite d'autres convolutions, nous faisons du sous-échantillonnage, d'autres convolutions, du sous-échantillonnage, etc.

44
00:03:38,659 --> 00:03:42,079
Jusqu’à temps que nous obtenons cette représentation finale ici. Donc nous arrivons à un volume dont nous sommes satisfaits,

45
00:03:42,079 --> 00:03:45,650
mais ce qui est vraiment intéressant avec UNet,

46
00:03:45,650 --> 00:03:50,180
c'est qu'ils font ces convolutions transposées pour revenir à la taille initiale du réseau.

47
00:03:50,180 --> 00:03:54,109
Mais ce qui est bien, c’est qu’il a ces connexions saute-couche,

48
00:03:54,109 --> 00:03:58,069
comme dans ResNet. Ils prennent ces sorties que nous avions à des niveaux supérieurs et

49
00:03:58,069 --> 00:04:01,669
ils les combinent avec les caractéristiques qu'ils ont combiné à partir des niveaux inférieurs

50
00:04:01,669 --> 00:04:05,419
et qu’ils transposent à nouveau vers des niveaux supérieurs. Alors vous avez maintenant ce mélange d'informations provenant de

51
00:04:05,419 --> 00:04:09,680
ces caractéristiques de niveaux supérieurs plus proches de l'image originale et des caractéristiques de niveaux inférieurs

52
00:04:09,680 --> 00:04:13,220
à mesure que vous vous enfoncez dans votre modèle, que vous pouvoir recombiner et réempiler

53
00:04:13,220 --> 00:04:18,440
pour finalement obtenir votre carte de segmentation de sorties.

54
00:04:18,440 --> 00:04:22,699
Le modèle fonctionne assez bien aussi pour une segmentation sémantique et beaucoup de travail a été fait à partir de celui-ci

55
00:04:22,699 --> 00:04:28,250
pour l'améliorer, mais une chose vraiment agréable à propos d'UNet est que

56
00:04:28,250 --> 00:04:34,639
c'est un modèle relativement simple à mettre en œuvre, comparé à Mask R-CNN.

57
00:04:34,639 --> 00:04:37,920
Par exemple qui, lorsque vous lirez l’article, vous verrez qu'il y a beaucoup de

58
00:04:37,920 --> 00:04:44,880
différentes astuces pour le faire fonctionner. SSD (détecteur à exemple unique) est un autre modèle pour la détection d'objets que j'aimerais mentionner

59
00:04:44,880 --> 00:04:50,310
SSD est l'abréviation de « single shots detector » et

60
00:04:50,310 --> 00:04:54,780
il a été conçu pour la détection d'objets en temps réel.

61
00:04:54,780 --> 00:05:00,270
Il peut en fait atteindre un niveau de performance assez élevé et

62
00:05:00,270 --> 00:05:05,730
fonctionner à des fréquences d'images assez élevées. L'idée derrière le réseau est assez intéressant,

63
00:05:05,730 --> 00:05:10,860
donc que ce qu'ils font, c'est qu'ils ont prédéfini

64
00:05:10,860 --> 00:05:17,100
les boîtes englobantes dans lesquelles les objets peuvent se trouver.

65
00:05:17,100 --> 00:05:20,970
Disons qu'ici, à chaque pixel, nous allons placer une série de boîtes englobantes et

66
00:05:20,970 --> 00:05:25,080
nous allons essayer de détecter s'il y a un objet dans cette boîte ou non.

67
00:05:25,080 --> 00:05:28,470
Ils vont le faire à plusieurs étapes de leur réseau, donc ils vont le faire

68
00:05:28,470 --> 00:05:31,530
à la première couche du réseau, à la deuxième couche du réseau, à la troisième couche du réseau

69
00:05:31,530 --> 00:05:35,090
et à chaque couche du réseau, ils vont décider si

70
00:05:35,090 --> 00:05:40,830
dans les boîtes que le réseau a examinés, s’il y avait un objet d'intérêt ou non

71
00:05:40,830 --> 00:05:46,380
et classifier en fonction de cela et s’y baser pour déterminer les coordonnées

72
00:05:46,380 --> 00:05:50,040
des boîtes englobantes. Il s'avère que ce modèle peut fonctionner très bien.

73
00:05:50,040 --> 00:05:54,660
Ici nous avons la comparaison de ces différents modèles :

74
00:05:54,660 --> 00:06:00,750
Faster R-CNN, que nous avons vu plus tôt, comparé à SSD.

75
00:06:00,750 --> 00:06:04,920
Cette comparaison était faite sur un ensemble de données spécifiques qui, je pense, était composé d’images de plus faible résolution et

76
00:06:04,920 --> 00:06:08,760
vous pouvez voir que la métrique de performance qu'ils ont pris pour l’évaluation,

77
00:06:08,760 --> 00:06:14,700
l’exactitude moyenne, que nous n'avons pas examiné en profondeur,

78
00:06:14,700 --> 00:06:20,700
où un score plus élevé est généralement meilleur. Vous pouvez voir que SSD, comparé à Faster R-CNN, atteint des mesures comparables, sinon meilleures,

79
00:06:20,700 --> 00:06:25,380
avec plus d’images par seconde. Après la sortie de

80
00:06:25,380 --> 00:06:31,140
ces réseaux principaux sur la détection d'objets,

81
00:06:31,140 --> 00:06:35,310
maintenant il y a eu cette course à améliorer la performance de ces modèles

82
00:06:35,310 --> 00:06:42,210
tout en utilisant beaucoup moins de ressources et en étant beaucoup plus rapide.

83
00:06:42,210 --> 00:06:47,610
Cela nous amène à MobileNet, qui est un bon exemple d'architecture pré-entraînée.

84
00:06:47,610 --> 00:06:51,750
MobileNet est entraîné sur ImageNet et MobileNet a été introduit

85
00:06:51,750 --> 00:06:58,260
dans le contexte de déployer des modèles sur des téléphones mobiles

86
00:06:58,260 --> 00:07:02,850
de façon peu coûteuse et réduire de manière significative le nombre de paramètres

87
00:07:02,850 --> 00:07:09,120
sans compromettre l'exactitude. Ils utilisent ce truc vraiment intéressant appelé

88
00:07:09,120 --> 00:07:13,290
convolutions séparables en profondeur, mais nous n'entrerons pas dans les détails de son fonctionnement.

89
00:07:13,290 --> 00:07:17,430
Si vous êtes intéressé, le lien vers le document est ici. Je vous recommande vraiment de le lire.

90
00:07:17,430 --> 00:07:21,570
Nous avons vu aujourd’hui le fonctionnement de la convolution

91
00:07:21,570 --> 00:07:27,000
et ils proposent une nouvelle ou différente façon de calculer les convolutions

92
00:07:27,000 --> 00:07:30,600
sur les cartes de caractéristiques pour les rendre beaucoup moins chères et montrer qu'en faisant cela,

93
00:07:30,600 --> 00:07:35,280
non seulement vous pouvez utiliser beaucoup moins de paramètres, mais vous pouvez aussi obtenir les mêmes mesures de d’exactitude

94
00:07:35,280 --> 00:07:41,850
ou des mesures très similaires. Par exemple en comparant VGG à MobileNet,

95
00:07:41,850 --> 00:07:46,169
vous pouvez voir que vous pouvez obtenir des erreurs de classification par critère top-5

96
00:07:46,169 --> 00:07:50,160
sur ImageNet, mais regardez le nombre de paramètres qui ont été réduits de façon considérable.

97
00:07:50,160 --> 00:07:54,210
Encore une fois, vous savez qu’avec le temps, les gens essaient

98
00:07:54,210 --> 00:07:57,900
toutes sortes de configurations et finalement, une fois que vous avez trouvé le modèle qui fonctionne,

99
00:07:57,900 --> 00:08:01,410
vous pouvez itérer et réitérer sur ces idées et essayer de trouver

100
00:08:01,410 --> 00:08:10,100
des meilleurs modèles plus performants. Regardons cette vidéo qui fait une comparaison des  différents modèles.

101
00:08:10,880 --> 00:08:17,669
Donc ici nous avons différents modèles, nous avons Faster R-CNN, nous avons SSD,

102
00:08:17,669 --> 00:08:20,850
nous avons un autre modèle appelé YOLO dont je n'ai pas vraiment parlé, mais qui est aussi efficace pour la détection en temps réel.

103
00:08:20,850 --> 00:08:25,680
Vous pouvez voir la différence dans la détection des objets, donc

104
00:08:25,680 --> 00:08:31,290
tous ces modèles essaient de calculer la position des objets en temps réel et

105
00:08:31,290 --> 00:08:35,280
vous pouvez voir toutes sortes de statistiques différentes. C'est vraiment intéressant parce que vous pouvez voir les inconvénients,

106
00:08:35,280 --> 00:08:39,539
par exemple, Faster R-CNN est très stable,

107
00:08:39,539 --> 00:08:43,650
il y a très peu de papillotements entre les images et

108
00:08:43,650 --> 00:08:47,010
il est important de noter ici que tous ces éléments sont évalués

109
00:08:47,010 --> 00:08:50,730
sur une base image par image, alors il n'y a pas nécessairement de cohérence temporelle dans ces modèles,

110
00:08:50,730 --> 00:08:54,750
mais vous verrez que Faster R-CNN est beaucoup plus robuste en termes de variations

111
00:08:54,750 --> 00:08:59,040
alors que des modèles tels que SSD pourraient avoir plus de papillotements.

112
00:08:59,040 --> 00:09:02,730
C'est là que Faster R-CNN peut être beaucoup plus lent mais peut vous permettre d'obtenir

113
00:09:02,730 --> 00:09:05,520
beaucoup plus d'exactitude, donc c'est vraiment important

114
00:09:05,520 --> 00:09:12,180
de comparer équitablement ces modèles lorsque vous les étudiez ici.

115
00:09:12,180 --> 00:09:17,010
Nous avons aussi ce tableau très intéressant, tiré d’un article qui fait une

116
00:09:17,010 --> 00:09:22,320
comparaison en profondeur des différents modèles et de leurs performances respectives.

117
00:09:22,320 --> 00:09:26,459
Nous avons des extracteurs de caractéristiques ou des réseaux pré-entraînés,

118
00:09:26,459 --> 00:09:30,720
donc différentes versions d'Inception et il existe même des combinaisons de

119
00:09:30,720 --> 00:09:35,220
Inception et ResNet qui ont été publiés, nous avons donc ResNet, VGG,

120
00:09:35,220 --> 00:09:39,630
MobileNet et Inception. Ce sont les différentes architectures dorsales qui sont utilisées.

121
00:09:39,630 --> 00:09:43,560
Nous avons alors ces différentes méta-architectures, donc ces réseaux de base sont

122
00:09:43,560 --> 00:09:47,940
utilisés sur Faster R-CNN puis nous avons une comparaison avec SSD et

123
00:09:47,940 --> 00:09:52,800
nous avons ici une courbe très intéressante où nous comparons le temps de GPU,

124
00:09:52,800 --> 00:09:57,540
qui est le temps nécessaire pour calculer ces caractéristiques par rapport à la moyenne d'exactitude générale

125
00:09:57,540 --> 00:10:00,750
que nous obtenons en utilisant ces différents modèles. Donc encore une fois,

126
00:10:00,750 --> 00:10:05,580
beaucoup de différentes études comparent ces différents modèles et

127
00:10:05,580 --> 00:10:09,000
peuvent vous donner une idée de l'inconvénient entre le temps qu'il vous faut pour

128
00:10:09,000 --> 00:10:12,990
faire des inférences et l’exactitude à laquelle vous pouvez vous attendre

129
00:10:12,990 --> 00:10:16,560
pour ces modèles. Donc c'est vraiment intéressant quand vous mettez

130
00:10:16,560 --> 00:10:21,750
les modèles en production, demandez-vous s’il vaut la peine de prioriser l'exactitude au détriment de la vitesse

131
00:10:21,750 --> 00:10:24,930
et parfois la réponse est oui, parfois la réponse est non, mais maintenant

132
00:10:24,930 --> 00:10:28,050
nous avons une sorte de mesure de la manière dont ces modèles se comparent réellement l’un à l'autre

133
00:10:28,050 --> 00:10:31,790
sur des ensembles de données similaires.