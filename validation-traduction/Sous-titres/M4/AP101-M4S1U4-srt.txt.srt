1
00:00:12,980 --> 00:00:17,490
Maintenant nous allons parler des problèmes lors de l’entraînement.

2
00:00:17,490 --> 00:00:23,580
Alors qu’est-ce qui se produit pendant la formation qui peut rendre celle-ci instable? D'abord, je vais présenter

3
00:00:23,580 --> 00:00:30,510
cette idée de dépendances à long terme. En gros, cela signifie que parfois,

4
00:00:30,510 --> 00:00:34,739
afin de faire une prédiction comme, par exemple, le y_T que vous voyez ici

5
00:00:34,739 --> 00:00:39,870
à droite, j'ai besoin d'informations qui viennent de loin dans la séquence.

6
00:00:39,870 --> 00:00:46,290
Disons que pour faire cette prédiction, j'ai besoin d'informations provenant de x_0 et

7
00:00:46,290 --> 00:00:51,030
supposons que cette séquence est très longue, peut-être qu’elle contient 100 éléments.

8
00:00:51,030 --> 00:00:54,570
C’est ce qu’on appelle une dépendance à long terme, car je dois saisir cette dépendance pour

9
00:00:54,570 --> 00:01:04,470
pour faire la bonne prédiction et voyons ce qui peut mal tourner. Alors voici

10
00:01:04,470 --> 00:01:07,560
une vision simplifiée de ce que nous avons vu auparavant et je me concentre sur

11
00:01:07,560 --> 00:01:12,240
la rétropropagation et sur cette longue chaîne de multiplication que vous avez vu

12
00:01:12,240 --> 00:01:18,470
dans l'exemple. Lorsque nous avons calculé le gradient en

13
00:01:18,470 --> 00:01:23,100
en particulier, vous voyez que le nombre d'éléments dans cette multiplication est

14
00:01:23,100 --> 00:01:30,170
le nombre d'éléments dans ma séquence et c'est parce que je dois aller de chaque

15
00:01:30,170 --> 00:01:35,670
h_t au h_(t-1) précédent et je dois le faire jusqu'à ce que j'arrive au premier,

16
00:01:35,670 --> 00:01:42,720
alors h_0 ici. Nous avons donc dit que cette séquence

17
00:01:42,720 --> 00:01:47,159
peut être très longue, disons 100 éléments. Alors nous avons ces 100 éléments que nous multiplions.

18
00:01:47,159 --> 00:01:55,140
Malheureusement, si l'on revient à l'équation qui régit

19
00:01:55,140 --> 00:02:01,260
le RNN et nous calculons la dérivée, et je n'entrerai pas dans les détails car

20
00:02:01,260 --> 00:02:05,700
je n'a même pas calculé cette partie de la dérivée. Je veux juste vous montrer que si je

21
00:02:05,700 --> 00:02:08,819
calcule la dérivée d'un état interne par rapport à l'état précédent,

22
00:02:08,819 --> 00:02:14,909
il y a ce W qui multiplie cette partie que je n'ai pas résolue,

23
00:02:14,909 --> 00:02:18,959
puisque ce n'est pas si important pour cette intuition. Vous voyez que si je multiplie

24
00:02:18,959 --> 00:02:24,780
cet état interne par rapport au précédent plusieurs fois

25
00:02:24,780 --> 00:02:28,680
et ainsi de suite, donc ces gradients, si je les multiplie plusieurs fois, disons 100

26
00:02:28,680 --> 00:02:35,630
fois, je finis par multiplier 100 fois par W. C'est un problème

27
00:02:35,630 --> 00:02:43,739
parce qu'il y a deux cas possibles : le gradient

28
00:02:43,739 --> 00:02:50,220
peut exploser ou il peut disparaître. Nous allons maintenant examiner

29
00:02:50,220 --> 00:02:55,800
ces deux cas et nous essaierons de trouver une solution au problème. Alors nous allons

30
00:02:55,800 --> 00:03:04,620
commencer par le cas de l'explosion : il s'agit ici d'un exemple très simplifié où

31
00:03:04,620 --> 00:03:12,239
Je considère W, qui est cette matrice avec un paramètre, comme un scalaire.

32
00:03:12,239 --> 00:03:17,900
Et c’est pour voir ce qui se passe du point de vue de l'intuition

33
00:03:17,900 --> 00:03:24,260
quand je multiplie encore et encore par la même matrice, qui est ce scalaire.

34
00:03:24,260 --> 00:03:30,480
Et vous voyez que si j'ai beaucoup d'éléments dans ma séquence, je multiplie T fois par W.

35
00:03:30,480 --> 00:03:36,269
En gros, je multiplie par W à la puissance de T

36
00:03:36,269 --> 00:03:43,530
et supposons que dans ce cas, W a une valeur de 10, donc ce nombre va être très élevé

37
00:03:43,530 --> 00:03:46,590
parce que, par exemple, je peux avoir 100 éléments dans ma séquence. Vous voyez que

38
00:03:46,590 --> 00:03:51,690
le gradient va exploser et ça peut mener au débordement, donc la rétropropagation

39
00:03:51,690 --> 00:03:57,690
ne fonctionnera pas. Il y a en fait une solution très simple à ce problème, je peux simplement imposer une limite

40
00:03:57,690 --> 00:04:03,480
à mon gradient Je dis que si le gradient devient

41
00:04:03,480 --> 00:04:07,859
plus grand qu’un certain seuil, je le redimensionne de manière à ce que le gradient soit

42
00:04:07,859 --> 00:04:15,180
dans le pire des cas, de la même valeur que le seuil. Nous pouvons le faire avec

43
00:04:15,180 --> 00:04:20,669
ce code ici où, en gros, je prends un gradient générique g, je calcule

44
00:04:20,669 --> 00:04:25,919
la norme si elle est supérieure au seuil et je la redimensionne. La façon dont je la redimensionne,

45
00:04:25,919 --> 00:04:30,300
c'est en prenant mon gradient g et en divisant par la norme de g. La norme est désormais de 1

46
00:04:30,300 --> 00:04:35,190
et je multiplie par le seuil, donc maintenant la norme a la valeur du seuil, et ainsi que je peux

47
00:04:35,190 --> 00:04:39,169
résoudre ce problème. Ça fonctionne réellement et c'est une solution très facile.

48
00:04:39,169 --> 00:04:47,970
Le problème inverse est la disparition du gradient. Alors dans ce cas-ci, W a une valeur de 0,1.

49
00:04:47,970 --> 00:04:55,889
La même intuition qu'avant s'applique et à la fin je multiplie mon

50
00:04:55,889 --> 00:05:01,380
gradient par ce nombre, qui est de 0,1, à la puissance 100, par exemple.

51
00:05:01,380 --> 00:05:05,580
Le résultat tend vers zéro, donc mon gradient tend maintenant vers zéro. Voyons si nous pouvons résoudre

52
00:05:05,580 --> 00:05:11,430
ce problème et, en fait, nous ne pouvons pas le résoudre.

53
00:05:11,430 --> 00:05:17,400
Cette architecture ne fonctionnera pas bien avec des séquences qui sont très longues

54
00:05:17,400 --> 00:05:24,229
en raison de cet effet de disparition du gradient. Il y a une raison derrière cela :

55
00:05:24,229 --> 00:05:28,410
comme le gradient tend vers 0, l’apprentissage de l'architecture sera extrêmement lent

56
00:05:28,410 --> 00:05:33,210
ou bien il n’y aura pas apprentissage du tout, notamment des dépendances à long terme,

57
00:05:33,210 --> 00:05:37,520
c’est-à-dire lorsque j'ai besoin que mes informations circulent

58
00:05:37,520 --> 00:05:42,979
à travers de nombreux pas dans la séquence. Afin de résoudre ce problème, nous devons introduire

59
00:05:42,979 --> 00:05:52,820
de nouvelles architectures, ce que nous ferons dans les diapositives suivantes.