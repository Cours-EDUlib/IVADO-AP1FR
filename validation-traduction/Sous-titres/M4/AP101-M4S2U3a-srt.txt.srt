1
00:00:13,360 --> 00:00:18,500
Poursuivons avec l'idée du modèle séquence à séquence. Alors voici

2
00:00:18,500 --> 00:00:23,689
une représentation où le décodeur n’est pas très intéressant, donc

3
00:00:23,689 --> 00:00:29,300
vous voyez que le décodeur n'est qu'une boîte verte. Concentrons-nous sur l’encodeur.

4
00:00:29,300 --> 00:00:38,180
Nous avons dit que l’encodeur ne fait que traiter la séquence d'entrée,

5
00:00:38,180 --> 00:00:43,460
nous ne nous intéressons pas à la sortie, mais nous sommes intéressés par le fait que

6
00:00:43,460 --> 00:00:47,570
toute l’information pertinente de la séquence d'entrée doit figurer dans cet élément,

7
00:00:47,570 --> 00:00:51,890
qui est h_2 ici, donc dans le dernier état interne de l'encodeur car c'est le

8
00:00:51,890 --> 00:00:57,649
seul élément qui va au décodeur. Une chose à remarquer est que

9
00:00:57,649 --> 00:01:04,339
ces états cachés h_0, h_1, et h_2, en particulier h_2 est un vecteur et il a une taille fixe.

10
00:01:04,339 --> 00:01:10,690
Ce n'est pas quelque chose que je peux changer pendant l’entraînement, donc dans cet exemple particulier,

11
00:01:10,690 --> 00:01:17,210
il n'y a pas de problème, la phrase est juste « je pense que », c’est très court et

12
00:01:17,210 --> 00:01:22,430
il y a très peu d'informations. L’encodeur sera probablement

13
00:01:22,430 --> 00:01:30,439
capable de stocker cette information dans ce vecteur h_2. Donc ça fonctionne, mais nous pouvons

14
00:01:30,439 --> 00:01:38,720
avoir un exemple où la séquence d'entrée est très longue et dans ce cas, le problème

15
00:01:38,720 --> 00:01:43,159
c'est que maintenant j'ai beaucoup d'informations que le décodeur essaie d'extraire et

16
00:01:43,159 --> 00:01:50,030
essaie de stocker dans le dernier état interne, en l'occurrence h_n.

17
00:01:50,030 --> 00:01:55,340
Le problème est que h_n a de nouveau une taille fixe, donc la même taille que dans le précédent

18
00:01:55,340 --> 00:02:01,969
exemple et vous pouvez maintenant voir qu'il est facile de trouver une séquence qui est suffisamment longue

19
00:02:01,969 --> 00:02:10,070
pour que l'information ne rentre pas dans cet état interne h_n et

20
00:02:10,070 --> 00:02:13,250
cela signifie que nous allons perdre certaines informations et que le décodeur

21
00:02:13,250 --> 00:02:19,699
ne sera pas en mesure de produire un résultat correct. Encore une fois,

22
00:02:19,699 --> 00:02:23,700
l'intuition clé ici est qu'il n'y a qu'un seul point de communication entre

23
00:02:23,700 --> 00:02:28,470
l’encodeur et le décodeur et en plus c'est un point de communication

24
00:02:28,470 --> 00:02:32,819
de taille fixe, donc c'est le problème que nous avons ici,

25
00:02:32,819 --> 00:02:37,680
le goulot d'étranglement que j'ai mentionné dans la diapositive et que nous aimerions résoudre.

26
00:02:37,680 --> 00:02:42,150
Voici ce que j'ai dit auparavant : si je n'ai aucune contrainte sur la longueur

27
00:02:42,150 --> 00:02:46,650
de ma séquence, je peux toujours trouver une séquence où j'ai

28
00:02:46,650 --> 00:02:52,099
tellement d’information qu'elle ne rentrera pas dans un vecteur de taille fixe. Voyons comment nous pouvons

29
00:02:52,099 --> 00:02:59,849
résoudre ce problème. Une solution possible est de trouver un moyen

30
00:02:59,849 --> 00:03:07,440
pour que mon décodeur regarde la séquence d'entrée afin d’extraire l’information pertinente

31
00:03:07,440 --> 00:03:15,930
lorsqu'il essaie de produire la sortie. L'idée est que l’encodeur

32
00:03:15,930 --> 00:03:21,840
fait toujours son travail et nous faisons toujours parvenir cet état interne h_n

33
00:03:21,840 --> 00:03:27,150
au décodeur mais celui-ci, tout en produisant la sortie à chaque pas de temps,

34
00:03:27,150 --> 00:03:34,139
aurait une façon particulière de revenir sur la séquence codée h afin d’extraire

35
00:03:34,139 --> 00:03:38,609
l’information dont il a besoin. C'est donc ce que nous aimerions faire, mais nous ne savons pas encore

36
00:03:38,609 --> 00:03:43,200
comment y arriver. Mais vous pouvez voir que si nous trouvons un moyen de le faire,

37
00:03:43,200 --> 00:03:49,019
l'information peut désormais circuler beaucoup plus facilement car à chaque pas de temps, le décodeur

38
00:03:49,019 --> 00:03:53,940
peut simplement regarder en arrière et extraire les informations dont il a besoin.

39
00:03:53,940 --> 00:04:00,750
Ici le mot clé est l’observation sélective car nous voulons non seulement que le décodeur

40
00:04:00,750 --> 00:04:05,400
revienne à la séquence codée pour obtenir toute l’information, nous voulons que

41
00:04:05,400 --> 00:04:11,670
le décodeur se concentre uniquement sur l’information qui est pertinente pour l'état actuel

42
00:04:11,670 --> 00:04:16,590
du décodeur au pas de temps actuel et en fait,

43
00:04:16,590 --> 00:04:22,710
c’est l'idée de l’attention. Alors l'attention est essentiellement un mécanisme que nous

44
00:04:22,710 --> 00:04:30,030
allons voir comment implémenter plus loin. Ce mécanisme aide le décodeur à décider

45
00:04:30,030 --> 00:04:34,950
quelles informations de la séquence d'entrée sont importantes au pas de temps actuel.

46
00:04:34,950 --> 00:04:37,470
Pour que ce soit plus clair, je vais vous donner un exemple :

47
00:04:37,470 --> 00:04:44,130
si le décodeur est dans l'état où il essaie de produire le mot « think »,

48
00:04:44,130 --> 00:04:50,370
qui est la traduction du mot « pense », nous aimerions que ce mécanisme d'attention

49
00:04:50,370 --> 00:04:56,190
observe l'entrée pour extraire l'information du mot « pense ».

50
00:04:56,190 --> 00:05:01,800
Le décodeur doit comprendre quel mot est important dans cette séquence pour l'état actuel,

51
00:05:01,800 --> 00:05:09,630
qui est de produire le mot « think » dans ce cas-ci.

52
00:05:09,630 --> 00:05:14,970
Afin de le formaliser un peu plus, l'attention doit être une fonction, donc

53
00:05:14,970 --> 00:05:18,540
ce sera un système qui aidera le décodeur à trouver l’information pertinente.

54
00:05:18,540 --> 00:05:26,010
Et nous avons dit que cette fonction fonctionnera avec deux entrées.

55
00:05:26,010 --> 00:05:32,430
La première entrée est cette séquence h où le mécanisme d’attention

56
00:05:32,430 --> 00:05:36,960
doit essayer de trouver l’information pertinente. Et l’autre information

57
00:05:36,960 --> 00:05:44,390
de la fonction d'attention, donc cet autre paramètre, c’est

58
00:05:44,390 --> 00:05:49,980
l'état de décodage s. Donc que le décodeur essaie de faire, si vous vous souveniez

59
00:05:49,980 --> 00:05:54,960
de l’exemple avant où le décodeur essayait de produire le mot « think », c'était notre état

60
00:05:54,960 --> 00:05:59,340
et nous avions la séquence avec tous ces mots et nous avons dit que l'attention

61
00:05:59,340 --> 00:06:03,120
doit indiquer au décodeur les informations que vous voulez

62
00:06:03,120 --> 00:06:08,550
qui proviennent de l'élément 1 de cette séquence, le mot « pense ».

63
00:06:08,550 --> 00:06:11,669
Nous ne savons toujours pas comment implémenter cette fonction, mais c’est ce que nous souhaitons

64
00:06:11,669 --> 00:06:17,070
faire avec cette fonction. Pour faire un pas de plus, quand on dit que l'attention

65
00:06:17,070 --> 00:06:24,720
essaie de trouver quel élément est important dans une séquence,

66
00:06:24,720 --> 00:06:29,460
cela signifie essentiellement que l'attention attribue un poids à tous les éléments

67
00:06:29,460 --> 00:06:33,480
dans cette séquence et le poids indique au décodeur quelles informations sont

68
00:06:33,480 --> 00:06:36,900
importantes. Évidemment, ces poids sont normalisés, donc tous ces

69
00:06:36,900 --> 00:06:41,910
poids sont de 0 à 1 et si nous additionnons tous les poids sur cette séquence, nous

70
00:06:41,910 --> 00:06:47,070
obtenons une une somme de 1. En gros, c'est ce que l'attention essaie de faire

71
00:06:47,070 --> 00:06:51,210
et ce qui se passe une fois que nous avons ces poids, et nous supposons 

72
00:06:51,210 --> 00:06:55,350
qu’ils sont corrects, comme dans cet exemple où nous produisions le mot « think »,

73
00:06:55,350 --> 00:06:58,980
vous pouvez voir que le plus grand poids est attribué au mot « pense » et vous voyez que 

74
00:06:58,980 --> 00:07:06,780
le poids w_1 est de 0,6. À ce stade, nous faisons simplement une somme pondérée,

75
00:07:06,780 --> 00:07:13,350
quelque chose comme ça, et en gros vous voyez que nous faisons cette somme qui est

76
00:07:13,350 --> 00:07:19,860
à l'échelle de l'importance de l'élément. À la fin de

77
00:07:19,860 --> 00:07:24,570
cette somme pondérée, on obtient un vecteur que l'on appelle c, également appelé

78
00:07:24,570 --> 00:07:30,180
vecteur de contexte car ce vecteur représente les informations qui sont

79
00:07:30,180 --> 00:07:35,040
pertinentes pour que le décodeur produise la sortie actuelle. En fait, à ce stade,

80
00:07:35,040 --> 00:07:40,290
nous envoyons ce vecteur au décodeur et

81
00:07:40,290 --> 00:07:44,760
celui-ci devrait être plus efficace à produire la sortie car il possède toute

82
00:07:44,760 --> 00:07:50,820
l’information dont elle a besoin pour le pas de temps actuel.

83
00:07:50,820 --> 00:07:57,750
Pour que ce soit plus clair, je dois d'abord mentionner que nous n'avons toujours pas vu comment implémenter cette

84
00:07:57,750 --> 00:08:04,200
attention A, ce que nous ferons plus loin. Pour l'instant, vous pouvez la considérer comme une boîte noire qui fait l’opération correcte.

85
00:08:04,200 --> 00:08:08,340
Mais avant d'en arriver là,

86
00:08:08,340 --> 00:08:14,340
je vais vous donner un exemple étape par étape qui montrera comment l'attention fonctionne et

87
00:08:14,340 --> 00:08:17,880
vous verrez que cet exemple est très similaire à l'exemple

88
00:08:17,880 --> 00:08:21,960
du modèle séquence à séquence que nous avons déjà vu, sauf que maintenant nous allons ajouter l’élément supplémentaire

89
00:08:21,960 --> 00:08:29,580
d'attention. Et vous pouvez vous souvenir de cet exemple que nous allons utiliser,

90
00:08:29,580 --> 00:08:36,900
« je suis malade ». Nous avons essayé de traduire du français vers l’anglais et nous avons dit

91
00:08:36,900 --> 00:08:41,550
que nous avons vu dans le précédent  système séquence à séquence, celui sans

92
00:08:41,550 --> 00:08:48,510
l'attention, que nous faisions fonctionner l’encodeur et maintenant nous faisons

93
00:08:48,510 --> 00:08:52,290
la même chose, donc ici rien n'a changé, nous n'avons toujours pas d'attention

94
00:08:52,290 --> 00:08:56,160
parce que l'attention est quelque chose qui se passe dans le décodeur. Donc ici nous faisons

95
00:08:56,160 --> 00:08:59,400
exactement la même chose : nous faisons traiter la séquence « je suis malade »

96
00:08:59,400 --> 00:09:03,330
par l’encodeur et encore une fois, la sortie de l’encodeur ne nous intéresse pas.

97
00:09:03,330 --> 00:09:07,139
Ce que nous intéresse, c’est la façon dont elle encode cette

98
00:09:07,139 --> 00:09:11,760
informations dans les différents h et en particulier le dernier h, qui sera envoyé au décodeur,

99
00:09:11,760 --> 00:09:17,040
même si celui-ci pourra alors utiliser l’attention

100
00:09:17,040 --> 00:09:20,940
et nous verrons comment. Mais pour l'instant, disons que l’encodeur produit

101
00:09:20,940 --> 00:09:25,560
l’état interne, en particulier l'état interne h_2 qui sera

102
00:09:25,560 --> 00:09:28,889
envoyé au décodeur. Et vous voyez que nous avons exactement le même problème qu'avant :

103
00:09:28,889 --> 00:09:33,139
h_2 a une taille fixe, donc jusqu'ici rien n'a changé.

104
00:09:33,139 --> 00:09:40,769
Puis nous initialisons le décodeur et encore une fois, rien n'a changé ici, vous voyez

105
00:09:40,769 --> 00:09:45,209
que nous envoyons h_2 au décodeur afin que celui-ci soit initialisé

106
00:09:45,209 --> 00:09:51,360
avec ce dernier état interne qui provenait de l’encodeur. Vous voyez que

107
00:09:51,360 --> 00:09:56,480
nous avons ici ce symbole de début de séquence qui est à nouveau un signal pour le décodeur

108
00:09:56,480 --> 00:10:01,470
pour commencer à générer le texte et nous utilisons le décodeur de façon autorégressive.

109
00:10:01,470 --> 00:10:09,120
Jusqu'ici, tout est exactement pareil comme avant, mais avant la mise à jour de

110
00:10:09,120 --> 00:10:12,600
l'état interne, donc avant de faire le travail de décodage. C'est là que nous utilisons l'attention.

111
00:10:12,600 --> 00:10:22,500
L’attention, encore une fois, est une boîte noire ici. Nous avons mentionné que

112
00:10:22,500 --> 00:10:25,940
l’attention a deux entrées : la première entrée est l'état du décodeur et

113
00:10:25,940 --> 00:10:31,620
la deuxième entrée est la séquence de l'encodeur, alors je vais vous montrer cela à l'aide de ces lignes.

114
00:10:31,620 --> 00:10:36,540
Vous voyez que l'attention reçoit l'état du décodeur qui,

115
00:10:36,540 --> 00:10:43,470
étant donné que nous sommes à la toute première étape, est en fait le dernier état de l’encodeur mais

116
00:10:43,470 --> 00:10:51,540
l'attention observe également les différents éléments ici, donc h_0 h_1 h_2 et nous n’avons pas précisé comment,

117
00:10:51,540 --> 00:10:55,800
mais d'une manière ou d'une autre, l'attention attribuera des poids, puis

118
00:10:55,800 --> 00:11:01,410
nous ferons une somme pondérée et nous obtiendrons un vecteur de contexte. Je le montre

119
00:11:01,410 --> 00:11:05,910
ici en rouge, alors c'est le vecteur de contexte mais vous pouvez aussi voir ici sur

120
00:11:05,910 --> 00:11:11,519
ces lignes d'attention qu’une d'entre elle est plus épaisse que l’autre et c'est parce que, idéalement

121
00:11:11,519 --> 00:11:15,700
l'attention fait l’opération correcte et elle attribue

122
00:11:15,700 --> 00:11:22,810
un poids plus élevé au premier mot, « je », simplement parce qu'elle a compris que

123
00:11:22,810 --> 00:11:26,890
le décodeur a maintenant besoin des informations provenant de ce mot pour produire la sortie actuelle.

124
00:11:26,890 --> 00:11:33,130
Nous pouvons enfin faire fonctionner le décodeur, donc le décodeur a maintenant

125
00:11:33,130 --> 00:11:37,870
l’entrée en début de séquence et l'état interne précédent, mais aussi

126
00:11:37,870 --> 00:11:42,460
le vecteur de contexte de l’attention qui, espérons-le, aide le décodeur à mettre à jour

127
00:11:42,460 --> 00:11:49,690
l’état interne. Nous avons donc maintenant s_0 et pour produire la sortie qui est le mot « I » dans ce cas,

128
00:11:49,690 --> 00:11:56,230
étant donné que c'est un modèle autorégressif, cela deviendra la prochain entrée.

129
00:11:56,230 --> 00:12:05,110
Maintenant nous sommes au pas de temps 1, donc le deuxième pour le décodeur et vous voyez

130
00:12:05,110 --> 00:12:09,160
que maintenant nous appliquons l'attention à nouveau. C'est un point clé ici.

131
00:12:09,160 --> 00:12:14,530
Avant que le décodeur ne mette à jour l'état interne, nous allons donc à nouveau faire fonctionner l'attention. Donc dans cette partie,

132
00:12:14,530 --> 00:12:19,540
vous pouvez voir que l'attention a pris comme entrée l'état interne précédent

133
00:12:19,540 --> 00:12:24,910
du décodeur et observe à nouveau la séquence codée ici de l’encodeur

134
00:12:24,910 --> 00:12:30,130
et maintenant l’attention attribue le poids le plus élevé sur le deuxième mot,

135
00:12:30,130 --> 00:12:36,160
parce que le décodeur essaie de prédire le verbe de cette phrase.

136
00:12:36,160 --> 00:12:41,200
Cette information est envoyée au décodeur et celui-ci

137
00:12:41,200 --> 00:12:48,130
crée cet état interne s_1 et ensuite nous pouvons produire la sortie « am ».

138
00:12:48,130 --> 00:12:53,590
Vous pouvez voir que « am » devient l'entrée suivante parce que nous sommes à nouveau dans un modèle

139
00:12:53,590 --> 00:13:00,070
autorégressif et encore une fois, avant de mettre à jour l'état interne s_2,

140
00:13:00,070 --> 00:13:04,780
nous faisons fonctionner l'attention. Alors vous comprenez que nous faisons fonctionner le mécanisme d’attention à chaque pas de temps.

141
00:13:04,780 --> 00:13:09,100
Cela donne au décodeur beaucoup de flexibilité car maintenant, à chaque pas de temps,

142
00:13:09,100 --> 00:13:13,090
vous pouvez revenir en arrière et extraire l’information pertinente.

143
00:13:13,090 --> 00:13:17,680
Cela améliore définitivement le flux d'informations dans ce modèle. Dans ce cas particulier,

144
00:13:17,680 --> 00:13:21,700
l'attention prend comme entrée

145
00:13:21,700 --> 00:13:27,910
l'état interne précédent s_1, observe la séquence codée ici

146
00:13:27,910 --> 00:13:32,200
et décide que le mot le plus important est « malade » qui est le troisième mot,

147
00:13:32,200 --> 00:13:35,470
donc il lui attribue un poids élevé.

148
00:13:35,470 --> 00:13:40,210
Ensuite nous faisons une somme pondérée et nous avons ici ce vecteur de contexte qui est envoyé

149
00:13:40,210 --> 00:13:46,990
au décodeur. Alors le décodeur, espérons-le, met à jour l'état interne

150
00:13:46,990 --> 00:13:51,520
h_2 de façon correcte et produit la sortie « sick » qui est correct.

151
00:13:51,520 --> 00:13:57,010
Alors il nous reste une étape, car le décodeur continuera de fonctionner

152
00:13:57,010 --> 00:14:05,260
jusqu'à ce qu'il décide qu'il est temps d'arrêter et en produisant la séquence de fin de phrase.

153
00:14:05,260 --> 00:14:12,130
Encore une fois, vous voyez qu'avant de mettre à jour s_3, nous faisons fonctionner le mécanisme attention encore,

154
00:14:12,130 --> 00:14:19,530
alors vous voyez qu'il fonctionne à chaque pas de temps. À ce stade,

155
00:14:19,530 --> 00:14:24,670
il devrait être clair pourquoi le modèle avec le mécanisme d'attention fonctionne mieux que le modèle sans mécanisme d’attention.

156
00:14:24,670 --> 00:14:28,630
Si nous avons beaucoup d'informations provenant de l'encodeur, sans le mécanisme d’attention,

157
00:14:28,630 --> 00:14:35,530
nous avons un goulot d'étranglement ici et nous finirons par remplir cet état interne h_2.

158
00:14:35,530 --> 00:14:39,010
Maintenant l’état interne h_2 est utilisé pour initialiser le décodeur,

159
00:14:39,010 --> 00:14:43,930
mais le décodeur a cette flexibilité qu'à chaque pas de temps, il peut revenir

160
00:14:43,930 --> 00:14:49,870
et extraire l’information pertinente pour ce pas de temps, puis

161
00:14:49,870 --> 00:14:54,670
produire le résultat. Le décodeur peut le faire à nouveau de manière itérative pendant ce pas de temps,

162
00:14:54,670 --> 00:15:00,160
donc le système traite beaucoup plus d'informations que

163
00:15:00,160 --> 00:15:03,510
le modèle séquence à séquence sans attention.