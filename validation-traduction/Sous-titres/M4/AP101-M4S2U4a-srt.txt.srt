1
00:00:13,120 --> 00:00:19,790
En fait, le mécanisme d'attention est tellement une bonne idée que les chercheurs ont trouvé une

2
00:00:19,790 --> 00:00:27,099
l'architecture qui est entièrement basée sur l'attention, qui est le transformateur.

3
00:00:27,099 --> 00:00:33,199
Avant d'introduire le transformateur (réseau de neurones à auto-attention), examinons ce que nous avons appris à ce stade :

4
00:00:33,199 --> 00:00:40,910
le modèle séquence à séquence qui est capable de traiter

5
00:00:40,910 --> 00:00:44,360
les tâches dans lesquelles nous avons une séquence d'entrée et séquence de sortie. Mais nous avons la flexibilité totale,

6
00:00:44,360 --> 00:00:48,980
donc nous n’avons aucune contrainte sur le nombre d'éléments.

7
00:00:48,980 --> 00:00:54,489
Pour améliorer le modèle, nous avons ajouté le mécanisme d'attention qui permet au décodeur

8
00:00:54,489 --> 00:01:00,320
d’observer la séquence codée d'entrée pour en extraire

9
00:01:00,320 --> 00:01:05,750
l'information sur demande. Et nous avons vu que cela fonctionne très bien,

10
00:01:05,750 --> 00:01:12,560
mais il y a un pépin : nous utilisons encore des RNN dans les modèles séquence à séquence et si vous vous souvenez bien,

11
00:01:12,560 --> 00:01:16,400
le modèle séquence à séquence est composée essentiellement de l’encodeur et du décodeur et

12
00:01:16,400 --> 00:01:21,470
nous avons un RNN qui peut être l’architecture tanh, LSTM, ou GRU pour l’encodeur

13
00:01:21,470 --> 00:01:26,090
et nous avons un autre RNN pour le décodeur, donc nous utilisons toujours des RNN

14
00:01:26,090 --> 00:01:34,130
et malheureusement les RNN ont deux problèmes que nous aimerions surmonter.

15
00:01:34,130 --> 00:01:40,009
Le premier problème est qu'il n'est pas facile de les mettre en parallèle et

16
00:01:40,009 --> 00:01:44,299
nous verrons cela plus en détail. Le deuxième problème est que même si nous

17
00:01:44,299 --> 00:01:48,290
utilisons des implémentations très complexe comme celles de LSTM que nous avons vue

18
00:01:48,290 --> 00:01:55,729
à la fin de la première présentation, elles ont encore du mal à prendre en compte

19
00:01:55,729 --> 00:02:00,770
les dépendances à très long terme, par exemple, si j'avais beaucoup de mots parce que je travaille

20
00:02:00,770 --> 00:02:07,340
avec un texte. Supposons que j’ai un texte de 300 mots. Étant donné la nature du RNN,

21
00:02:07,340 --> 00:02:11,870
il aura encore du mal à prendre en compte ces dépendances à long terme.

22
00:02:11,870 --> 00:02:18,019
Maintenant je vous donnerai plus de détails sur ces deux problèmes. La première chose est que les RNN

23
00:02:18,019 --> 00:02:22,609
ne sont pas faciles à mettre en parallèle et la raison est l'architecture elle-même. Si vous vous en souvenez bien,

24
00:02:22,609 --> 00:02:28,709
le RNN exécute essentiellement certaines fonctions

25
00:02:28,709 --> 00:02:34,440
encore et encore à chaque pas de temps et pour exécuter cette fonction, il

26
00:02:34,440 --> 00:02:40,739
doit examiner l'état interne précédent, comme vous voyez ici.

27
00:02:40,739 --> 00:02:50,040
Supposons que que nous voulons calculer h_4. Afin de calculer h_4, je dois connaître h_3 et

28
00:02:50,040 --> 00:02:55,920
afin de calculer h_3, j'ai besoin de connaître h_2 et ainsi de suite. Alors vous voyiez que

29
00:02:55,920 --> 00:03:02,360
nous avons un ordre de calcul ici. Je dois commencer par le premier h, qui est h_0,

30
00:03:02,360 --> 00:03:07,410
et seulement quand j'ai fini ce calcul, je peux commencer h_1 et faire

31
00:03:07,410 --> 00:03:11,850
le calcul de h_1 et une fois que j'ai fini, je peux passer à h_2 et ainsi de suite. Alors vous voyez qu'il y a

32
00:03:11,850 --> 00:03:17,790
un ordre temporel ici et c'est quelque chose de gênant parce que,

33
00:03:17,790 --> 00:03:23,430
comme j'ai mentionné au cours des jours précédents, les GPU et les TPU sont

34
00:03:23,430 --> 00:03:28,440
très efficaces pour exécuter les fonctions en parallèle. Ici nous avons quelque chose que

35
00:03:28,440 --> 00:03:33,299
nous ne pouvons pas exécuter en parallèle en raison de cette chaîne de calcul. Ce que nous aimerions faire ici,

36
00:03:33,299 --> 00:03:40,290
par exemple, est de faire fonctionner h_0, h_1, h_2, h_3 et h_4 tous en parallèle et

37
00:03:40,290 --> 00:03:44,640
en même temps. Ce serait très efficace pour l’entraînement.

38
00:03:44,640 --> 00:03:50,310
Mais ici, ce n'est pas efficace car même si le GPU la capacité de faire fonctionner de nombreuses fonctions

39
00:03:50,310 --> 00:03:54,000
en parallèle, je ne peux pas l'utiliser car je dois suivre cet ordre de calcul.

40
00:03:54,000 --> 00:04:01,820
C'est un problème que nous voudrions surmonter. Le deuxième problème que j'ai mentionné est

41
00:04:01,820 --> 00:04:09,090
la dépendance à long terme, alors vous vous souvenez peut-être de cette diapositive que nous avons utilisée

42
00:04:09,090 --> 00:04:14,549
pour présenter le problème d'explosion du gradient et de disparition du gradient. Et nous avons dit que le LSTM

43
00:04:14,549 --> 00:04:20,639
nous permet d’atténuer fortement le problème de disparition du gradient.

44
00:04:20,639 --> 00:04:26,610
Mais si vous examinez la nature du RNN, qui comprend également le LSTM et les GRU,

45
00:04:26,610 --> 00:04:32,060
la seule façon que l'information peut circuler est de passer par tous ces états internes.

46
00:04:32,060 --> 00:04:37,199
Cela signifie que, si cette séquence est composé de 200 à 300 éléments,

47
00:04:37,199 --> 00:04:40,949
et je dois prendre en compte ces dépendances très longues,

48
00:04:40,949 --> 00:04:46,139
l'information doit circuler pour de nombreux pas de temps. Dans le cas de LSTM, même si nous avons

49
00:04:46,139 --> 00:04:50,039
les cellules de mémoire, cela peut être un problème car nous traitons

50
00:04:50,039 --> 00:04:57,060
de très longues séquences et l’information doit circuler à travers tous les éléments dans la séquence.

51
00:04:57,060 --> 00:05:02,580
Il s'agit donc d'un autre problème que nous aimerions résoudre ou du moins

52
00:05:02,580 --> 00:05:09,860
atténuer. Avant d'introduire le transformateur, examinons ceci

53
00:05:09,860 --> 00:05:16,620
à la lumière du modèle séquence à séquence avec attention. Nous avons dit que

54
00:05:16,620 --> 00:05:25,139
nous aimerions trouver un remplaçant du RNN pour remédier

55
00:05:25,139 --> 00:05:29,060
aux deux problèmes que j’ai mentionnés, mais si l'on regarde l'architecture séquence à séquence au complet,

56
00:05:29,060 --> 00:05:32,639
il y a aussi des choses que nous voulons garder.

57
00:05:32,639 --> 00:05:39,810
Nous voulons supprimer la partie comportant le RNN, comme vous le voyez ici, mais nous voulons garder

58
00:05:39,810 --> 00:05:42,719
certaines choses. Nous voulons garder l'architecture encodeur-décodeur parce que nous avons vu que 

59
00:05:42,719 --> 00:05:48,899
c'est un moyen très efficace de traiter ces problèmes lorsque la longueur de la séquence d’entrée

60
00:05:48,899 --> 00:05:51,330
est différente de la séquence de sortie. Nous voulons donc garder

61
00:05:51,330 --> 00:05:57,180
cette structure encodeur-décodeur et nous voulons aussi garder le mécanisme d'attention. Nous avons vu comment

62
00:05:57,180 --> 00:06:01,830
le mécanisme d'attention améliore le flux d'informations dans

63
00:06:01,830 --> 00:06:06,860
ce modèle et nous voulons garder la nature auto-régressive du décodeur,

64
00:06:06,860 --> 00:06:13,379
car nous avons vu que c'est une façon très intelligente de produire des résultats qui sont aussi cohérents

65
00:06:13,379 --> 00:06:18,659
parce que le décodeur sait toujours ce qui a été produit auparavant. Nous voulons donc garder ces points en tête

66
00:06:18,659 --> 00:06:23,490
lors de la création de cette nouvelle architecture qui est appelée

67
00:06:23,490 --> 00:06:25,969
transformateur.