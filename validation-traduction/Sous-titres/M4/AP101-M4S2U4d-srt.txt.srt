1
00:00:14,070 --> 00:00:20,609
Je vous ai présenté l’auto-attention, mais en fait nous pouvons améliorer l'auto-attention en utilisant

2
00:00:20,609 --> 00:00:25,710
l'idée de têtes multiples, qui signifie essentiellement que nous faisons exécutons le mécanisme d’auto-attention

3
00:00:25,710 --> 00:00:32,730
plus d'une fois. Supposons que nous utilisons deux têtes, comme dans l'exemple que je vais vous montrer.

4
00:00:32,730 --> 00:00:39,149
Cela signifie que nous allons exécuter le mécanisme d'auto-attention deux fois,

5
00:00:39,149 --> 00:00:44,579
puis fusionner les résultats. Pourquoi faisons-nous cela? C'est parce que dans beaucoup de problèmes,

6
00:00:44,579 --> 00:00:49,829
en traitement du langage naturel, par exemple,

7
00:00:49,829 --> 00:00:54,510
nous avons beaucoup de différents types de relations entre les éléments.

8
00:00:54,510 --> 00:00:59,100
En traitement du langage naturel, nous pouvons avoir une relation de type sujet-verbe-objet,

9
00:00:59,100 --> 00:01:05,100
nous pouvons avoir des pronoms que nous voulons résoudre, comme « ça »

10
00:01:05,100 --> 00:01:08,460
qui réfère à un objet spécifique. C'est un autre type de

11
00:01:08,460 --> 00:01:13,950
relation que nous voulons extraire et si nous avons plusieurs mécanismes d'auto-attention,

12
00:01:13,950 --> 00:01:19,320
c’est-à-dire plusieurs têtes, chaque tête peut se spécialiser dans un type spécifique de relation.

13
00:01:19,320 --> 00:01:25,250
Nous verrons ceci dans une visualisation plus tard.

14
00:01:25,250 --> 00:01:30,270
Alors, comment pouvons-nous implémenter ce que je viens d'expliquer? J’ai dit que je vais exécuter

15
00:01:30,270 --> 00:01:38,340
l’auto-attention plusieurs fois. Dans cet exemple je n'utilise que deux têtes,

16
00:01:38,340 --> 00:01:42,540
mais pour le moment concentrons-nous sur une seule tête, tête#0. Vous voyez que l'équation

17
00:01:42,540 --> 00:01:46,970
est exactement la même que celle que vous avez vue plus tôt Donc nous faisons exactement la même opération :

18
00:01:46,970 --> 00:01:55,409
nous calculons les poids, nous calculons la somme pondérée et

19
00:01:55,409 --> 00:02:04,619
nous obtenons cette version encodée h_0. L'autre tête fera la même opération :

20
00:02:04,619 --> 00:02:11,039
calculer les pondérations, faire la somme pondérée et aboutir à

21
00:02:11,039 --> 00:02:16,340
la version encodée h_1. À ce stade, vous vous demandez peut-être

22
00:02:16,340 --> 00:02:19,070
pourquoi nous obtenons deux résultats différents si nous faisons la même opération :

23
00:02:19,070 --> 00:02:24,920
c’est à cause de la façon dont nous calculons les poids.

24
00:02:24,920 --> 00:02:30,170
J’expliquerai cela plus en détail dans la prochaine diapositive, mais l'intuition est que chaque tête

25
00:02:30,170 --> 00:02:35,180
se spécialise dans l'extraction d'un type particulier de relation et une fois que nous avons

26
00:02:35,180 --> 00:02:42,230
tous ces états encodés, seulement deux dans ce cas-ci, nous pouvons les mettre ensemble.

27
00:02:42,230 --> 00:02:49,730
Nous pouvons, par exemple, les concaténer et aboutir à l'état encodé final h_0,

28
00:02:49,730 --> 00:02:57,020
et c’est que nous voulons calculer. À ce stade, nous ne savons pas encore

29
00:02:57,020 --> 00:03:06,440
pourquoi les deux têtes produisent des résultats différents.

30
00:03:06,440 --> 00:03:12,020
Comme vous pouvez voir ici, supposons que nous voulons calculer le

31
00:03:12,020 --> 00:03:19,340
poids pour deux mots dans cet exemple : x_3 et x_4. Ces deux mots sont

32
00:03:19,340 --> 00:03:23,110
représentés ici comme des vecteurs et nous verrons dans la présentation au sujet du traitement du langage naturel

33
00:03:23,110 --> 00:03:29,260
que c'est une façon courante de représenter les mots dans ce domaine.

34
00:03:29,260 --> 00:03:36,590
Vous voyez ici que nous avons ces deux vecteurs, mais avant

35
00:03:36,590 --> 00:03:45,160
de calculer le produit scalaire pour chaque tête d'attention, nous projetterons le

36
00:03:45,160 --> 00:03:50,510
vecteur de l'espace original sur un espace différent et cette projection

37
00:03:50,510 --> 00:03:56,000
sera apprise par le modèle, donc je n'ai pas besoin de le préciser. Nous souhaitons que

38
00:03:56,000 --> 00:03:59,480
le modèle apprenne à projeter ces mots d'une manière qui permettra de tenir en compte

39
00:03:59,480 --> 00:04:05,330
certains type spécifiques de relations et vous voyez maintenant que la projection de

40
00:04:05,330 --> 00:04:11,420
la tête#0 est différente de la projection de la tête#1 et si vous calculez

41
00:04:11,420 --> 00:04:18,650
le produit scalaire, vous obtiendriez des résultats différents pour le cas 0 et pour le cas 1.

42
00:04:18,650 --> 00:04:23,950
C'est pourquoi nous nous retrouvons avec des mots différents pour chaque tête et

43
00:04:23,950 --> 00:04:27,180
évidemment, une fois que j'ai le poids, il reste à faire

44
00:04:27,180 --> 00:04:35,220
la somme pondérée et c’est tout. Alors examinons la visualisation

45
00:04:35,220 --> 00:04:40,050
de l’auto-attention.Dans ce cas-ci, la première chose à noter

46
00:04:40,050 --> 00:04:45,030
est qu'ici vous avez une séquence et ici vous avez la même séquence

47
00:04:45,030 --> 00:04:48,449
parce qu’il ne s'agit pas ici de l'idée générale d'attention que nous avons vue plus tôt

48
00:04:48,449 --> 00:04:53,699
en traduction automatique. Nous utilisons l'attention sur la même séquence.

49
00:04:53,699 --> 00:04:59,370
Alors vous voyez que nous avons le mot « its » ici et nous essayons de visualiser où

50
00:04:59,370 --> 00:05:04,470
le mécanisme d’auto-attention regarde quand il produit cette nouvelle version codée de ce mot.

51
00:05:04,470 --> 00:05:09,690
Vous voyez qu'il y a deux têtes ici, celle qui est représentée en violet

52
00:05:09,690 --> 00:05:14,430
regarde seulement le mot « law », auquel le mot « its » réfère.

53
00:05:14,430 --> 00:05:19,710
Ici, au contraire, vous voyez que cette autre tête, qui est surlignée en brun, englobe plus de mots.

54
00:05:19,710 --> 00:05:24,030
Elle regarde le mot « law » mais aussi le mot « application ».

55
00:05:24,030 --> 00:05:28,680
Alors vous pouvez voir comment chaque tête peut se spécialiser dans l'extraction d’un type de relation

56
00:05:28,680 --> 00:05:33,240
et nous pouvons ensuite les fusionner pour créer cette nouvelle

57
00:05:33,240 --> 00:05:42,960
représentation encodée. À ce stade, il nous reste une seule chose à faire :

58
00:05:42,960 --> 00:05:47,760
nous avons mentionné deux problèmes au début, nous avons dit que les RNR ne sont pas faciles à

59
00:05:47,760 --> 00:05:52,740
mettre en parallèle et qu’ils ne sont pas efficaces si nous avons de très longues séquences.

60
00:05:52,740 --> 00:05:58,250
Voyons ce qui se passe maintenant que nous avons l'architecture de transformateur :

61
00:05:58,250 --> 00:06:06,630
le problème de parallélisation est résolu car si vous avez vu comment

62
00:06:06,630 --> 00:06:11,220
nous appliquons l'auto-attention, elle ne dépend que de la séquence d'entrée et elle ne dépend plus

63
00:06:11,220 --> 00:06:16,110
de l'état interne précédent ni du suivant.

64
00:06:16,110 --> 00:06:25,380
Donc il n’y a plus d’interaction entre ces versions encodées, h_0, h_1 et h_2.

65
00:06:25,380 --> 00:06:30,449
Nous ne tenons pas en compte de l’interaction entre ces éléments

66
00:06:30,449 --> 00:06:36,479
lorsque nous appliquons l'équation pour obtenir ces valeurs de h.

67
00:06:36,479 --> 00:06:40,469
Je peux les exécuter tous en parallèle

68
00:06:40,469 --> 00:06:43,290
car la séquence d'entrée est disponible,

69
00:06:43,290 --> 00:06:49,530
et c'est génial parce que sur un GPU, cela signifie que si j'ai un bon GPU,

70
00:06:49,530 --> 00:06:54,720
il exécutera toutes ces étapes en même temps et maintenant, au lieu de cinq secondes,

71
00:06:54,720 --> 00:07:00,630
il n’en prendra qu'une seule. Ce n'est pas exactement linéaire mais

72
00:07:00,630 --> 00:07:06,240
vous comprenez l'idée. Nous n'avons plus le problème de

73
00:07:06,240 --> 00:07:12,450
cette chaîne de calcul que les RNR impliquaient. Le deuxième problème avec les RNR est

74
00:07:12,450 --> 00:07:18,060
les dépendances à long terme et maintenant nous n'avons plus ce problème.

75
00:07:18,060 --> 00:07:22,650
Cela posait problème parce que dans le RNR, supposons que nous avons

76
00:07:22,650 --> 00:07:29,160
cet élément x_5 ici et nous voulons extraire des informations qui

77
00:07:29,160 --> 00:07:35,190
proviennent de x_5 mais aussi de x_0. Si j'ai beaucoup d’étapes,

78
00:07:35,190 --> 00:07:39,270
cela pose problème avec les RNR car l'information doit passer par

79
00:07:39,270 --> 00:07:44,030
des étapes intermédiaires et il est possible que je perde des informations en cours de route.

80
00:07:44,030 --> 00:07:50,340
Avec le mécanisme d’auto-attention, nous n'avons plus cette structure où je

81
00:07:50,340 --> 00:07:55,650
je dois passer par tous les éléments et je n'applique que cette somme pondérée.

82
00:07:55,650 --> 00:08:04,230
Dans le cas de x_5, on peut dire que x_5 peut obtenir des informations de x_4 et de x_0,

83
00:08:04,230 --> 00:08:09,990
de la même manière que x_0 n’est plus pénalisé par rapport à x_4

84
00:08:09,990 --> 00:08:15,420
juste parce qu’il est situé plus loin dans le temps. Ici aussi, vous pouvez voir que

85
00:08:15,420 --> 00:08:22,340
l'auto-attention offre des avantages.