1
00:00:13,330 --> 00:00:21,349
À ce stade, nous savons comment créer ces vecteurs-mots et

2
00:00:21,349 --> 00:00:25,640
ils sont très intéressants car ils présentent des propriétés très utiles et

3
00:00:25,640 --> 00:00:30,410
en gros, ils prennent en compte la sémantique.

4
00:00:30,410 --> 00:00:36,140
Vous pouvez en voir un exemple dans cette diapositive. Donc dans cette diapositive je parle du problème <b></b>de synonymie,<b></b>

5
00:00:36,140 --> 00:00:43,250
c'est lorsque vous avez deux mots qui ont la même signification ou une signification très similaire.

6
00:00:43,250 --> 00:00:51,770
Vous voyez dans cette image

7
00:00:51,770 --> 00:00:57,320
ce qui devrait se produire si nous utilisons des vecteurs-mots.

8
00:00:57,320 --> 00:01:05,089
Les deux vecteurs de ces mots qui ont une sémantique très similaire

9
00:01:05,089 --> 00:01:13,580
sont extrêmement proches et maintenant,

10
00:01:13,580 --> 00:01:18,020
si j'utilise un modèle pour ces vecteurs-mots, ce sera beaucoup plus facile pour le modèle

11
00:01:18,020 --> 00:01:23,689
de comprendre que ces deux mots ont une signification similaire.

12
00:01:23,689 --> 00:01:29,270
Nous pouvons affirmer que les vecteurs-mots contribuent à résoudre le problème de la synonymie,

13
00:01:29,270 --> 00:01:34,579
mais malheureusement, ils ne contribuent pas du tout au problème inverse.

14
00:01:34,579 --> 00:01:39,560
Alors le problème inverse, appelé polysémie, est lorsque vous avez un mot

15
00:01:39,560 --> 00:01:44,540
qui a deux significations et ces significations dépendent du contexte dans lequel nous utilisons le mot.

16
00:01:44,540 --> 00:01:50,930
Vous voyez l'exemple ici avec le mot « book ».

17
00:01:50,930 --> 00:01:56,960
À gauche, j'ai la phrase « I read a book » et maintenant book est cet objet que je peux lire.

18
00:01:56,960 --> 00:02:03,950
À droite j’ai la phrase « did you book that room » et maintenant book est un verbe

19
00:02:03,950 --> 00:02:11,750
qui signifie réserver une chambre. Malheureusement, si j'utilise Word2Vec ou

20
00:02:11,750 --> 00:02:17,560
des vecteurs-mots en général, lorsque je veux obtenir le vecteur pour le mot « book »,

21
00:02:17,560 --> 00:02:23,359
j’obtiens toujours le même vecteur. Alors vous voyez qu'il n'y a plus de distinction et

22
00:02:23,359 --> 00:02:27,030
le modèle doit faire plus de travail pour essayer de comprendre que

23
00:02:27,030 --> 00:02:32,780
nous parlons de deux sémantiques différentes ici.

24
00:02:32,780 --> 00:02:38,730
Alors comment résoudre ce problème? Selon l'intuition, avec les vecteurs-mots,

25
00:02:38,730 --> 00:02:44,760
nous répondons essentiellement à la question « quel est le vecteur-mot pour un mot donné »?

26
00:02:44,760 --> 00:02:51,389
Comme dans le cas présent, nous voulons trouver le vecteur-mot pour le mot « book ».

27
00:02:51,389 --> 00:02:55,950
Mais si nous voulons tenir compte du contexte afin de remédier au problème de la polysémie,

28
00:02:55,950 --> 00:03:01,650
nous devrions changer cette question.

29
00:03:01,650 --> 00:03:07,169
Elle devrait être, quel est le vecteur-mot du mot « book », par exemple, étant donné que le contexte est

30
00:03:07,169 --> 00:03:16,169
la phrase « I read a book ». C'est effectivement ce qui se produit avec ce système appelé ELMo.

31
00:03:16,169 --> 00:03:23,280
ELMo signifie Embeddings from Language Models (vecteurs-mots de modèles de langage). 

32
00:03:23,280 --> 00:03:29,479
ELMo génère donc des vecteurs-mots contextualisés

33
00:03:29,479 --> 00:03:35,609
et il le fait dans une phase de pré-entraînement non supervisée.

34
00:03:35,609 --> 00:03:44,340
C'est exactement ce que faisait Word2Vec ou FastText.

35
00:03:44,340 --> 00:03:49,049
Pour ce faire, ELMo fonde la phase de pré-entraînement sur cette tâche appelée modèle de langage qui est une tâche importante en TLN aujourd’hui.

36
00:03:49,049 --> 00:03:57,229
En fait, vous verrez cette tâche aujourd'hui dans le tutoriel.

37
00:03:57,290 --> 00:04:07,040
La modélisation du langage consiste donc à modéliser la probabilité de certains textes.

38
00:04:07,040 --> 00:04:11,189
Mais l'une des applications qui nous intéresse vraiment ici est la prédiction du mot suivant.

39
00:04:11,189 --> 00:04:16,739
Alors nous pouvons former un modèle juste pour prédire le mot suivant.

40
00:04:16,739 --> 00:04:21,690
L'idée est que si le modèle est en mesure de le faire, il devrait prendre en compte

41
00:04:21,690 --> 00:04:29,280
toutes ces informations syntaxiques et sémantiques qui devraient aider

42
00:04:29,280 --> 00:04:33,690
les vecteurs-mots à présenter les propriétés que nous avons vues plus tôt dans cette présentation<b>.</b>

43
00:04:33,690 --> 00:04:38,460
Pour vous donner un exemple de la manière dont la modélisation du langage fonctionne,

44
00:04:38,460 --> 00:04:45,030
nous pouvons saisir comme entrée le mot Alan et

45
00:04:45,030 --> 00:04:50,360
nous demandons au modèle quel est le mot suivant. Dans ce cas-ci, nous espérons qu'il produira la sortie Turing.

46
00:04:50,360 --> 00:04:56,310
Nous pouvons continuer à faire cette opération. Nous saisissons en entrée le mot Alan Turing

47
00:04:56,310 --> 00:05:01,979
et le modèle devrait produire le monde « lived » et ainsi de suite.

48
00:05:01,979 --> 00:05:07,380
Ensuite nous avons « Alan Turing lived » et ainsi de suite.

49
00:05:07,380 --> 00:05:11,729
Le point clé est que la modélisation du langage est non supervisée

50
00:05:11,729 --> 00:05:16,080
et vous pouvez voir dans cet exemple pourquoi je n'ai pas besoin d'étiqueter la sortie.

51
00:05:16,080 --> 00:05:21,570
Je peux simplement le lire à partir du texte et c'est très important car

52
00:05:21,570 --> 00:05:27,960
ça veut dire que je pourrai obtenir  un grand nombre de données de manière très peu coûteuse.

53
00:05:27,960 --> 00:05:31,440
Je peux juste aller sur Wikipédia et télécharger tout le contenu de Wikipédia et

54
00:05:31,440 --> 00:05:37,560
je peux entraîner tout le modèle avec Wikipédia. La modélisation des langues, donc ici vous voyez qu'il va à gauche à droite,

55
00:05:37,560 --> 00:05:43,530
mais en fait, il peut aussi aller de droite à gauche. Donc nous demandons ici au modèle de prédire le mot précédent.

56
00:05:43,530 --> 00:05:49,889
Encore une fois, nous saisissons le mot « England » comme entrée pour le modèle

57
00:05:49,889 --> 00:05:54,210
et il prédit le mot « in ».

58
00:05:54,210 --> 00:05:59,669
Si je saisis les mots « England » et ensuite « in », il prédira le mot « lived » et ainsi de suite.

59
00:05:59,669 --> 00:06:03,180
C'est très intéressant car, comme nous l'avons mentionné dans la première présentation,

60
00:06:03,180 --> 00:06:07,830
parfois nous avons des LMCT ou RNR bidirectionnels et

61
00:06:07,830 --> 00:06:14,099
si nous voulons entraîner un modèle linguistique, nous avons besoin de ces deux tâches.

62
00:06:14,099 --> 00:06:23,880
Alors examinons l'architecture du modèle d'ELMo.

63
00:06:23,880 --> 00:06:33,680
En fait, le modèle ELMo est un LMCT bidirectionnel à N couches. Donc le modèle ELMo est basé sur les LMCT

64
00:06:33,680 --> 00:06:39,090
et en gros, c'est comme avoir deux morceaux du modèle.

65
00:06:39,090 --> 00:06:46,469
Vous voyez ici cette partie de gauche à droite et vous voyez ici la partie de droite à gauche.

66
00:06:46,469 --> 00:06:51,210
De gauche à droite, je saisis peut-être comme entrée Alan Turing et

67
00:06:51,210 --> 00:06:56,940
je demande à cette partie du modèle de produire le mot « lived ».

68
00:06:56,940 --> 00:07:02,250
Ici, à la place, la direction est de droite à gauche, donc ici je saisis « England » et je demande au modèle de produire, encore une fois, le mot « lived », qui est le mot juste avant ces deux-là.

69
00:07:02,250 --> 00:07:09,120
Ce qui est intéressant comme idée,

70
00:07:09,120 --> 00:07:18,199
c’est qu'en faisant cela, le modèle ELMo devrait prendre en compte 

71
00:07:18,199 --> 00:07:23,639
la représentation contextualisée des vecteurs-mots.

72
00:07:23,639 --> 00:07:30,330
Si vous vous souvenez bien du fonctionnement du RNR, il crée cet état interne qui

73
00:07:30,330 --> 00:07:34,020
prend aussi en compte le contexte, en particulier le contexte du passé.

74
00:07:34,020 --> 00:07:38,940
Dans ce cas-ci, ce serait de gauche à droite.

75
00:07:38,940 --> 00:07:41,789
Le contexte de l'avenir dans ce cas-ci serait de droite à gauche.

76
00:07:41,789 --> 00:07:47,940
Donc il crée une vue contextualisée de l'entrée actuelle et c'est tout ce dont nous avons besoin.

77
00:07:47,940 --> 00:07:55,800
Donc ELMo nous donne effectivement ces vecteurs-mots et

78
00:07:55,800 --> 00:08:00,449
la façon dont nous utilisons ELMo est différent de Word2Vec.

79
00:08:00,449 --> 00:08:05,909
Dans Word2Vec, nous créons une cartographique au vecteur mais vous voyez qu'ici ELMo a besoin de tout le contexte,

80
00:08:05,909 --> 00:08:11,009
donc nous allons pré-entraîner ELMo

81
00:08:11,009 --> 00:08:19,320
et ensuite nous insérons ELMo entre l'entrée et le modèle que nous voulons utiliser pour résoudre notre tâche.

82
00:08:19,320 --> 00:08:24,570
Alors Elmo est au milieu et

83
00:08:24,570 --> 00:08:31,349
il crée cette vue contextualisée pour les vecteurs-mots.

84
00:08:31,349 --> 00:08:46,829
Ensuite, je peux juste utiliser le même modèle que je voulais utiliser auparavant pour résoudre ma tâche.

85
00:08:46,829 --> 00:08:53,820
Voici les résultats pour ELMo. Je n'entrerai pas dans les détails, mais en gros, les tâche que vous voyez,

86
00:08:53,820 --> 00:08:55,860
certaine d'entre elles sont celles que nous avons vus au début de cette présentation.

87
00:08:55,860 --> 00:08:58,920
Certains d'entre elles,

88
00:08:58,920 --> 00:09:03,830
comme la résolution de coréférence, consiste simplement à résoudre les pronoms dans une phrase.

89
00:09:03,830 --> 00:09:08,779
Vous pouvez voir que la partie intéressante est cette dernière colonne.

90
00:09:08,779 --> 00:09:14,120
Dans cette colonne, on compare les résultats de l'utilisation d'un modèle appelé référence,

91
00:09:14,120 --> 00:09:22,490
donc on utilise un modèle pour résoudre ces tâches à gauche.

92
00:09:22,490 --> 00:09:29,630
Ensuite, on utilise ELMo en plus du même modèle pour résoudre la même tâche.

93
00:09:29,630 --> 00:09:38,360
Vous pouvez voir les résultats ont augmenté pour toutes les tâches.

94
00:09:38,360 --> 00:09:44,900
Alors vous voyez que les vecteurs-mots contextualisés sont vraiment utiles.

95
00:09:44,900 --> 00:09:49,070
Ici vous pouvez voir une visualisation. Nous avons mentionné qu'ELMo crée

96
00:09:49,070 --> 00:09:58,490
des vecteurs-mots contextualisés, ce qui me permet maintenant de tracer les mots

97
00:09:58,490 --> 00:10:03,350
en considérant le contexte. Maintenant, si j'ai le mot « bank » et

98
00:10:03,350 --> 00:10:10,790
je considère le contexte, ici en haut à gauche

99
00:10:10,790 --> 00:10:16,190
j’ai le mot « bank » dans le contexte de « river bank »

100
00:10:16,190 --> 00:10:21,530
et en bas à gauche, j'ai toutes les phrases qui sont liées à « bank » qui désigne l'endroit où je vais déposer mon argent.

101
00:10:21,530 --> 00:10:28,310
Les mots ont maintenant deux sémantiques différentes et ils sont éloignés dans cet espace vectoriel,

102
00:10:28,310 --> 00:10:34,970
ce qui est évidemment utile pour le modèle.

103
00:10:34,970 --> 00:10:41,410
Nous avons vu comment cela améliore les résultats des tâches dans la diapositive précédente.