1
00:00:13,030 --> 00:00:18,800
À ce stade, faisons un vite récapitulatif. Alors nous savons comment faire des vecteurs-mots.

2
00:00:18,800 --> 00:00:23,450
Dans cette présentation, je ne montre que deux algorithmes qui y sont destinés :

3
00:00:23,450 --> 00:00:30,740
Word2Vec et FastText. Nous avons vu que ces vecteurs-mots

4
00:00:30,740 --> 00:00:34,699
présentent des propriétés très intéressantes qui sont utiles pour les modèles.

5
00:00:34,699 --> 00:00:40,839
En particulier, ils prennent en compte des sémantiques associées à ces mots.

6
00:00:40,839 --> 00:00:46,039
Mais nous sommes allés plus loin et nous avons vu qu'il ne suffit pas d'avoir cette cartographie.

7
00:00:46,039 --> 00:00:50,120
Nous voulons également prendre en compte le contexte, et c'est pourquoi nous avons introduit ELMo.

8
00:00:50,120 --> 00:00:57,020
Donc ELMo crée essentiellement cette vue contextualisée pour les vecteurs-mots

9
00:00:57,020 --> 00:01:04,280
et maintenant nous pouvons les utiliser pour former notre modèle.

10
00:01:04,280 --> 00:01:12,640
Nous pouvons mettre ELMo entre l'entrée et notre modèle. Alors est-ce le meilleur modèle?

11
00:01:12,640 --> 00:01:17,530
En fait, il existe un autre système. Il existe bien d'autres systèmes, mais pour cette présentation, nous n’allons examiner qu’un seul.

12
00:01:17,530 --> 00:01:22,909
Ce système s'appelle BERT. BERT est l'abbriéviation de Bidirectional Encoder Representation Transformers

13
00:01:22,909 --> 00:01:30,619
et le modèle BERT a été une architecture très importante.

14
00:01:30,619 --> 00:01:36,350
Il a permis d’améliorer considérablement les résultats de nombreuses tâches et c'est probablement votre premier choix à essayer

15
00:01:36,350 --> 00:01:41,689
si vous avez des tâches de TLN que vous voulez résoudre, au moins pour obtenir une très bonne référence.

16
00:01:41,689 --> 00:01:49,070
Alors quelle est la différence entre BERT et ELMo?

17
00:01:49,070 --> 00:01:55,670
BERT est basée sur l'architecture des transformateurs. Si vous vous souvenez bien, ELMo était basée sur les LMCT

18
00:01:55,670 --> 00:02:00,320
et BERT n'utilise pas de modélisation du langage pendant la phase de pré-entraînement. Il a légèrement modifié cette tâche

19
00:02:00,320 --> 00:02:05,509
et il introduit une tâche qui est appelée modélisation du langage masqué.

20
00:02:05,509 --> 00:02:10,100
Nous verrons cela en détail dans la prochaine diapositive,

21
00:02:10,100 --> 00:02:15,230
et en fait il utilise une autre tâche appelée « Prédiction de la phrase suivante ».

22
00:02:15,230 --> 00:02:21,949
Examinons alors ces deux tâches. La modélisation du langage masqué est très semblable à la modélisation du langage :

23
00:02:21,949 --> 00:02:26,870
nous prenons une séquence et nous masquons certaines unités lexicales.

24
00:02:26,870 --> 00:02:31,129
Dans cet exemple, nous n’en avons masqué qu’une, mais vous pouvez en masquer plusieurs et vous demandez ensuite au modèle de prédire l’unité lexicale.

25
00:02:31,129 --> 00:02:38,810
C’est très similaire à la modélisation linguistique,

26
00:02:38,810 --> 00:02:43,129
mais il n'y a pas de direction de gauche à droite ou de droite à gauche, comme vous voyez ici,

27
00:02:43,129 --> 00:02:47,569
j'essaie de prédire ce mot au milieu qui devrait être « lived » et le modèle est capable d’observer le mot Alan Turing

28
00:02:47,569 --> 00:02:52,280
qui se trouve dans le passé, mais aussi le mot « England », qui se trouve dans le futur.

29
00:02:52,280 --> 00:02:56,689
Avec la modélisation linguistique, nous avons un ordre que nous devons suivre et nous ne pouvons observer seulement le passé ou le futur,

30
00:02:56,689 --> 00:03:01,040
mais la modélisation du langage masqué fonctionne différemment.

31
00:03:01,040 --> 00:03:05,780
En plus, je peux masquer plus d’unités lexicales, ici il y en a juste une de masquée mais je peux en maquer plus qu’une.

32
00:03:05,780 --> 00:03:17,030
Donc c’était la première tâche. La deuxième tâche est la prédiction de la phrase suivante,

33
00:03:17,030 --> 00:03:22,280
En gros, ici, nous donnons deux phrases au modèle et nous demandons aux modèles de m’indiquer

34
00:03:22,280 --> 00:03:27,530
si les deux phrases sont contiguës ou non.

35
00:03:27,530 --> 00:03:31,220
Le premier cas est contigu car quelqu'un est allé au magasin et il acheté du lait,

36
00:03:31,220 --> 00:03:36,560
donc il s'agit très probablement d'un texte contigu,

37
00:03:36,560 --> 00:03:41,299
ce qui n'est pas le cas dans le deuxième exemple et cela devrait nous aider avec la tâche que nous avons vue au début de la présentation,

38
00:03:41,299 --> 00:03:44,989
où nous avons ces tâches basées deux phrases.

39
00:03:44,989 --> 00:03:48,949
C'est parce que le modèle est maintenant obligé de prendre en compte une relation entre les phrases.

40
00:03:48,949 --> 00:03:56,680
Alors ce qui est bien avec le modèle BERT,

41
00:03:56,680 --> 00:04:03,500
c'est qu'avec ELMo nous avions encore besoin de construire notre modèle et

42
00:04:03,500 --> 00:04:08,449
ELMo était entre l'entrée et notre modèle, mais avec BERT nous n'avons plus besoin de notre modèle.

43
00:04:08,449 --> 00:04:13,189
Nous pouvons utiliser directement BERT, ce qui est très pratique.

44
00:04:13,189 --> 00:04:17,560
BERT a été pré-entraîné par quelqu'un d'autre parce qu'il faut une éternité pour pré-entraîner. 

45
00:04:17,560 --> 00:04:24,440
Nous pouvons télécharger BERT et nous pouvons choisir la tâche qui nous intéresse et maintenant nous pouvons le perfectionner.

46
00:04:24,440 --> 00:04:29,780
Nous pouvons continuer à entraîner le modèle avec les données dont nous disposons pour notre tâche,

47
00:04:29,780 --> 00:04:33,950
même si ce n'est pas beaucoup de données et vous pouvez voir la différence ici.

48
00:04:33,950 --> 00:04:36,650
Au lieu de créer un autre modèle, nous pouvons maintenant utiliser directement BERT

49
00:04:36,650 --> 00:04:43,500
et l'architecture de BERT changera légèrement en fonction de la tâche que nous sommes en train de résoudre,

50
00:04:43,500 --> 00:04:48,150
mais l'idée géniale est que la partie centrale de BERT est déjà pré-entraînée

51
00:04:48,150 --> 00:04:53,340
dans la phase de pré-entraînement avec beaucoup de données car

52
00:04:53,340 --> 00:04:57,990
cette phase n'a pas été supervisé . Vous pouvez voir ici deux exemples : je n'entrerai donc pas

53
00:04:57,990 --> 00:05:02,130
les détails parce que nous n'avons pas beaucoup de temps

54
00:05:02,130 --> 00:05:08,220
mais, en gros, vous voyez que l'architecture changera surtout au sommet, juste pour s'adapter au type de sortie que nous voulons prédire,

55
00:05:08,220 --> 00:05:14,400
mais là encore, la partie essentielle reste la même.

56
00:05:14,400 --> 00:05:21,030
Voici quelques résultats, mais nous n'entrerons pas dans les détails,

57
00:05:21,030 --> 00:05:26,220
vous pouvez juste voir que BERT donne de meilleurs résultats par rapport aux approches précédentes.

58
00:05:26,220 --> 00:05:34,170
Il y a deux types de modèles, je pense que celui de base est de 12 couches et le large en a 24, donc nous parlons de très grands modèles maintenant

59
00:05:34,170 --> 00:05:40,490
et vous voyez comment BERT améliore considérablement les résultats.